{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Akanksha-Chandra/Loan_Approval_Prediction/blob/main/OCR.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BjEeZ8CTjDvi",
        "outputId": "9d1a9e92-1817-4f40-c187-b8a9952d83dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.44.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.4.1+cu121)\n",
            "Collecting datasets\n",
            "  Downloading datasets-3.0.1-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (0.24.7)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: tokenizers<0.20,>=0.19 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.19.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.5)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2024.6.1)\n",
            "Collecting pyarrow>=15.0.0 (from datasets)\n",
            "  Downloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.3 kB)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Collecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl.metadata (7.2 kB)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.8.30)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading datasets-3.0.1-py3-none-any.whl (471 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m471.6/471.6 kB\u001b[0m \u001b[31m9.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-17.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (39.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m39.9/39.9 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xxhash, pyarrow, dill, multiprocess, datasets\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 17.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed datasets-3.0.1 dill-0.3.8 multiprocess-0.70.16 pyarrow-17.0.0 xxhash-3.5.0\n",
            "Collecting pytesseract\n",
            "  Downloading pytesseract-0.3.13-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (10.4.0)\n",
            "Collecting verovio\n",
            "  Downloading verovio-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.0 kB)\n",
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (3.0.1)\n",
            "Requirement already satisfied: packaging>=21.3 in /usr/local/lib/python3.10/dist-packages (from pytesseract) (24.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2024.9.11)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (2.1.4)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.5)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.6.1,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.6.1,>=2023.1.0->datasets) (2024.6.1)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.22.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.24.7)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.22.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
            "Downloading pytesseract-0.3.13-py3-none-any.whl (14 kB)\n",
            "Downloading verovio-4.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (8.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.1/8.1 MB\u001b[0m \u001b[31m58.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: verovio, pytesseract, tiktoken\n",
            "Successfully installed pytesseract-0.3.13 tiktoken-0.7.0 verovio-4.3.1\n"
          ]
        }
      ],
      "source": [
        "!pip install transformers torch datasets huggingface_hub\n",
        "!pip install pytesseract Pillow verovio tiktoken datasets\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pAM-BzuWKbKq",
        "outputId": "56eed42c-5c09-45aa-cd2a-1b35f1e37330"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found existing installation: pyarrow 17.0.0\n",
            "Uninstalling pyarrow-17.0.0:\n",
            "  Successfully uninstalled pyarrow-17.0.0\n",
            "Collecting pyarrow==14.0.1\n",
            "  Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow==14.0.1) (1.26.4)\n",
            "Downloading pyarrow-14.0.1-cp310-cp310-manylinux_2_28_x86_64.whl (38.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.0/38.0 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyarrow\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datasets 3.0.1 requires pyarrow>=15.0.0, but you have pyarrow 14.0.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pyarrow-14.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall pyarrow -y\n",
        "!pip install pyarrow==14.0.1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H0IvHnDPK4RR",
        "outputId": "389bed55-5d20-46a3-c2e3-f40f79f75b84"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: cudf-cu12 in /usr/local/lib/python3.10/dist-packages (24.4.1)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (5.5.0)\n",
            "Requirement already satisfied: cuda-python<13.0a0,>=12.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (12.2.1)\n",
            "Requirement already satisfied: cupy-cuda12x>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (12.2.0)\n",
            "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (2024.6.1)\n",
            "Requirement already satisfied: numba>=0.57 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.60.0)\n",
            "Requirement already satisfied: numpy<2.0a0,>=1.23 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (1.26.4)\n",
            "Requirement already satisfied: nvtx>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.2.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.1)\n",
            "Requirement already satisfied: pandas<2.2.2dev0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (2.1.4)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (3.20.3)\n",
            "Requirement already satisfied: pynvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (0.3.0)\n",
            "Requirement already satisfied: pyarrow<15.0.0a0,>=14.0.1 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (14.0.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (13.8.1)\n",
            "Requirement already satisfied: rmm-cu12==24.4.* in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (24.4.0)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from cudf-cu12) (4.12.2)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python<13.0a0,>=12.0->cudf-cu12) (3.0.11)\n",
            "Requirement already satisfied: fastrlock>=0.5 in /usr/local/lib/python3.10/dist-packages (from cupy-cuda12x>=12.0.0->cudf-cu12) (0.8.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.57->cudf-cu12) (0.43.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<2.2.2dev0,>=2.0->cudf-cu12) (2024.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->cudf-cu12) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->cudf-cu12) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<2.2.2dev0,>=2.0->cudf-cu12) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install cudf-cu12\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FaTCSqzU9IX6",
        "outputId": "cadb560d-ea06-4d76-8172-c4af05050499"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: kaggle in /usr/local/lib/python3.10/dist-packages (1.6.17)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.10/dist-packages (from kaggle) (1.16.0)\n",
            "Requirement already satisfied: certifi>=2023.7.22 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.8.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.32.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from kaggle) (4.66.5)\n",
            "Requirement already satisfied: python-slugify in /usr/local/lib/python3.10/dist-packages (from kaggle) (8.0.4)\n",
            "Requirement already satisfied: urllib3 in /usr/local/lib/python3.10/dist-packages (from kaggle) (2.2.3)\n",
            "Requirement already satisfied: bleach in /usr/local/lib/python3.10/dist-packages (from kaggle) (6.1.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from bleach->kaggle) (0.5.1)\n",
            "Requirement already satisfied: text-unidecode>=1.3 in /usr/local/lib/python3.10/dist-packages (from python-slugify->kaggle) (1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->kaggle) (3.10)\n",
            "cp: cannot stat 'kaggle.json': No such file or directory\n",
            "chmod: cannot access '/root/.kaggle/kaggle.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "! pip install kaggle\n",
        "! mkdir ~/.kaggle\n",
        "! cp kaggle.json ~/.kaggle/\n",
        "! chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "opdGkEzK9dQM",
        "outputId": "4a77690e-e482-4e8a-a300-a366d37ffc70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/prathmeshzade/hindi-ocr-synthetic-line-image-text-pair\n",
            "License(s): MIT\n",
            "Downloading hindi-ocr-synthetic-line-image-text-pair.zip to /content\n",
            " 99% 729M/735M [00:09<00:00, 96.7MB/s]\n",
            "100% 735M/735M [00:09<00:00, 77.7MB/s]\n"
          ]
        }
      ],
      "source": [
        "! kaggle datasets download -d prathmeshzade/hindi-ocr-synthetic-line-image-text-pair"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WXV4ACKpB01a",
        "outputId": "b570bd8e-a4eb-49d8-9873-5bace6269686"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "  inflating: data_80k/output_images/775.png  \n",
            "  inflating: data_80k/output_images/7750.png  \n",
            "  inflating: data_80k/output_images/77500.png  \n",
            "  inflating: data_80k/output_images/77501.png  \n",
            "  inflating: data_80k/output_images/77502.png  \n",
            "  inflating: data_80k/output_images/77503.png  \n",
            "  inflating: data_80k/output_images/77504.png  \n",
            "  inflating: data_80k/output_images/77505.png  \n",
            "  inflating: data_80k/output_images/77506.png  \n",
            "  inflating: data_80k/output_images/77507.png  \n",
            "  inflating: data_80k/output_images/77508.png  \n",
            "  inflating: data_80k/output_images/77509.png  \n",
            "  inflating: data_80k/output_images/7751.png  \n",
            "  inflating: data_80k/output_images/77510.png  \n",
            "  inflating: data_80k/output_images/77511.png  \n",
            "  inflating: data_80k/output_images/77512.png  \n",
            "  inflating: data_80k/output_images/77513.png  \n",
            "  inflating: data_80k/output_images/77514.png  \n",
            "  inflating: data_80k/output_images/77515.png  \n",
            "  inflating: data_80k/output_images/77516.png  \n",
            "  inflating: data_80k/output_images/77517.png  \n",
            "  inflating: data_80k/output_images/77518.png  \n",
            "  inflating: data_80k/output_images/77519.png  \n",
            "  inflating: data_80k/output_images/7752.png  \n",
            "  inflating: data_80k/output_images/77520.png  \n",
            "  inflating: data_80k/output_images/77521.png  \n",
            "  inflating: data_80k/output_images/77522.png  \n",
            "  inflating: data_80k/output_images/77523.png  \n",
            "  inflating: data_80k/output_images/77524.png  \n",
            "  inflating: data_80k/output_images/77525.png  \n",
            "  inflating: data_80k/output_images/77526.png  \n",
            "  inflating: data_80k/output_images/77527.png  \n",
            "  inflating: data_80k/output_images/77528.png  \n",
            "  inflating: data_80k/output_images/77529.png  \n",
            "  inflating: data_80k/output_images/7753.png  \n",
            "  inflating: data_80k/output_images/77530.png  \n",
            "  inflating: data_80k/output_images/77531.png  \n",
            "  inflating: data_80k/output_images/77532.png  \n",
            "  inflating: data_80k/output_images/77533.png  \n",
            "  inflating: data_80k/output_images/77534.png  \n",
            "  inflating: data_80k/output_images/77535.png  \n",
            "  inflating: data_80k/output_images/77536.png  \n",
            "  inflating: data_80k/output_images/77537.png  \n",
            "  inflating: data_80k/output_images/77538.png  \n",
            "  inflating: data_80k/output_images/77539.png  \n",
            "  inflating: data_80k/output_images/7754.png  \n",
            "  inflating: data_80k/output_images/77540.png  \n",
            "  inflating: data_80k/output_images/77541.png  \n",
            "  inflating: data_80k/output_images/77542.png  \n",
            "  inflating: data_80k/output_images/77543.png  \n",
            "  inflating: data_80k/output_images/77544.png  \n",
            "  inflating: data_80k/output_images/77545.png  \n",
            "  inflating: data_80k/output_images/77546.png  \n",
            "  inflating: data_80k/output_images/77547.png  \n",
            "  inflating: data_80k/output_images/77548.png  \n",
            "  inflating: data_80k/output_images/77549.png  \n",
            "  inflating: data_80k/output_images/7755.png  \n",
            "  inflating: data_80k/output_images/77550.png  \n",
            "  inflating: data_80k/output_images/77551.png  \n",
            "  inflating: data_80k/output_images/77552.png  \n",
            "  inflating: data_80k/output_images/77553.png  \n",
            "  inflating: data_80k/output_images/77554.png  \n",
            "  inflating: data_80k/output_images/77555.png  \n",
            "  inflating: data_80k/output_images/77556.png  \n",
            "  inflating: data_80k/output_images/77557.png  \n",
            "  inflating: data_80k/output_images/77558.png  \n",
            "  inflating: data_80k/output_images/77559.png  \n",
            "  inflating: data_80k/output_images/7756.png  \n",
            "  inflating: data_80k/output_images/77560.png  \n",
            "  inflating: data_80k/output_images/77561.png  \n",
            "  inflating: data_80k/output_images/77562.png  \n",
            "  inflating: data_80k/output_images/77563.png  \n",
            "  inflating: data_80k/output_images/77564.png  \n",
            "  inflating: data_80k/output_images/77565.png  \n",
            "  inflating: data_80k/output_images/77566.png  \n",
            "  inflating: data_80k/output_images/77567.png  \n",
            "  inflating: data_80k/output_images/77568.png  \n",
            "  inflating: data_80k/output_images/77569.png  \n",
            "  inflating: data_80k/output_images/7757.png  \n",
            "  inflating: data_80k/output_images/77570.png  \n",
            "  inflating: data_80k/output_images/77571.png  \n",
            "  inflating: data_80k/output_images/77572.png  \n",
            "  inflating: data_80k/output_images/77573.png  \n",
            "  inflating: data_80k/output_images/77574.png  \n",
            "  inflating: data_80k/output_images/77575.png  \n",
            "  inflating: data_80k/output_images/77576.png  \n",
            "  inflating: data_80k/output_images/77577.png  \n",
            "  inflating: data_80k/output_images/77578.png  \n",
            "  inflating: data_80k/output_images/77579.png  \n",
            "  inflating: data_80k/output_images/7758.png  \n",
            "  inflating: data_80k/output_images/77580.png  \n",
            "  inflating: data_80k/output_images/77581.png  \n",
            "  inflating: data_80k/output_images/77582.png  \n",
            "  inflating: data_80k/output_images/77583.png  \n",
            "  inflating: data_80k/output_images/77584.png  \n",
            "  inflating: data_80k/output_images/77585.png  \n",
            "  inflating: data_80k/output_images/77586.png  \n",
            "  inflating: data_80k/output_images/77587.png  \n",
            "  inflating: data_80k/output_images/77588.png  \n",
            "  inflating: data_80k/output_images/77589.png  \n",
            "  inflating: data_80k/output_images/7759.png  \n",
            "  inflating: data_80k/output_images/77590.png  \n",
            "  inflating: data_80k/output_images/77591.png  \n",
            "  inflating: data_80k/output_images/77592.png  \n",
            "  inflating: data_80k/output_images/77593.png  \n",
            "  inflating: data_80k/output_images/77594.png  \n",
            "  inflating: data_80k/output_images/77595.png  \n",
            "  inflating: data_80k/output_images/77596.png  \n",
            "  inflating: data_80k/output_images/77597.png  \n",
            "  inflating: data_80k/output_images/77598.png  \n",
            "  inflating: data_80k/output_images/77599.png  \n",
            "  inflating: data_80k/output_images/776.png  \n",
            "  inflating: data_80k/output_images/7760.png  \n",
            "  inflating: data_80k/output_images/77600.png  \n",
            "  inflating: data_80k/output_images/77601.png  \n",
            "  inflating: data_80k/output_images/77602.png  \n",
            "  inflating: data_80k/output_images/77603.png  \n",
            "  inflating: data_80k/output_images/77604.png  \n",
            "  inflating: data_80k/output_images/77605.png  \n",
            "  inflating: data_80k/output_images/77606.png  \n",
            "  inflating: data_80k/output_images/77607.png  \n",
            "  inflating: data_80k/output_images/77608.png  \n",
            "  inflating: data_80k/output_images/77609.png  \n",
            "  inflating: data_80k/output_images/7761.png  \n",
            "  inflating: data_80k/output_images/77610.png  \n",
            "  inflating: data_80k/output_images/77611.png  \n",
            "  inflating: data_80k/output_images/77612.png  \n",
            "  inflating: data_80k/output_images/77613.png  \n",
            "  inflating: data_80k/output_images/77614.png  \n",
            "  inflating: data_80k/output_images/77615.png  \n",
            "  inflating: data_80k/output_images/77616.png  \n",
            "  inflating: data_80k/output_images/77617.png  \n",
            "  inflating: data_80k/output_images/77618.png  \n",
            "  inflating: data_80k/output_images/77619.png  \n",
            "  inflating: data_80k/output_images/7762.png  \n",
            "  inflating: data_80k/output_images/77620.png  \n",
            "  inflating: data_80k/output_images/77621.png  \n",
            "  inflating: data_80k/output_images/77622.png  \n",
            "  inflating: data_80k/output_images/77623.png  \n",
            "  inflating: data_80k/output_images/77624.png  \n",
            "  inflating: data_80k/output_images/77625.png  \n",
            "  inflating: data_80k/output_images/77626.png  \n",
            "  inflating: data_80k/output_images/77627.png  \n",
            "  inflating: data_80k/output_images/77628.png  \n",
            "  inflating: data_80k/output_images/77629.png  \n",
            "  inflating: data_80k/output_images/7763.png  \n",
            "  inflating: data_80k/output_images/77630.png  \n",
            "  inflating: data_80k/output_images/77631.png  \n",
            "  inflating: data_80k/output_images/77632.png  \n",
            "  inflating: data_80k/output_images/77633.png  \n",
            "  inflating: data_80k/output_images/77634.png  \n",
            "  inflating: data_80k/output_images/77635.png  \n",
            "  inflating: data_80k/output_images/77636.png  \n",
            "  inflating: data_80k/output_images/77637.png  \n",
            "  inflating: data_80k/output_images/77638.png  \n",
            "  inflating: data_80k/output_images/77639.png  \n",
            "  inflating: data_80k/output_images/7764.png  \n",
            "  inflating: data_80k/output_images/77640.png  \n",
            "  inflating: data_80k/output_images/77641.png  \n",
            "  inflating: data_80k/output_images/77642.png  \n",
            "  inflating: data_80k/output_images/77643.png  \n",
            "  inflating: data_80k/output_images/77644.png  \n",
            "  inflating: data_80k/output_images/77645.png  \n",
            "  inflating: data_80k/output_images/77646.png  \n",
            "  inflating: data_80k/output_images/77647.png  \n",
            "  inflating: data_80k/output_images/77648.png  \n",
            "  inflating: data_80k/output_images/77649.png  \n",
            "  inflating: data_80k/output_images/7765.png  \n",
            "  inflating: data_80k/output_images/77650.png  \n",
            "  inflating: data_80k/output_images/77651.png  \n",
            "  inflating: data_80k/output_images/77652.png  \n",
            "  inflating: data_80k/output_images/77653.png  \n",
            "  inflating: data_80k/output_images/77654.png  \n",
            "  inflating: data_80k/output_images/77655.png  \n",
            "  inflating: data_80k/output_images/77656.png  \n",
            "  inflating: data_80k/output_images/77657.png  \n",
            "  inflating: data_80k/output_images/77658.png  \n",
            "  inflating: data_80k/output_images/77659.png  \n",
            "  inflating: data_80k/output_images/7766.png  \n",
            "  inflating: data_80k/output_images/77660.png  \n",
            "  inflating: data_80k/output_images/77661.png  \n",
            "  inflating: data_80k/output_images/77662.png  \n",
            "  inflating: data_80k/output_images/77663.png  \n",
            "  inflating: data_80k/output_images/77664.png  \n",
            "  inflating: data_80k/output_images/77665.png  \n",
            "  inflating: data_80k/output_images/77666.png  \n",
            "  inflating: data_80k/output_images/77667.png  \n",
            "  inflating: data_80k/output_images/77668.png  \n",
            "  inflating: data_80k/output_images/77669.png  \n",
            "  inflating: data_80k/output_images/7767.png  \n",
            "  inflating: data_80k/output_images/77670.png  \n",
            "  inflating: data_80k/output_images/77671.png  \n",
            "  inflating: data_80k/output_images/77672.png  \n",
            "  inflating: data_80k/output_images/77673.png  \n",
            "  inflating: data_80k/output_images/77674.png  \n",
            "  inflating: data_80k/output_images/77675.png  \n",
            "  inflating: data_80k/output_images/77676.png  \n",
            "  inflating: data_80k/output_images/77677.png  \n",
            "  inflating: data_80k/output_images/77678.png  \n",
            "  inflating: data_80k/output_images/77679.png  \n",
            "  inflating: data_80k/output_images/7768.png  \n",
            "  inflating: data_80k/output_images/77680.png  \n",
            "  inflating: data_80k/output_images/77681.png  \n",
            "  inflating: data_80k/output_images/77682.png  \n",
            "  inflating: data_80k/output_images/77683.png  \n",
            "  inflating: data_80k/output_images/77684.png  \n",
            "  inflating: data_80k/output_images/77685.png  \n",
            "  inflating: data_80k/output_images/77686.png  \n",
            "  inflating: data_80k/output_images/77687.png  \n",
            "  inflating: data_80k/output_images/77688.png  \n",
            "  inflating: data_80k/output_images/77689.png  \n",
            "  inflating: data_80k/output_images/7769.png  \n",
            "  inflating: data_80k/output_images/77690.png  \n",
            "  inflating: data_80k/output_images/77691.png  \n",
            "  inflating: data_80k/output_images/77692.png  \n",
            "  inflating: data_80k/output_images/77693.png  \n",
            "  inflating: data_80k/output_images/77694.png  \n",
            "  inflating: data_80k/output_images/77695.png  \n",
            "  inflating: data_80k/output_images/77696.png  \n",
            "  inflating: data_80k/output_images/77697.png  \n",
            "  inflating: data_80k/output_images/77698.png  \n",
            "  inflating: data_80k/output_images/77699.png  \n",
            "  inflating: data_80k/output_images/777.png  \n",
            "  inflating: data_80k/output_images/7770.png  \n",
            "  inflating: data_80k/output_images/77700.png  \n",
            "  inflating: data_80k/output_images/77701.png  \n",
            "  inflating: data_80k/output_images/77702.png  \n",
            "  inflating: data_80k/output_images/77703.png  \n",
            "  inflating: data_80k/output_images/77704.png  \n",
            "  inflating: data_80k/output_images/77705.png  \n",
            "  inflating: data_80k/output_images/77706.png  \n",
            "  inflating: data_80k/output_images/77707.png  \n",
            "  inflating: data_80k/output_images/77708.png  \n",
            "  inflating: data_80k/output_images/77709.png  \n",
            "  inflating: data_80k/output_images/7771.png  \n",
            "  inflating: data_80k/output_images/77710.png  \n",
            "  inflating: data_80k/output_images/77711.png  \n",
            "  inflating: data_80k/output_images/77712.png  \n",
            "  inflating: data_80k/output_images/77713.png  \n",
            "  inflating: data_80k/output_images/77714.png  \n",
            "  inflating: data_80k/output_images/77715.png  \n",
            "  inflating: data_80k/output_images/77716.png  \n",
            "  inflating: data_80k/output_images/77717.png  \n",
            "  inflating: data_80k/output_images/77718.png  \n",
            "  inflating: data_80k/output_images/77719.png  \n",
            "  inflating: data_80k/output_images/7772.png  \n",
            "  inflating: data_80k/output_images/77720.png  \n",
            "  inflating: data_80k/output_images/77721.png  \n",
            "  inflating: data_80k/output_images/77722.png  \n",
            "  inflating: data_80k/output_images/77723.png  \n",
            "  inflating: data_80k/output_images/77724.png  \n",
            "  inflating: data_80k/output_images/77725.png  \n",
            "  inflating: data_80k/output_images/77726.png  \n",
            "  inflating: data_80k/output_images/77727.png  \n",
            "  inflating: data_80k/output_images/77728.png  \n",
            "  inflating: data_80k/output_images/77729.png  \n",
            "  inflating: data_80k/output_images/7773.png  \n",
            "  inflating: data_80k/output_images/77730.png  \n",
            "  inflating: data_80k/output_images/77731.png  \n",
            "  inflating: data_80k/output_images/77732.png  \n",
            "  inflating: data_80k/output_images/77733.png  \n",
            "  inflating: data_80k/output_images/77734.png  \n",
            "  inflating: data_80k/output_images/77735.png  \n",
            "  inflating: data_80k/output_images/77736.png  \n",
            "  inflating: data_80k/output_images/77737.png  \n",
            "  inflating: data_80k/output_images/77738.png  \n",
            "  inflating: data_80k/output_images/77739.png  \n",
            "  inflating: data_80k/output_images/7774.png  \n",
            "  inflating: data_80k/output_images/77740.png  \n",
            "  inflating: data_80k/output_images/77741.png  \n",
            "  inflating: data_80k/output_images/77742.png  \n",
            "  inflating: data_80k/output_images/77743.png  \n",
            "  inflating: data_80k/output_images/77744.png  \n",
            "  inflating: data_80k/output_images/77745.png  \n",
            "  inflating: data_80k/output_images/77746.png  \n",
            "  inflating: data_80k/output_images/77747.png  \n",
            "  inflating: data_80k/output_images/77748.png  \n",
            "  inflating: data_80k/output_images/77749.png  \n",
            "  inflating: data_80k/output_images/7775.png  \n",
            "  inflating: data_80k/output_images/77750.png  \n",
            "  inflating: data_80k/output_images/77751.png  \n",
            "  inflating: data_80k/output_images/77752.png  \n",
            "  inflating: data_80k/output_images/77753.png  \n",
            "  inflating: data_80k/output_images/77754.png  \n",
            "  inflating: data_80k/output_images/77755.png  \n",
            "  inflating: data_80k/output_images/77756.png  \n",
            "  inflating: data_80k/output_images/77757.png  \n",
            "  inflating: data_80k/output_images/77758.png  \n",
            "  inflating: data_80k/output_images/77759.png  \n",
            "  inflating: data_80k/output_images/7776.png  \n",
            "  inflating: data_80k/output_images/77760.png  \n",
            "  inflating: data_80k/output_images/77761.png  \n",
            "  inflating: data_80k/output_images/77762.png  \n",
            "  inflating: data_80k/output_images/77763.png  \n",
            "  inflating: data_80k/output_images/77764.png  \n",
            "  inflating: data_80k/output_images/77765.png  \n",
            "  inflating: data_80k/output_images/77766.png  \n",
            "  inflating: data_80k/output_images/77767.png  \n",
            "  inflating: data_80k/output_images/77768.png  \n",
            "  inflating: data_80k/output_images/77769.png  \n",
            "  inflating: data_80k/output_images/7777.png  \n",
            "  inflating: data_80k/output_images/77770.png  \n",
            "  inflating: data_80k/output_images/77771.png  \n",
            "  inflating: data_80k/output_images/77772.png  \n",
            "  inflating: data_80k/output_images/77773.png  \n",
            "  inflating: data_80k/output_images/77774.png  \n",
            "  inflating: data_80k/output_images/77775.png  \n",
            "  inflating: data_80k/output_images/77776.png  \n",
            "  inflating: data_80k/output_images/77777.png  \n",
            "  inflating: data_80k/output_images/77778.png  \n",
            "  inflating: data_80k/output_images/77779.png  \n",
            "  inflating: data_80k/output_images/7778.png  \n",
            "  inflating: data_80k/output_images/77780.png  \n",
            "  inflating: data_80k/output_images/77781.png  \n",
            "  inflating: data_80k/output_images/77782.png  \n",
            "  inflating: data_80k/output_images/77783.png  \n",
            "  inflating: data_80k/output_images/77784.png  \n",
            "  inflating: data_80k/output_images/77785.png  \n",
            "  inflating: data_80k/output_images/77786.png  \n",
            "  inflating: data_80k/output_images/77787.png  \n",
            "  inflating: data_80k/output_images/77788.png  \n",
            "  inflating: data_80k/output_images/77789.png  \n",
            "  inflating: data_80k/output_images/7779.png  \n",
            "  inflating: data_80k/output_images/77790.png  \n",
            "  inflating: data_80k/output_images/77791.png  \n",
            "  inflating: data_80k/output_images/77792.png  \n",
            "  inflating: data_80k/output_images/77793.png  \n",
            "  inflating: data_80k/output_images/77794.png  \n",
            "  inflating: data_80k/output_images/77795.png  \n",
            "  inflating: data_80k/output_images/77796.png  \n",
            "  inflating: data_80k/output_images/77797.png  \n",
            "  inflating: data_80k/output_images/77798.png  \n",
            "  inflating: data_80k/output_images/77799.png  \n",
            "  inflating: data_80k/output_images/778.png  \n",
            "  inflating: data_80k/output_images/7780.png  \n",
            "  inflating: data_80k/output_images/77800.png  \n",
            "  inflating: data_80k/output_images/77801.png  \n",
            "  inflating: data_80k/output_images/77802.png  \n",
            "  inflating: data_80k/output_images/77803.png  \n",
            "  inflating: data_80k/output_images/77804.png  \n",
            "  inflating: data_80k/output_images/77805.png  \n",
            "  inflating: data_80k/output_images/77806.png  \n",
            "  inflating: data_80k/output_images/77807.png  \n",
            "  inflating: data_80k/output_images/77808.png  \n",
            "  inflating: data_80k/output_images/77809.png  \n",
            "  inflating: data_80k/output_images/7781.png  \n",
            "  inflating: data_80k/output_images/77810.png  \n",
            "  inflating: data_80k/output_images/77811.png  \n",
            "  inflating: data_80k/output_images/77812.png  \n",
            "  inflating: data_80k/output_images/77813.png  \n",
            "  inflating: data_80k/output_images/77814.png  \n",
            "  inflating: data_80k/output_images/77815.png  \n",
            "  inflating: data_80k/output_images/77816.png  \n",
            "  inflating: data_80k/output_images/77817.png  \n",
            "  inflating: data_80k/output_images/77818.png  \n",
            "  inflating: data_80k/output_images/77819.png  \n",
            "  inflating: data_80k/output_images/7782.png  \n",
            "  inflating: data_80k/output_images/77820.png  \n",
            "  inflating: data_80k/output_images/77821.png  \n",
            "  inflating: data_80k/output_images/77822.png  \n",
            "  inflating: data_80k/output_images/77823.png  \n",
            "  inflating: data_80k/output_images/77824.png  \n",
            "  inflating: data_80k/output_images/77825.png  \n",
            "  inflating: data_80k/output_images/77826.png  \n",
            "  inflating: data_80k/output_images/77827.png  \n",
            "  inflating: data_80k/output_images/77828.png  \n",
            "  inflating: data_80k/output_images/77829.png  \n",
            "  inflating: data_80k/output_images/7783.png  \n",
            "  inflating: data_80k/output_images/77830.png  \n",
            "  inflating: data_80k/output_images/77831.png  \n",
            "  inflating: data_80k/output_images/77832.png  \n",
            "  inflating: data_80k/output_images/77833.png  \n",
            "  inflating: data_80k/output_images/77834.png  \n",
            "  inflating: data_80k/output_images/77835.png  \n",
            "  inflating: data_80k/output_images/77836.png  \n",
            "  inflating: data_80k/output_images/77837.png  \n",
            "  inflating: data_80k/output_images/77838.png  \n",
            "  inflating: data_80k/output_images/77839.png  \n",
            "  inflating: data_80k/output_images/7784.png  \n",
            "  inflating: data_80k/output_images/77840.png  \n",
            "  inflating: data_80k/output_images/77841.png  \n",
            "  inflating: data_80k/output_images/77842.png  \n",
            "  inflating: data_80k/output_images/77843.png  \n",
            "  inflating: data_80k/output_images/77844.png  \n",
            "  inflating: data_80k/output_images/77845.png  \n",
            "  inflating: data_80k/output_images/77846.png  \n",
            "  inflating: data_80k/output_images/77847.png  \n",
            "  inflating: data_80k/output_images/77848.png  \n",
            "  inflating: data_80k/output_images/77849.png  \n",
            "  inflating: data_80k/output_images/7785.png  \n",
            "  inflating: data_80k/output_images/77850.png  \n",
            "  inflating: data_80k/output_images/77851.png  \n",
            "  inflating: data_80k/output_images/77852.png  \n",
            "  inflating: data_80k/output_images/77853.png  \n",
            "  inflating: data_80k/output_images/77854.png  \n",
            "  inflating: data_80k/output_images/77855.png  \n",
            "  inflating: data_80k/output_images/77856.png  \n",
            "  inflating: data_80k/output_images/77857.png  \n",
            "  inflating: data_80k/output_images/77858.png  \n",
            "  inflating: data_80k/output_images/77859.png  \n",
            "  inflating: data_80k/output_images/7786.png  \n",
            "  inflating: data_80k/output_images/77860.png  \n",
            "  inflating: data_80k/output_images/77861.png  \n",
            "  inflating: data_80k/output_images/77862.png  \n",
            "  inflating: data_80k/output_images/77863.png  \n",
            "  inflating: data_80k/output_images/77864.png  \n",
            "  inflating: data_80k/output_images/77865.png  \n",
            "  inflating: data_80k/output_images/77866.png  \n",
            "  inflating: data_80k/output_images/77867.png  \n",
            "  inflating: data_80k/output_images/77868.png  \n",
            "  inflating: data_80k/output_images/77869.png  \n",
            "  inflating: data_80k/output_images/7787.png  \n",
            "  inflating: data_80k/output_images/77870.png  \n",
            "  inflating: data_80k/output_images/77871.png  \n",
            "  inflating: data_80k/output_images/77872.png  \n",
            "  inflating: data_80k/output_images/77873.png  \n",
            "  inflating: data_80k/output_images/77874.png  \n",
            "  inflating: data_80k/output_images/77875.png  \n",
            "  inflating: data_80k/output_images/77876.png  \n",
            "  inflating: data_80k/output_images/77877.png  \n",
            "  inflating: data_80k/output_images/77878.png  \n",
            "  inflating: data_80k/output_images/77879.png  \n",
            "  inflating: data_80k/output_images/7788.png  \n",
            "  inflating: data_80k/output_images/77880.png  \n",
            "  inflating: data_80k/output_images/77881.png  \n",
            "  inflating: data_80k/output_images/77882.png  \n",
            "  inflating: data_80k/output_images/77883.png  \n",
            "  inflating: data_80k/output_images/77884.png  \n",
            "  inflating: data_80k/output_images/77885.png  \n",
            "  inflating: data_80k/output_images/77886.png  \n",
            "  inflating: data_80k/output_images/77887.png  \n",
            "  inflating: data_80k/output_images/77888.png  \n",
            "  inflating: data_80k/output_images/77889.png  \n",
            "  inflating: data_80k/output_images/7789.png  \n",
            "  inflating: data_80k/output_images/77890.png  \n",
            "  inflating: data_80k/output_images/77891.png  \n",
            "  inflating: data_80k/output_images/77892.png  \n",
            "  inflating: data_80k/output_images/77893.png  \n",
            "  inflating: data_80k/output_images/77894.png  \n",
            "  inflating: data_80k/output_images/77895.png  \n",
            "  inflating: data_80k/output_images/77896.png  \n",
            "  inflating: data_80k/output_images/77897.png  \n",
            "  inflating: data_80k/output_images/77898.png  \n",
            "  inflating: data_80k/output_images/77899.png  \n",
            "  inflating: data_80k/output_images/779.png  \n",
            "  inflating: data_80k/output_images/7790.png  \n",
            "  inflating: data_80k/output_images/77900.png  \n",
            "  inflating: data_80k/output_images/77901.png  \n",
            "  inflating: data_80k/output_images/77902.png  \n",
            "  inflating: data_80k/output_images/77903.png  \n",
            "  inflating: data_80k/output_images/77904.png  \n",
            "  inflating: data_80k/output_images/77905.png  \n",
            "  inflating: data_80k/output_images/77906.png  \n",
            "  inflating: data_80k/output_images/77907.png  \n",
            "  inflating: data_80k/output_images/77908.png  \n",
            "  inflating: data_80k/output_images/77909.png  \n",
            "  inflating: data_80k/output_images/7791.png  \n",
            "  inflating: data_80k/output_images/77910.png  \n",
            "  inflating: data_80k/output_images/77911.png  \n",
            "  inflating: data_80k/output_images/77912.png  \n",
            "  inflating: data_80k/output_images/77913.png  \n",
            "  inflating: data_80k/output_images/77914.png  \n",
            "  inflating: data_80k/output_images/77915.png  \n",
            "  inflating: data_80k/output_images/77916.png  \n",
            "  inflating: data_80k/output_images/77917.png  \n",
            "  inflating: data_80k/output_images/77918.png  \n",
            "  inflating: data_80k/output_images/77919.png  \n",
            "  inflating: data_80k/output_images/7792.png  \n",
            "  inflating: data_80k/output_images/77920.png  \n",
            "  inflating: data_80k/output_images/77921.png  \n",
            "  inflating: data_80k/output_images/77922.png  \n",
            "  inflating: data_80k/output_images/77923.png  \n",
            "  inflating: data_80k/output_images/77924.png  \n",
            "  inflating: data_80k/output_images/77925.png  \n",
            "  inflating: data_80k/output_images/77926.png  \n",
            "  inflating: data_80k/output_images/77927.png  \n",
            "  inflating: data_80k/output_images/77928.png  \n",
            "  inflating: data_80k/output_images/77929.png  \n",
            "  inflating: data_80k/output_images/7793.png  \n",
            "  inflating: data_80k/output_images/77930.png  \n",
            "  inflating: data_80k/output_images/77931.png  \n",
            "  inflating: data_80k/output_images/77932.png  \n",
            "  inflating: data_80k/output_images/77933.png  \n",
            "  inflating: data_80k/output_images/77934.png  \n",
            "  inflating: data_80k/output_images/77935.png  \n",
            "  inflating: data_80k/output_images/77936.png  \n",
            "  inflating: data_80k/output_images/77937.png  \n",
            "  inflating: data_80k/output_images/77938.png  \n",
            "  inflating: data_80k/output_images/77939.png  \n",
            "  inflating: data_80k/output_images/7794.png  \n",
            "  inflating: data_80k/output_images/77940.png  \n",
            "  inflating: data_80k/output_images/77941.png  \n",
            "  inflating: data_80k/output_images/77942.png  \n",
            "  inflating: data_80k/output_images/77943.png  \n",
            "  inflating: data_80k/output_images/77944.png  \n",
            "  inflating: data_80k/output_images/77945.png  \n",
            "  inflating: data_80k/output_images/77946.png  \n",
            "  inflating: data_80k/output_images/77947.png  \n",
            "  inflating: data_80k/output_images/77948.png  \n",
            "  inflating: data_80k/output_images/77949.png  \n",
            "  inflating: data_80k/output_images/7795.png  \n",
            "  inflating: data_80k/output_images/77950.png  \n",
            "  inflating: data_80k/output_images/77951.png  \n",
            "  inflating: data_80k/output_images/77952.png  \n",
            "  inflating: data_80k/output_images/77953.png  \n",
            "  inflating: data_80k/output_images/77954.png  \n",
            "  inflating: data_80k/output_images/77955.png  \n",
            "  inflating: data_80k/output_images/77956.png  \n",
            "  inflating: data_80k/output_images/77957.png  \n",
            "  inflating: data_80k/output_images/77958.png  \n",
            "  inflating: data_80k/output_images/77959.png  \n",
            "  inflating: data_80k/output_images/7796.png  \n",
            "  inflating: data_80k/output_images/77960.png  \n",
            "  inflating: data_80k/output_images/77961.png  \n",
            "  inflating: data_80k/output_images/77962.png  \n",
            "  inflating: data_80k/output_images/77963.png  \n",
            "  inflating: data_80k/output_images/77964.png  \n",
            "  inflating: data_80k/output_images/77965.png  \n",
            "  inflating: data_80k/output_images/77966.png  \n",
            "  inflating: data_80k/output_images/77967.png  \n",
            "  inflating: data_80k/output_images/77968.png  \n",
            "  inflating: data_80k/output_images/77969.png  \n",
            "  inflating: data_80k/output_images/7797.png  \n",
            "  inflating: data_80k/output_images/77970.png  \n",
            "  inflating: data_80k/output_images/77971.png  \n",
            "  inflating: data_80k/output_images/77972.png  \n",
            "  inflating: data_80k/output_images/77973.png  \n",
            "  inflating: data_80k/output_images/77974.png  \n",
            "  inflating: data_80k/output_images/77975.png  \n",
            "  inflating: data_80k/output_images/77976.png  \n",
            "  inflating: data_80k/output_images/77977.png  \n",
            "  inflating: data_80k/output_images/77978.png  \n",
            "  inflating: data_80k/output_images/77979.png  \n",
            "  inflating: data_80k/output_images/7798.png  \n",
            "  inflating: data_80k/output_images/77980.png  \n",
            "  inflating: data_80k/output_images/77981.png  \n",
            "  inflating: data_80k/output_images/77982.png  \n",
            "  inflating: data_80k/output_images/77983.png  \n",
            "  inflating: data_80k/output_images/77984.png  \n",
            "  inflating: data_80k/output_images/77985.png  \n",
            "  inflating: data_80k/output_images/77986.png  \n",
            "  inflating: data_80k/output_images/77987.png  \n",
            "  inflating: data_80k/output_images/77988.png  \n",
            "  inflating: data_80k/output_images/77989.png  \n",
            "  inflating: data_80k/output_images/7799.png  \n",
            "  inflating: data_80k/output_images/77990.png  \n",
            "  inflating: data_80k/output_images/77991.png  \n",
            "  inflating: data_80k/output_images/77992.png  \n",
            "  inflating: data_80k/output_images/77993.png  \n",
            "  inflating: data_80k/output_images/77994.png  \n",
            "  inflating: data_80k/output_images/77995.png  \n",
            "  inflating: data_80k/output_images/77996.png  \n",
            "  inflating: data_80k/output_images/77997.png  \n",
            "  inflating: data_80k/output_images/77998.png  \n",
            "  inflating: data_80k/output_images/77999.png  \n",
            "  inflating: data_80k/output_images/78.png  \n",
            "  inflating: data_80k/output_images/780.png  \n",
            "  inflating: data_80k/output_images/7800.png  \n",
            "  inflating: data_80k/output_images/78000.png  \n",
            "  inflating: data_80k/output_images/78001.png  \n",
            "  inflating: data_80k/output_images/78002.png  \n",
            "  inflating: data_80k/output_images/78003.png  \n",
            "  inflating: data_80k/output_images/78004.png  \n",
            "  inflating: data_80k/output_images/78005.png  \n",
            "  inflating: data_80k/output_images/78006.png  \n",
            "  inflating: data_80k/output_images/78007.png  \n",
            "  inflating: data_80k/output_images/78008.png  \n",
            "  inflating: data_80k/output_images/78009.png  \n",
            "  inflating: data_80k/output_images/7801.png  \n",
            "  inflating: data_80k/output_images/78010.png  \n",
            "  inflating: data_80k/output_images/78011.png  \n",
            "  inflating: data_80k/output_images/78012.png  \n",
            "  inflating: data_80k/output_images/78013.png  \n",
            "  inflating: data_80k/output_images/78014.png  \n",
            "  inflating: data_80k/output_images/78015.png  \n",
            "  inflating: data_80k/output_images/78016.png  \n",
            "  inflating: data_80k/output_images/78017.png  \n",
            "  inflating: data_80k/output_images/78018.png  \n",
            "  inflating: data_80k/output_images/78019.png  \n",
            "  inflating: data_80k/output_images/7802.png  \n",
            "  inflating: data_80k/output_images/78020.png  \n",
            "  inflating: data_80k/output_images/78021.png  \n",
            "  inflating: data_80k/output_images/78022.png  \n",
            "  inflating: data_80k/output_images/78023.png  \n",
            "  inflating: data_80k/output_images/78024.png  \n",
            "  inflating: data_80k/output_images/78025.png  \n",
            "  inflating: data_80k/output_images/78026.png  \n",
            "  inflating: data_80k/output_images/78027.png  \n",
            "  inflating: data_80k/output_images/78028.png  \n",
            "  inflating: data_80k/output_images/78029.png  \n",
            "  inflating: data_80k/output_images/7803.png  \n",
            "  inflating: data_80k/output_images/78030.png  \n",
            "  inflating: data_80k/output_images/78031.png  \n",
            "  inflating: data_80k/output_images/78032.png  \n",
            "  inflating: data_80k/output_images/78033.png  \n",
            "  inflating: data_80k/output_images/78034.png  \n",
            "  inflating: data_80k/output_images/78035.png  \n",
            "  inflating: data_80k/output_images/78036.png  \n",
            "  inflating: data_80k/output_images/78037.png  \n",
            "  inflating: data_80k/output_images/78038.png  \n",
            "  inflating: data_80k/output_images/78039.png  \n",
            "  inflating: data_80k/output_images/7804.png  \n",
            "  inflating: data_80k/output_images/78040.png  \n",
            "  inflating: data_80k/output_images/78041.png  \n",
            "  inflating: data_80k/output_images/78042.png  \n",
            "  inflating: data_80k/output_images/78043.png  \n",
            "  inflating: data_80k/output_images/78044.png  \n",
            "  inflating: data_80k/output_images/78045.png  \n",
            "  inflating: data_80k/output_images/78046.png  \n",
            "  inflating: data_80k/output_images/78047.png  \n",
            "  inflating: data_80k/output_images/78048.png  \n",
            "  inflating: data_80k/output_images/78049.png  \n",
            "  inflating: data_80k/output_images/7805.png  \n",
            "  inflating: data_80k/output_images/78050.png  \n",
            "  inflating: data_80k/output_images/78051.png  \n",
            "  inflating: data_80k/output_images/78052.png  \n",
            "  inflating: data_80k/output_images/78053.png  \n",
            "  inflating: data_80k/output_images/78054.png  \n",
            "  inflating: data_80k/output_images/78055.png  \n",
            "  inflating: data_80k/output_images/78056.png  \n",
            "  inflating: data_80k/output_images/78057.png  \n",
            "  inflating: data_80k/output_images/78058.png  \n",
            "  inflating: data_80k/output_images/78059.png  \n",
            "  inflating: data_80k/output_images/7806.png  \n",
            "  inflating: data_80k/output_images/78060.png  \n",
            "  inflating: data_80k/output_images/78061.png  \n",
            "  inflating: data_80k/output_images/78062.png  \n",
            "  inflating: data_80k/output_images/78063.png  \n",
            "  inflating: data_80k/output_images/78064.png  \n",
            "  inflating: data_80k/output_images/78065.png  \n",
            "  inflating: data_80k/output_images/78066.png  \n",
            "  inflating: data_80k/output_images/78067.png  \n",
            "  inflating: data_80k/output_images/78068.png  \n",
            "  inflating: data_80k/output_images/78069.png  \n",
            "  inflating: data_80k/output_images/7807.png  \n",
            "  inflating: data_80k/output_images/78070.png  \n",
            "  inflating: data_80k/output_images/78071.png  \n",
            "  inflating: data_80k/output_images/78072.png  \n",
            "  inflating: data_80k/output_images/78073.png  \n",
            "  inflating: data_80k/output_images/78074.png  \n",
            "  inflating: data_80k/output_images/78075.png  \n",
            "  inflating: data_80k/output_images/78076.png  \n",
            "  inflating: data_80k/output_images/78077.png  \n",
            "  inflating: data_80k/output_images/78078.png  \n",
            "  inflating: data_80k/output_images/78079.png  \n",
            "  inflating: data_80k/output_images/7808.png  \n",
            "  inflating: data_80k/output_images/78080.png  \n",
            "  inflating: data_80k/output_images/78081.png  \n",
            "  inflating: data_80k/output_images/78082.png  \n",
            "  inflating: data_80k/output_images/78083.png  \n",
            "  inflating: data_80k/output_images/78084.png  \n",
            "  inflating: data_80k/output_images/78085.png  \n",
            "  inflating: data_80k/output_images/78086.png  \n",
            "  inflating: data_80k/output_images/78087.png  \n",
            "  inflating: data_80k/output_images/78088.png  \n",
            "  inflating: data_80k/output_images/78089.png  \n",
            "  inflating: data_80k/output_images/7809.png  \n",
            "  inflating: data_80k/output_images/78090.png  \n",
            "  inflating: data_80k/output_images/78091.png  \n",
            "  inflating: data_80k/output_images/78092.png  \n",
            "  inflating: data_80k/output_images/78093.png  \n",
            "  inflating: data_80k/output_images/78094.png  \n",
            "  inflating: data_80k/output_images/78095.png  \n",
            "  inflating: data_80k/output_images/78096.png  \n",
            "  inflating: data_80k/output_images/78097.png  \n",
            "  inflating: data_80k/output_images/78098.png  \n",
            "  inflating: data_80k/output_images/78099.png  \n",
            "  inflating: data_80k/output_images/781.png  \n",
            "  inflating: data_80k/output_images/7810.png  \n",
            "  inflating: data_80k/output_images/78100.png  \n",
            "  inflating: data_80k/output_images/78101.png  \n",
            "  inflating: data_80k/output_images/78102.png  \n",
            "  inflating: data_80k/output_images/78103.png  \n",
            "  inflating: data_80k/output_images/78104.png  \n",
            "  inflating: data_80k/output_images/78105.png  \n",
            "  inflating: data_80k/output_images/78106.png  \n",
            "  inflating: data_80k/output_images/78107.png  \n",
            "  inflating: data_80k/output_images/78108.png  \n",
            "  inflating: data_80k/output_images/78109.png  \n",
            "  inflating: data_80k/output_images/7811.png  \n",
            "  inflating: data_80k/output_images/78110.png  \n",
            "  inflating: data_80k/output_images/78111.png  \n",
            "  inflating: data_80k/output_images/78112.png  \n",
            "  inflating: data_80k/output_images/78113.png  \n",
            "  inflating: data_80k/output_images/78114.png  \n",
            "  inflating: data_80k/output_images/78115.png  \n",
            "  inflating: data_80k/output_images/78116.png  \n",
            "  inflating: data_80k/output_images/78117.png  \n",
            "  inflating: data_80k/output_images/78118.png  \n",
            "  inflating: data_80k/output_images/78119.png  \n",
            "  inflating: data_80k/output_images/7812.png  \n",
            "  inflating: data_80k/output_images/78120.png  \n",
            "  inflating: data_80k/output_images/78121.png  \n",
            "  inflating: data_80k/output_images/78122.png  \n",
            "  inflating: data_80k/output_images/78123.png  \n",
            "  inflating: data_80k/output_images/78124.png  \n",
            "  inflating: data_80k/output_images/78125.png  \n",
            "  inflating: data_80k/output_images/78126.png  \n",
            "  inflating: data_80k/output_images/78127.png  \n",
            "  inflating: data_80k/output_images/78128.png  \n",
            "  inflating: data_80k/output_images/78129.png  \n",
            "  inflating: data_80k/output_images/7813.png  \n",
            "  inflating: data_80k/output_images/78130.png  \n",
            "  inflating: data_80k/output_images/78131.png  \n",
            "  inflating: data_80k/output_images/78132.png  \n",
            "  inflating: data_80k/output_images/78133.png  \n",
            "  inflating: data_80k/output_images/78134.png  \n",
            "  inflating: data_80k/output_images/78135.png  \n",
            "  inflating: data_80k/output_images/78136.png  \n",
            "  inflating: data_80k/output_images/78137.png  \n",
            "  inflating: data_80k/output_images/78138.png  \n",
            "  inflating: data_80k/output_images/78139.png  \n",
            "  inflating: data_80k/output_images/7814.png  \n",
            "  inflating: data_80k/output_images/78140.png  \n",
            "  inflating: data_80k/output_images/78141.png  \n",
            "  inflating: data_80k/output_images/78142.png  \n",
            "  inflating: data_80k/output_images/78143.png  \n",
            "  inflating: data_80k/output_images/78144.png  \n",
            "  inflating: data_80k/output_images/78145.png  \n",
            "  inflating: data_80k/output_images/78146.png  \n",
            "  inflating: data_80k/output_images/78147.png  \n",
            "  inflating: data_80k/output_images/78148.png  \n",
            "  inflating: data_80k/output_images/78149.png  \n",
            "  inflating: data_80k/output_images/7815.png  \n",
            "  inflating: data_80k/output_images/78150.png  \n",
            "  inflating: data_80k/output_images/78151.png  \n",
            "  inflating: data_80k/output_images/78152.png  \n",
            "  inflating: data_80k/output_images/78153.png  \n",
            "  inflating: data_80k/output_images/78154.png  \n",
            "  inflating: data_80k/output_images/78155.png  \n",
            "  inflating: data_80k/output_images/78156.png  \n",
            "  inflating: data_80k/output_images/78157.png  \n",
            "  inflating: data_80k/output_images/78158.png  \n",
            "  inflating: data_80k/output_images/78159.png  \n",
            "  inflating: data_80k/output_images/7816.png  \n",
            "  inflating: data_80k/output_images/78160.png  \n",
            "  inflating: data_80k/output_images/78161.png  \n",
            "  inflating: data_80k/output_images/78162.png  \n",
            "  inflating: data_80k/output_images/78163.png  \n",
            "  inflating: data_80k/output_images/78164.png  \n",
            "  inflating: data_80k/output_images/78165.png  \n",
            "  inflating: data_80k/output_images/78166.png  \n",
            "  inflating: data_80k/output_images/78167.png  \n",
            "  inflating: data_80k/output_images/78168.png  \n",
            "  inflating: data_80k/output_images/78169.png  \n",
            "  inflating: data_80k/output_images/7817.png  \n",
            "  inflating: data_80k/output_images/78170.png  \n",
            "  inflating: data_80k/output_images/78171.png  \n",
            "  inflating: data_80k/output_images/78172.png  \n",
            "  inflating: data_80k/output_images/78173.png  \n",
            "  inflating: data_80k/output_images/78174.png  \n",
            "  inflating: data_80k/output_images/78175.png  \n",
            "  inflating: data_80k/output_images/78176.png  \n",
            "  inflating: data_80k/output_images/78177.png  \n",
            "  inflating: data_80k/output_images/78178.png  \n",
            "  inflating: data_80k/output_images/78179.png  \n",
            "  inflating: data_80k/output_images/7818.png  \n",
            "  inflating: data_80k/output_images/78180.png  \n",
            "  inflating: data_80k/output_images/78181.png  \n",
            "  inflating: data_80k/output_images/78182.png  \n",
            "  inflating: data_80k/output_images/78183.png  \n",
            "  inflating: data_80k/output_images/78184.png  \n",
            "  inflating: data_80k/output_images/78185.png  \n",
            "  inflating: data_80k/output_images/78186.png  \n",
            "  inflating: data_80k/output_images/78187.png  \n",
            "  inflating: data_80k/output_images/78188.png  \n",
            "  inflating: data_80k/output_images/78189.png  \n",
            "  inflating: data_80k/output_images/7819.png  \n",
            "  inflating: data_80k/output_images/78190.png  \n",
            "  inflating: data_80k/output_images/78191.png  \n",
            "  inflating: data_80k/output_images/78192.png  \n",
            "  inflating: data_80k/output_images/78193.png  \n",
            "  inflating: data_80k/output_images/78194.png  \n",
            "  inflating: data_80k/output_images/78195.png  \n",
            "  inflating: data_80k/output_images/78196.png  \n",
            "  inflating: data_80k/output_images/78197.png  \n",
            "  inflating: data_80k/output_images/78198.png  \n",
            "  inflating: data_80k/output_images/78199.png  \n",
            "  inflating: data_80k/output_images/782.png  \n",
            "  inflating: data_80k/output_images/7820.png  \n",
            "  inflating: data_80k/output_images/78200.png  \n",
            "  inflating: data_80k/output_images/78201.png  \n",
            "  inflating: data_80k/output_images/78202.png  \n",
            "  inflating: data_80k/output_images/78203.png  \n",
            "  inflating: data_80k/output_images/78204.png  \n",
            "  inflating: data_80k/output_images/78205.png  \n",
            "  inflating: data_80k/output_images/78206.png  \n",
            "  inflating: data_80k/output_images/78207.png  \n",
            "  inflating: data_80k/output_images/78208.png  \n",
            "  inflating: data_80k/output_images/78209.png  \n",
            "  inflating: data_80k/output_images/7821.png  \n",
            "  inflating: data_80k/output_images/78210.png  \n",
            "  inflating: data_80k/output_images/78211.png  \n",
            "  inflating: data_80k/output_images/78212.png  \n",
            "  inflating: data_80k/output_images/78213.png  \n",
            "  inflating: data_80k/output_images/78214.png  \n",
            "  inflating: data_80k/output_images/78215.png  \n",
            "  inflating: data_80k/output_images/78216.png  \n",
            "  inflating: data_80k/output_images/78217.png  \n",
            "  inflating: data_80k/output_images/78218.png  \n",
            "  inflating: data_80k/output_images/78219.png  \n",
            "  inflating: data_80k/output_images/7822.png  \n",
            "  inflating: data_80k/output_images/78220.png  \n",
            "  inflating: data_80k/output_images/78221.png  \n",
            "  inflating: data_80k/output_images/78222.png  \n",
            "  inflating: data_80k/output_images/78223.png  \n",
            "  inflating: data_80k/output_images/78224.png  \n",
            "  inflating: data_80k/output_images/78225.png  \n",
            "  inflating: data_80k/output_images/78226.png  \n",
            "  inflating: data_80k/output_images/78227.png  \n",
            "  inflating: data_80k/output_images/78228.png  \n",
            "  inflating: data_80k/output_images/78229.png  \n",
            "  inflating: data_80k/output_images/7823.png  \n",
            "  inflating: data_80k/output_images/78230.png  \n",
            "  inflating: data_80k/output_images/78231.png  \n",
            "  inflating: data_80k/output_images/78232.png  \n",
            "  inflating: data_80k/output_images/78233.png  \n",
            "  inflating: data_80k/output_images/78234.png  \n",
            "  inflating: data_80k/output_images/78235.png  \n",
            "  inflating: data_80k/output_images/78236.png  \n",
            "  inflating: data_80k/output_images/78237.png  \n",
            "  inflating: data_80k/output_images/78238.png  \n",
            "  inflating: data_80k/output_images/78239.png  \n",
            "  inflating: data_80k/output_images/7824.png  \n",
            "  inflating: data_80k/output_images/78240.png  \n",
            "  inflating: data_80k/output_images/78241.png  \n",
            "  inflating: data_80k/output_images/78242.png  \n",
            "  inflating: data_80k/output_images/78243.png  \n",
            "  inflating: data_80k/output_images/78244.png  \n",
            "  inflating: data_80k/output_images/78245.png  \n",
            "  inflating: data_80k/output_images/78246.png  \n",
            "  inflating: data_80k/output_images/78247.png  \n",
            "  inflating: data_80k/output_images/78248.png  \n",
            "  inflating: data_80k/output_images/78249.png  \n",
            "  inflating: data_80k/output_images/7825.png  \n",
            "  inflating: data_80k/output_images/78250.png  \n",
            "  inflating: data_80k/output_images/78251.png  \n",
            "  inflating: data_80k/output_images/78252.png  \n",
            "  inflating: data_80k/output_images/78253.png  \n",
            "  inflating: data_80k/output_images/78254.png  \n",
            "  inflating: data_80k/output_images/78255.png  \n",
            "  inflating: data_80k/output_images/78256.png  \n",
            "  inflating: data_80k/output_images/78257.png  \n",
            "  inflating: data_80k/output_images/78258.png  \n",
            "  inflating: data_80k/output_images/78259.png  \n",
            "  inflating: data_80k/output_images/7826.png  \n",
            "  inflating: data_80k/output_images/78260.png  \n",
            "  inflating: data_80k/output_images/78261.png  \n",
            "  inflating: data_80k/output_images/78262.png  \n",
            "  inflating: data_80k/output_images/78263.png  \n",
            "  inflating: data_80k/output_images/78264.png  \n",
            "  inflating: data_80k/output_images/78265.png  \n",
            "  inflating: data_80k/output_images/78266.png  \n",
            "  inflating: data_80k/output_images/78267.png  \n",
            "  inflating: data_80k/output_images/78268.png  \n",
            "  inflating: data_80k/output_images/78269.png  \n",
            "  inflating: data_80k/output_images/7827.png  \n",
            "  inflating: data_80k/output_images/78270.png  \n",
            "  inflating: data_80k/output_images/78271.png  \n",
            "  inflating: data_80k/output_images/78272.png  \n",
            "  inflating: data_80k/output_images/78273.png  \n",
            "  inflating: data_80k/output_images/78274.png  \n",
            "  inflating: data_80k/output_images/78275.png  \n",
            "  inflating: data_80k/output_images/78276.png  \n",
            "  inflating: data_80k/output_images/78277.png  \n",
            "  inflating: data_80k/output_images/78278.png  \n",
            "  inflating: data_80k/output_images/78279.png  \n",
            "  inflating: data_80k/output_images/7828.png  \n",
            "  inflating: data_80k/output_images/78280.png  \n",
            "  inflating: data_80k/output_images/78281.png  \n",
            "  inflating: data_80k/output_images/78282.png  \n",
            "  inflating: data_80k/output_images/78283.png  \n",
            "  inflating: data_80k/output_images/78284.png  \n",
            "  inflating: data_80k/output_images/78285.png  \n",
            "  inflating: data_80k/output_images/78286.png  \n",
            "  inflating: data_80k/output_images/78287.png  \n",
            "  inflating: data_80k/output_images/78288.png  \n",
            "  inflating: data_80k/output_images/78289.png  \n",
            "  inflating: data_80k/output_images/7829.png  \n",
            "  inflating: data_80k/output_images/78290.png  \n",
            "  inflating: data_80k/output_images/78291.png  \n",
            "  inflating: data_80k/output_images/78292.png  \n",
            "  inflating: data_80k/output_images/78293.png  \n",
            "  inflating: data_80k/output_images/78294.png  \n",
            "  inflating: data_80k/output_images/78295.png  \n",
            "  inflating: data_80k/output_images/78296.png  \n",
            "  inflating: data_80k/output_images/78297.png  \n",
            "  inflating: data_80k/output_images/78298.png  \n",
            "  inflating: data_80k/output_images/78299.png  \n",
            "  inflating: data_80k/output_images/783.png  \n",
            "  inflating: data_80k/output_images/7830.png  \n",
            "  inflating: data_80k/output_images/78300.png  \n",
            "  inflating: data_80k/output_images/78301.png  \n",
            "  inflating: data_80k/output_images/78302.png  \n",
            "  inflating: data_80k/output_images/78303.png  \n",
            "  inflating: data_80k/output_images/78304.png  \n",
            "  inflating: data_80k/output_images/78305.png  \n",
            "  inflating: data_80k/output_images/78306.png  \n",
            "  inflating: data_80k/output_images/78307.png  \n",
            "  inflating: data_80k/output_images/78308.png  \n",
            "  inflating: data_80k/output_images/78309.png  \n",
            "  inflating: data_80k/output_images/7831.png  \n",
            "  inflating: data_80k/output_images/78310.png  \n",
            "  inflating: data_80k/output_images/78311.png  \n",
            "  inflating: data_80k/output_images/78312.png  \n",
            "  inflating: data_80k/output_images/78313.png  \n",
            "  inflating: data_80k/output_images/78314.png  \n",
            "  inflating: data_80k/output_images/78315.png  \n",
            "  inflating: data_80k/output_images/78316.png  \n",
            "  inflating: data_80k/output_images/78317.png  \n",
            "  inflating: data_80k/output_images/78318.png  \n",
            "  inflating: data_80k/output_images/78319.png  \n",
            "  inflating: data_80k/output_images/7832.png  \n",
            "  inflating: data_80k/output_images/78320.png  \n",
            "  inflating: data_80k/output_images/78321.png  \n",
            "  inflating: data_80k/output_images/78322.png  \n",
            "  inflating: data_80k/output_images/78323.png  \n",
            "  inflating: data_80k/output_images/78324.png  \n",
            "  inflating: data_80k/output_images/78325.png  \n",
            "  inflating: data_80k/output_images/78326.png  \n",
            "  inflating: data_80k/output_images/78327.png  \n",
            "  inflating: data_80k/output_images/78328.png  \n",
            "  inflating: data_80k/output_images/78329.png  \n",
            "  inflating: data_80k/output_images/7833.png  \n",
            "  inflating: data_80k/output_images/78330.png  \n",
            "  inflating: data_80k/output_images/78331.png  \n",
            "  inflating: data_80k/output_images/78332.png  \n",
            "  inflating: data_80k/output_images/78333.png  \n",
            "  inflating: data_80k/output_images/78334.png  \n",
            "  inflating: data_80k/output_images/78335.png  \n",
            "  inflating: data_80k/output_images/78336.png  \n",
            "  inflating: data_80k/output_images/78337.png  \n",
            "  inflating: data_80k/output_images/78338.png  \n",
            "  inflating: data_80k/output_images/78339.png  \n",
            "  inflating: data_80k/output_images/7834.png  \n",
            "  inflating: data_80k/output_images/78340.png  \n",
            "  inflating: data_80k/output_images/78341.png  \n",
            "  inflating: data_80k/output_images/78342.png  \n",
            "  inflating: data_80k/output_images/78343.png  \n",
            "  inflating: data_80k/output_images/78344.png  \n",
            "  inflating: data_80k/output_images/78345.png  \n",
            "  inflating: data_80k/output_images/78346.png  \n",
            "  inflating: data_80k/output_images/78347.png  \n",
            "  inflating: data_80k/output_images/78348.png  \n",
            "  inflating: data_80k/output_images/78349.png  \n",
            "  inflating: data_80k/output_images/7835.png  \n",
            "  inflating: data_80k/output_images/78350.png  \n",
            "  inflating: data_80k/output_images/78351.png  \n",
            "  inflating: data_80k/output_images/78352.png  \n",
            "  inflating: data_80k/output_images/78353.png  \n",
            "  inflating: data_80k/output_images/78354.png  \n",
            "  inflating: data_80k/output_images/78355.png  \n",
            "  inflating: data_80k/output_images/78356.png  \n",
            "  inflating: data_80k/output_images/78357.png  \n",
            "  inflating: data_80k/output_images/78358.png  \n",
            "  inflating: data_80k/output_images/78359.png  \n",
            "  inflating: data_80k/output_images/7836.png  \n",
            "  inflating: data_80k/output_images/78360.png  \n",
            "  inflating: data_80k/output_images/78361.png  \n",
            "  inflating: data_80k/output_images/78362.png  \n",
            "  inflating: data_80k/output_images/78363.png  \n",
            "  inflating: data_80k/output_images/78364.png  \n",
            "  inflating: data_80k/output_images/78365.png  \n",
            "  inflating: data_80k/output_images/78366.png  \n",
            "  inflating: data_80k/output_images/78367.png  \n",
            "  inflating: data_80k/output_images/78368.png  \n",
            "  inflating: data_80k/output_images/78369.png  \n",
            "  inflating: data_80k/output_images/7837.png  \n",
            "  inflating: data_80k/output_images/78370.png  \n",
            "  inflating: data_80k/output_images/78371.png  \n",
            "  inflating: data_80k/output_images/78372.png  \n",
            "  inflating: data_80k/output_images/78373.png  \n",
            "  inflating: data_80k/output_images/78374.png  \n",
            "  inflating: data_80k/output_images/78375.png  \n",
            "  inflating: data_80k/output_images/78376.png  \n",
            "  inflating: data_80k/output_images/78377.png  \n",
            "  inflating: data_80k/output_images/78378.png  \n",
            "  inflating: data_80k/output_images/78379.png  \n",
            "  inflating: data_80k/output_images/7838.png  \n",
            "  inflating: data_80k/output_images/78380.png  \n",
            "  inflating: data_80k/output_images/78381.png  \n",
            "  inflating: data_80k/output_images/78382.png  \n",
            "  inflating: data_80k/output_images/78383.png  \n",
            "  inflating: data_80k/output_images/78384.png  \n",
            "  inflating: data_80k/output_images/78385.png  \n",
            "  inflating: data_80k/output_images/78386.png  \n",
            "  inflating: data_80k/output_images/78387.png  \n",
            "  inflating: data_80k/output_images/78388.png  \n",
            "  inflating: data_80k/output_images/78389.png  \n",
            "  inflating: data_80k/output_images/7839.png  \n",
            "  inflating: data_80k/output_images/78390.png  \n",
            "  inflating: data_80k/output_images/78391.png  \n",
            "  inflating: data_80k/output_images/78392.png  \n",
            "  inflating: data_80k/output_images/78393.png  \n",
            "  inflating: data_80k/output_images/78394.png  \n",
            "  inflating: data_80k/output_images/78395.png  \n",
            "  inflating: data_80k/output_images/78396.png  \n",
            "  inflating: data_80k/output_images/78397.png  \n",
            "  inflating: data_80k/output_images/78398.png  \n",
            "  inflating: data_80k/output_images/78399.png  \n",
            "  inflating: data_80k/output_images/784.png  \n",
            "  inflating: data_80k/output_images/7840.png  \n",
            "  inflating: data_80k/output_images/78400.png  \n",
            "  inflating: data_80k/output_images/78401.png  \n",
            "  inflating: data_80k/output_images/78402.png  \n",
            "  inflating: data_80k/output_images/78403.png  \n",
            "  inflating: data_80k/output_images/78404.png  \n",
            "  inflating: data_80k/output_images/78405.png  \n",
            "  inflating: data_80k/output_images/78406.png  \n",
            "  inflating: data_80k/output_images/78407.png  \n",
            "  inflating: data_80k/output_images/78408.png  \n",
            "  inflating: data_80k/output_images/78409.png  \n",
            "  inflating: data_80k/output_images/7841.png  \n",
            "  inflating: data_80k/output_images/78410.png  \n",
            "  inflating: data_80k/output_images/78411.png  \n",
            "  inflating: data_80k/output_images/78412.png  \n",
            "  inflating: data_80k/output_images/78413.png  \n",
            "  inflating: data_80k/output_images/78414.png  \n",
            "  inflating: data_80k/output_images/78415.png  \n",
            "  inflating: data_80k/output_images/78416.png  \n",
            "  inflating: data_80k/output_images/78417.png  \n",
            "  inflating: data_80k/output_images/78418.png  \n",
            "  inflating: data_80k/output_images/78419.png  \n",
            "  inflating: data_80k/output_images/7842.png  \n",
            "  inflating: data_80k/output_images/78420.png  \n",
            "  inflating: data_80k/output_images/78421.png  \n",
            "  inflating: data_80k/output_images/78422.png  \n",
            "  inflating: data_80k/output_images/78423.png  \n",
            "  inflating: data_80k/output_images/78424.png  \n",
            "  inflating: data_80k/output_images/78425.png  \n",
            "  inflating: data_80k/output_images/78426.png  \n",
            "  inflating: data_80k/output_images/78427.png  \n",
            "  inflating: data_80k/output_images/78428.png  \n",
            "  inflating: data_80k/output_images/78429.png  \n",
            "  inflating: data_80k/output_images/7843.png  \n",
            "  inflating: data_80k/output_images/78430.png  \n",
            "  inflating: data_80k/output_images/78431.png  \n",
            "  inflating: data_80k/output_images/78432.png  \n",
            "  inflating: data_80k/output_images/78433.png  \n",
            "  inflating: data_80k/output_images/78434.png  \n",
            "  inflating: data_80k/output_images/78435.png  \n",
            "  inflating: data_80k/output_images/78436.png  \n",
            "  inflating: data_80k/output_images/78437.png  \n",
            "  inflating: data_80k/output_images/78438.png  \n",
            "  inflating: data_80k/output_images/78439.png  \n",
            "  inflating: data_80k/output_images/7844.png  \n",
            "  inflating: data_80k/output_images/78440.png  \n",
            "  inflating: data_80k/output_images/78441.png  \n",
            "  inflating: data_80k/output_images/78442.png  \n",
            "  inflating: data_80k/output_images/78443.png  \n",
            "  inflating: data_80k/output_images/78444.png  \n",
            "  inflating: data_80k/output_images/78445.png  \n",
            "  inflating: data_80k/output_images/78446.png  \n",
            "  inflating: data_80k/output_images/78447.png  \n",
            "  inflating: data_80k/output_images/78448.png  \n",
            "  inflating: data_80k/output_images/78449.png  \n",
            "  inflating: data_80k/output_images/7845.png  \n",
            "  inflating: data_80k/output_images/78450.png  \n",
            "  inflating: data_80k/output_images/78451.png  \n",
            "  inflating: data_80k/output_images/78452.png  \n",
            "  inflating: data_80k/output_images/78453.png  \n",
            "  inflating: data_80k/output_images/78454.png  \n",
            "  inflating: data_80k/output_images/78455.png  \n",
            "  inflating: data_80k/output_images/78456.png  \n",
            "  inflating: data_80k/output_images/78457.png  \n",
            "  inflating: data_80k/output_images/78458.png  \n",
            "  inflating: data_80k/output_images/78459.png  \n",
            "  inflating: data_80k/output_images/7846.png  \n",
            "  inflating: data_80k/output_images/78460.png  \n",
            "  inflating: data_80k/output_images/78461.png  \n",
            "  inflating: data_80k/output_images/78462.png  \n",
            "  inflating: data_80k/output_images/78463.png  \n",
            "  inflating: data_80k/output_images/78464.png  \n",
            "  inflating: data_80k/output_images/78465.png  \n",
            "  inflating: data_80k/output_images/78466.png  \n",
            "  inflating: data_80k/output_images/78467.png  \n",
            "  inflating: data_80k/output_images/78468.png  \n",
            "  inflating: data_80k/output_images/78469.png  \n",
            "  inflating: data_80k/output_images/7847.png  \n",
            "  inflating: data_80k/output_images/78470.png  \n",
            "  inflating: data_80k/output_images/78471.png  \n",
            "  inflating: data_80k/output_images/78472.png  \n",
            "  inflating: data_80k/output_images/78473.png  \n",
            "  inflating: data_80k/output_images/78474.png  \n",
            "  inflating: data_80k/output_images/78475.png  \n",
            "  inflating: data_80k/output_images/78476.png  \n",
            "  inflating: data_80k/output_images/78477.png  \n",
            "  inflating: data_80k/output_images/78478.png  \n",
            "  inflating: data_80k/output_images/78479.png  \n",
            "  inflating: data_80k/output_images/7848.png  \n",
            "  inflating: data_80k/output_images/78480.png  \n",
            "  inflating: data_80k/output_images/78481.png  \n",
            "  inflating: data_80k/output_images/78482.png  \n",
            "  inflating: data_80k/output_images/78483.png  \n",
            "  inflating: data_80k/output_images/78484.png  \n",
            "  inflating: data_80k/output_images/78485.png  \n",
            "  inflating: data_80k/output_images/78486.png  \n",
            "  inflating: data_80k/output_images/78487.png  \n",
            "  inflating: data_80k/output_images/78488.png  \n",
            "  inflating: data_80k/output_images/78489.png  \n",
            "  inflating: data_80k/output_images/7849.png  \n",
            "  inflating: data_80k/output_images/78490.png  \n",
            "  inflating: data_80k/output_images/78491.png  \n",
            "  inflating: data_80k/output_images/78492.png  \n",
            "  inflating: data_80k/output_images/78493.png  \n",
            "  inflating: data_80k/output_images/78494.png  \n",
            "  inflating: data_80k/output_images/78495.png  \n",
            "  inflating: data_80k/output_images/78496.png  \n",
            "  inflating: data_80k/output_images/78497.png  \n",
            "  inflating: data_80k/output_images/78498.png  \n",
            "  inflating: data_80k/output_images/78499.png  \n",
            "  inflating: data_80k/output_images/785.png  \n",
            "  inflating: data_80k/output_images/7850.png  \n",
            "  inflating: data_80k/output_images/78500.png  \n",
            "  inflating: data_80k/output_images/78501.png  \n",
            "  inflating: data_80k/output_images/78502.png  \n",
            "  inflating: data_80k/output_images/78503.png  \n",
            "  inflating: data_80k/output_images/78504.png  \n",
            "  inflating: data_80k/output_images/78505.png  \n",
            "  inflating: data_80k/output_images/78506.png  \n",
            "  inflating: data_80k/output_images/78507.png  \n",
            "  inflating: data_80k/output_images/78508.png  \n",
            "  inflating: data_80k/output_images/78509.png  \n",
            "  inflating: data_80k/output_images/7851.png  \n",
            "  inflating: data_80k/output_images/78510.png  \n",
            "  inflating: data_80k/output_images/78511.png  \n",
            "  inflating: data_80k/output_images/78512.png  \n",
            "  inflating: data_80k/output_images/78513.png  \n",
            "  inflating: data_80k/output_images/78514.png  \n",
            "  inflating: data_80k/output_images/78515.png  \n",
            "  inflating: data_80k/output_images/78516.png  \n",
            "  inflating: data_80k/output_images/78517.png  \n",
            "  inflating: data_80k/output_images/78518.png  \n",
            "  inflating: data_80k/output_images/78519.png  \n",
            "  inflating: data_80k/output_images/7852.png  \n",
            "  inflating: data_80k/output_images/78520.png  \n",
            "  inflating: data_80k/output_images/78521.png  \n",
            "  inflating: data_80k/output_images/78522.png  \n",
            "  inflating: data_80k/output_images/78523.png  \n",
            "  inflating: data_80k/output_images/78524.png  \n",
            "  inflating: data_80k/output_images/78525.png  \n",
            "  inflating: data_80k/output_images/78526.png  \n",
            "  inflating: data_80k/output_images/78527.png  \n",
            "  inflating: data_80k/output_images/78528.png  \n",
            "  inflating: data_80k/output_images/78529.png  \n",
            "  inflating: data_80k/output_images/7853.png  \n",
            "  inflating: data_80k/output_images/78530.png  \n",
            "  inflating: data_80k/output_images/78531.png  \n",
            "  inflating: data_80k/output_images/78532.png  \n",
            "  inflating: data_80k/output_images/78533.png  \n",
            "  inflating: data_80k/output_images/78534.png  \n",
            "  inflating: data_80k/output_images/78535.png  \n",
            "  inflating: data_80k/output_images/78536.png  \n",
            "  inflating: data_80k/output_images/78537.png  \n",
            "  inflating: data_80k/output_images/78538.png  \n",
            "  inflating: data_80k/output_images/78539.png  \n",
            "  inflating: data_80k/output_images/7854.png  \n",
            "  inflating: data_80k/output_images/78540.png  \n",
            "  inflating: data_80k/output_images/78541.png  \n",
            "  inflating: data_80k/output_images/78542.png  \n",
            "  inflating: data_80k/output_images/78543.png  \n",
            "  inflating: data_80k/output_images/78544.png  \n",
            "  inflating: data_80k/output_images/78545.png  \n",
            "  inflating: data_80k/output_images/78546.png  \n",
            "  inflating: data_80k/output_images/78547.png  \n",
            "  inflating: data_80k/output_images/78548.png  \n",
            "  inflating: data_80k/output_images/78549.png  \n",
            "  inflating: data_80k/output_images/7855.png  \n",
            "  inflating: data_80k/output_images/78550.png  \n",
            "  inflating: data_80k/output_images/78551.png  \n",
            "  inflating: data_80k/output_images/78552.png  \n",
            "  inflating: data_80k/output_images/78553.png  \n",
            "  inflating: data_80k/output_images/78554.png  \n",
            "  inflating: data_80k/output_images/78555.png  \n",
            "  inflating: data_80k/output_images/78556.png  \n",
            "  inflating: data_80k/output_images/78557.png  \n",
            "  inflating: data_80k/output_images/78558.png  \n",
            "  inflating: data_80k/output_images/78559.png  \n",
            "  inflating: data_80k/output_images/7856.png  \n",
            "  inflating: data_80k/output_images/78560.png  \n",
            "  inflating: data_80k/output_images/78561.png  \n",
            "  inflating: data_80k/output_images/78562.png  \n",
            "  inflating: data_80k/output_images/78563.png  \n",
            "  inflating: data_80k/output_images/78564.png  \n",
            "  inflating: data_80k/output_images/78565.png  \n",
            "  inflating: data_80k/output_images/78566.png  \n",
            "  inflating: data_80k/output_images/78567.png  \n",
            "  inflating: data_80k/output_images/78568.png  \n",
            "  inflating: data_80k/output_images/78569.png  \n",
            "  inflating: data_80k/output_images/7857.png  \n",
            "  inflating: data_80k/output_images/78570.png  \n",
            "  inflating: data_80k/output_images/78571.png  \n",
            "  inflating: data_80k/output_images/78572.png  \n",
            "  inflating: data_80k/output_images/78573.png  \n",
            "  inflating: data_80k/output_images/78574.png  \n",
            "  inflating: data_80k/output_images/78575.png  \n",
            "  inflating: data_80k/output_images/78576.png  \n",
            "  inflating: data_80k/output_images/78577.png  \n",
            "  inflating: data_80k/output_images/78578.png  \n",
            "  inflating: data_80k/output_images/78579.png  \n",
            "  inflating: data_80k/output_images/7858.png  \n",
            "  inflating: data_80k/output_images/78580.png  \n",
            "  inflating: data_80k/output_images/78581.png  \n",
            "  inflating: data_80k/output_images/78582.png  \n",
            "  inflating: data_80k/output_images/78583.png  \n",
            "  inflating: data_80k/output_images/78584.png  \n",
            "  inflating: data_80k/output_images/78585.png  \n",
            "  inflating: data_80k/output_images/78586.png  \n",
            "  inflating: data_80k/output_images/78587.png  \n",
            "  inflating: data_80k/output_images/78588.png  \n",
            "  inflating: data_80k/output_images/78589.png  \n",
            "  inflating: data_80k/output_images/7859.png  \n",
            "  inflating: data_80k/output_images/78590.png  \n",
            "  inflating: data_80k/output_images/78591.png  \n",
            "  inflating: data_80k/output_images/78592.png  \n",
            "  inflating: data_80k/output_images/78593.png  \n",
            "  inflating: data_80k/output_images/78594.png  \n",
            "  inflating: data_80k/output_images/78595.png  \n",
            "  inflating: data_80k/output_images/78596.png  \n",
            "  inflating: data_80k/output_images/78597.png  \n",
            "  inflating: data_80k/output_images/78598.png  \n",
            "  inflating: data_80k/output_images/78599.png  \n",
            "  inflating: data_80k/output_images/786.png  \n",
            "  inflating: data_80k/output_images/7860.png  \n",
            "  inflating: data_80k/output_images/78600.png  \n",
            "  inflating: data_80k/output_images/78601.png  \n",
            "  inflating: data_80k/output_images/78602.png  \n",
            "  inflating: data_80k/output_images/78603.png  \n",
            "  inflating: data_80k/output_images/78604.png  \n",
            "  inflating: data_80k/output_images/78605.png  \n",
            "  inflating: data_80k/output_images/78606.png  \n",
            "  inflating: data_80k/output_images/78607.png  \n",
            "  inflating: data_80k/output_images/78608.png  \n",
            "  inflating: data_80k/output_images/78609.png  \n",
            "  inflating: data_80k/output_images/7861.png  \n",
            "  inflating: data_80k/output_images/78610.png  \n",
            "  inflating: data_80k/output_images/78611.png  \n",
            "  inflating: data_80k/output_images/78612.png  \n",
            "  inflating: data_80k/output_images/78613.png  \n",
            "  inflating: data_80k/output_images/78614.png  \n",
            "  inflating: data_80k/output_images/78615.png  \n",
            "  inflating: data_80k/output_images/78616.png  \n",
            "  inflating: data_80k/output_images/78617.png  \n",
            "  inflating: data_80k/output_images/78618.png  \n",
            "  inflating: data_80k/output_images/78619.png  \n",
            "  inflating: data_80k/output_images/7862.png  \n",
            "  inflating: data_80k/output_images/78620.png  \n",
            "  inflating: data_80k/output_images/78621.png  \n",
            "  inflating: data_80k/output_images/78622.png  \n",
            "  inflating: data_80k/output_images/78623.png  \n",
            "  inflating: data_80k/output_images/78624.png  \n",
            "  inflating: data_80k/output_images/78625.png  \n",
            "  inflating: data_80k/output_images/78626.png  \n",
            "  inflating: data_80k/output_images/78627.png  \n",
            "  inflating: data_80k/output_images/78628.png  \n",
            "  inflating: data_80k/output_images/78629.png  \n",
            "  inflating: data_80k/output_images/7863.png  \n",
            "  inflating: data_80k/output_images/78630.png  \n",
            "  inflating: data_80k/output_images/78631.png  \n",
            "  inflating: data_80k/output_images/78632.png  \n",
            "  inflating: data_80k/output_images/78633.png  \n",
            "  inflating: data_80k/output_images/78634.png  \n",
            "  inflating: data_80k/output_images/78635.png  \n",
            "  inflating: data_80k/output_images/78636.png  \n",
            "  inflating: data_80k/output_images/78637.png  \n",
            "  inflating: data_80k/output_images/78638.png  \n",
            "  inflating: data_80k/output_images/78639.png  \n",
            "  inflating: data_80k/output_images/7864.png  \n",
            "  inflating: data_80k/output_images/78640.png  \n",
            "  inflating: data_80k/output_images/78641.png  \n",
            "  inflating: data_80k/output_images/78642.png  \n",
            "  inflating: data_80k/output_images/78643.png  \n",
            "  inflating: data_80k/output_images/78644.png  \n",
            "  inflating: data_80k/output_images/78645.png  \n",
            "  inflating: data_80k/output_images/78646.png  \n",
            "  inflating: data_80k/output_images/78647.png  \n",
            "  inflating: data_80k/output_images/78648.png  \n",
            "  inflating: data_80k/output_images/78649.png  \n",
            "  inflating: data_80k/output_images/7865.png  \n",
            "  inflating: data_80k/output_images/78650.png  \n",
            "  inflating: data_80k/output_images/78651.png  \n",
            "  inflating: data_80k/output_images/78652.png  \n",
            "  inflating: data_80k/output_images/78653.png  \n",
            "  inflating: data_80k/output_images/78654.png  \n",
            "  inflating: data_80k/output_images/78655.png  \n",
            "  inflating: data_80k/output_images/78656.png  \n",
            "  inflating: data_80k/output_images/78657.png  \n",
            "  inflating: data_80k/output_images/78658.png  \n",
            "  inflating: data_80k/output_images/78659.png  \n",
            "  inflating: data_80k/output_images/7866.png  \n",
            "  inflating: data_80k/output_images/78660.png  \n",
            "  inflating: data_80k/output_images/78661.png  \n",
            "  inflating: data_80k/output_images/78662.png  \n",
            "  inflating: data_80k/output_images/78663.png  \n",
            "  inflating: data_80k/output_images/78664.png  \n",
            "  inflating: data_80k/output_images/78665.png  \n",
            "  inflating: data_80k/output_images/78666.png  \n",
            "  inflating: data_80k/output_images/78667.png  \n",
            "  inflating: data_80k/output_images/78668.png  \n",
            "  inflating: data_80k/output_images/78669.png  \n",
            "  inflating: data_80k/output_images/7867.png  \n",
            "  inflating: data_80k/output_images/78670.png  \n",
            "  inflating: data_80k/output_images/78671.png  \n",
            "  inflating: data_80k/output_images/78672.png  \n",
            "  inflating: data_80k/output_images/78673.png  \n",
            "  inflating: data_80k/output_images/78674.png  \n",
            "  inflating: data_80k/output_images/78675.png  \n",
            "  inflating: data_80k/output_images/78676.png  \n",
            "  inflating: data_80k/output_images/78677.png  \n",
            "  inflating: data_80k/output_images/78678.png  \n",
            "  inflating: data_80k/output_images/78679.png  \n",
            "  inflating: data_80k/output_images/7868.png  \n",
            "  inflating: data_80k/output_images/78680.png  \n",
            "  inflating: data_80k/output_images/78681.png  \n",
            "  inflating: data_80k/output_images/78682.png  \n",
            "  inflating: data_80k/output_images/78683.png  \n",
            "  inflating: data_80k/output_images/78684.png  \n",
            "  inflating: data_80k/output_images/78685.png  \n",
            "  inflating: data_80k/output_images/78686.png  \n",
            "  inflating: data_80k/output_images/78687.png  \n",
            "  inflating: data_80k/output_images/78688.png  \n",
            "  inflating: data_80k/output_images/78689.png  \n",
            "  inflating: data_80k/output_images/7869.png  \n",
            "  inflating: data_80k/output_images/78690.png  \n",
            "  inflating: data_80k/output_images/78691.png  \n",
            "  inflating: data_80k/output_images/78692.png  \n",
            "  inflating: data_80k/output_images/78693.png  \n",
            "  inflating: data_80k/output_images/78694.png  \n",
            "  inflating: data_80k/output_images/78695.png  \n",
            "  inflating: data_80k/output_images/78696.png  \n",
            "  inflating: data_80k/output_images/78697.png  \n",
            "  inflating: data_80k/output_images/78698.png  \n",
            "  inflating: data_80k/output_images/78699.png  \n",
            "  inflating: data_80k/output_images/787.png  \n",
            "  inflating: data_80k/output_images/7870.png  \n",
            "  inflating: data_80k/output_images/78700.png  \n",
            "  inflating: data_80k/output_images/78701.png  \n",
            "  inflating: data_80k/output_images/78702.png  \n",
            "  inflating: data_80k/output_images/78703.png  \n",
            "  inflating: data_80k/output_images/78704.png  \n",
            "  inflating: data_80k/output_images/78705.png  \n",
            "  inflating: data_80k/output_images/78706.png  \n",
            "  inflating: data_80k/output_images/78707.png  \n",
            "  inflating: data_80k/output_images/78708.png  \n",
            "  inflating: data_80k/output_images/78709.png  \n",
            "  inflating: data_80k/output_images/7871.png  \n",
            "  inflating: data_80k/output_images/78710.png  \n",
            "  inflating: data_80k/output_images/78711.png  \n",
            "  inflating: data_80k/output_images/78712.png  \n",
            "  inflating: data_80k/output_images/78713.png  \n",
            "  inflating: data_80k/output_images/78714.png  \n",
            "  inflating: data_80k/output_images/78715.png  \n",
            "  inflating: data_80k/output_images/78716.png  \n",
            "  inflating: data_80k/output_images/78717.png  \n",
            "  inflating: data_80k/output_images/78718.png  \n",
            "  inflating: data_80k/output_images/78719.png  \n",
            "  inflating: data_80k/output_images/7872.png  \n",
            "  inflating: data_80k/output_images/78720.png  \n",
            "  inflating: data_80k/output_images/78721.png  \n",
            "  inflating: data_80k/output_images/78722.png  \n",
            "  inflating: data_80k/output_images/78723.png  \n",
            "  inflating: data_80k/output_images/78724.png  \n",
            "  inflating: data_80k/output_images/78725.png  \n",
            "  inflating: data_80k/output_images/78726.png  \n",
            "  inflating: data_80k/output_images/78727.png  \n",
            "  inflating: data_80k/output_images/78728.png  \n",
            "  inflating: data_80k/output_images/78729.png  \n",
            "  inflating: data_80k/output_images/7873.png  \n",
            "  inflating: data_80k/output_images/78730.png  \n",
            "  inflating: data_80k/output_images/78731.png  \n",
            "  inflating: data_80k/output_images/78732.png  \n",
            "  inflating: data_80k/output_images/78733.png  \n",
            "  inflating: data_80k/output_images/78734.png  \n",
            "  inflating: data_80k/output_images/78735.png  \n",
            "  inflating: data_80k/output_images/78736.png  \n",
            "  inflating: data_80k/output_images/78737.png  \n",
            "  inflating: data_80k/output_images/78738.png  \n",
            "  inflating: data_80k/output_images/78739.png  \n",
            "  inflating: data_80k/output_images/7874.png  \n",
            "  inflating: data_80k/output_images/78740.png  \n",
            "  inflating: data_80k/output_images/78741.png  \n",
            "  inflating: data_80k/output_images/78742.png  \n",
            "  inflating: data_80k/output_images/78743.png  \n",
            "  inflating: data_80k/output_images/78744.png  \n",
            "  inflating: data_80k/output_images/78745.png  \n",
            "  inflating: data_80k/output_images/78746.png  \n",
            "  inflating: data_80k/output_images/78747.png  \n",
            "  inflating: data_80k/output_images/78748.png  \n",
            "  inflating: data_80k/output_images/78749.png  \n",
            "  inflating: data_80k/output_images/7875.png  \n",
            "  inflating: data_80k/output_images/78750.png  \n",
            "  inflating: data_80k/output_images/78751.png  \n",
            "  inflating: data_80k/output_images/78752.png  \n",
            "  inflating: data_80k/output_images/78753.png  \n",
            "  inflating: data_80k/output_images/78754.png  \n",
            "  inflating: data_80k/output_images/78755.png  \n",
            "  inflating: data_80k/output_images/78756.png  \n",
            "  inflating: data_80k/output_images/78757.png  \n",
            "  inflating: data_80k/output_images/78758.png  \n",
            "  inflating: data_80k/output_images/78759.png  \n",
            "  inflating: data_80k/output_images/7876.png  \n",
            "  inflating: data_80k/output_images/78760.png  \n",
            "  inflating: data_80k/output_images/78761.png  \n",
            "  inflating: data_80k/output_images/78762.png  \n",
            "  inflating: data_80k/output_images/78763.png  \n",
            "  inflating: data_80k/output_images/78764.png  \n",
            "  inflating: data_80k/output_images/78765.png  \n",
            "  inflating: data_80k/output_images/78766.png  \n",
            "  inflating: data_80k/output_images/78767.png  \n",
            "  inflating: data_80k/output_images/78768.png  \n",
            "  inflating: data_80k/output_images/78769.png  \n",
            "  inflating: data_80k/output_images/7877.png  \n",
            "  inflating: data_80k/output_images/78770.png  \n",
            "  inflating: data_80k/output_images/78771.png  \n",
            "  inflating: data_80k/output_images/78772.png  \n",
            "  inflating: data_80k/output_images/78773.png  \n",
            "  inflating: data_80k/output_images/78774.png  \n",
            "  inflating: data_80k/output_images/78775.png  \n",
            "  inflating: data_80k/output_images/78776.png  \n",
            "  inflating: data_80k/output_images/78777.png  \n",
            "  inflating: data_80k/output_images/78778.png  \n",
            "  inflating: data_80k/output_images/78779.png  \n",
            "  inflating: data_80k/output_images/7878.png  \n",
            "  inflating: data_80k/output_images/78780.png  \n",
            "  inflating: data_80k/output_images/78781.png  \n",
            "  inflating: data_80k/output_images/78782.png  \n",
            "  inflating: data_80k/output_images/78783.png  \n",
            "  inflating: data_80k/output_images/78784.png  \n",
            "  inflating: data_80k/output_images/78785.png  \n",
            "  inflating: data_80k/output_images/78786.png  \n",
            "  inflating: data_80k/output_images/78787.png  \n",
            "  inflating: data_80k/output_images/78788.png  \n",
            "  inflating: data_80k/output_images/78789.png  \n",
            "  inflating: data_80k/output_images/7879.png  \n",
            "  inflating: data_80k/output_images/78790.png  \n",
            "  inflating: data_80k/output_images/78791.png  \n",
            "  inflating: data_80k/output_images/78792.png  \n",
            "  inflating: data_80k/output_images/78793.png  \n",
            "  inflating: data_80k/output_images/78794.png  \n",
            "  inflating: data_80k/output_images/78795.png  \n",
            "  inflating: data_80k/output_images/78796.png  \n",
            "  inflating: data_80k/output_images/78797.png  \n",
            "  inflating: data_80k/output_images/78798.png  \n",
            "  inflating: data_80k/output_images/78799.png  \n",
            "  inflating: data_80k/output_images/788.png  \n",
            "  inflating: data_80k/output_images/7880.png  \n",
            "  inflating: data_80k/output_images/78800.png  \n",
            "  inflating: data_80k/output_images/78801.png  \n",
            "  inflating: data_80k/output_images/78802.png  \n",
            "  inflating: data_80k/output_images/78803.png  \n",
            "  inflating: data_80k/output_images/78804.png  \n",
            "  inflating: data_80k/output_images/78805.png  \n",
            "  inflating: data_80k/output_images/78806.png  \n",
            "  inflating: data_80k/output_images/78807.png  \n",
            "  inflating: data_80k/output_images/78808.png  \n",
            "  inflating: data_80k/output_images/78809.png  \n",
            "  inflating: data_80k/output_images/7881.png  \n",
            "  inflating: data_80k/output_images/78810.png  \n",
            "  inflating: data_80k/output_images/78811.png  \n",
            "  inflating: data_80k/output_images/78812.png  \n",
            "  inflating: data_80k/output_images/78813.png  \n",
            "  inflating: data_80k/output_images/78814.png  \n",
            "  inflating: data_80k/output_images/78815.png  \n",
            "  inflating: data_80k/output_images/78816.png  \n",
            "  inflating: data_80k/output_images/78817.png  \n",
            "  inflating: data_80k/output_images/78818.png  \n",
            "  inflating: data_80k/output_images/78819.png  \n",
            "  inflating: data_80k/output_images/7882.png  \n",
            "  inflating: data_80k/output_images/78820.png  \n",
            "  inflating: data_80k/output_images/78821.png  \n",
            "  inflating: data_80k/output_images/78822.png  \n",
            "  inflating: data_80k/output_images/78823.png  \n",
            "  inflating: data_80k/output_images/78824.png  \n",
            "  inflating: data_80k/output_images/78825.png  \n",
            "  inflating: data_80k/output_images/78826.png  \n",
            "  inflating: data_80k/output_images/78827.png  \n",
            "  inflating: data_80k/output_images/78828.png  \n",
            "  inflating: data_80k/output_images/78829.png  \n",
            "  inflating: data_80k/output_images/7883.png  \n",
            "  inflating: data_80k/output_images/78830.png  \n",
            "  inflating: data_80k/output_images/78831.png  \n",
            "  inflating: data_80k/output_images/78832.png  \n",
            "  inflating: data_80k/output_images/78833.png  \n",
            "  inflating: data_80k/output_images/78834.png  \n",
            "  inflating: data_80k/output_images/78835.png  \n",
            "  inflating: data_80k/output_images/78836.png  \n",
            "  inflating: data_80k/output_images/78837.png  \n",
            "  inflating: data_80k/output_images/78838.png  \n",
            "  inflating: data_80k/output_images/78839.png  \n",
            "  inflating: data_80k/output_images/7884.png  \n",
            "  inflating: data_80k/output_images/78840.png  \n",
            "  inflating: data_80k/output_images/78841.png  \n",
            "  inflating: data_80k/output_images/78842.png  \n",
            "  inflating: data_80k/output_images/78843.png  \n",
            "  inflating: data_80k/output_images/78844.png  \n",
            "  inflating: data_80k/output_images/78845.png  \n",
            "  inflating: data_80k/output_images/78846.png  \n",
            "  inflating: data_80k/output_images/78847.png  \n",
            "  inflating: data_80k/output_images/78848.png  \n",
            "  inflating: data_80k/output_images/78849.png  \n",
            "  inflating: data_80k/output_images/7885.png  \n",
            "  inflating: data_80k/output_images/78850.png  \n",
            "  inflating: data_80k/output_images/78851.png  \n",
            "  inflating: data_80k/output_images/78852.png  \n",
            "  inflating: data_80k/output_images/78853.png  \n",
            "  inflating: data_80k/output_images/78854.png  \n",
            "  inflating: data_80k/output_images/78855.png  \n",
            "  inflating: data_80k/output_images/78856.png  \n",
            "  inflating: data_80k/output_images/78857.png  \n",
            "  inflating: data_80k/output_images/78858.png  \n",
            "  inflating: data_80k/output_images/78859.png  \n",
            "  inflating: data_80k/output_images/7886.png  \n",
            "  inflating: data_80k/output_images/78860.png  \n",
            "  inflating: data_80k/output_images/78861.png  \n",
            "  inflating: data_80k/output_images/78862.png  \n",
            "  inflating: data_80k/output_images/78863.png  \n",
            "  inflating: data_80k/output_images/78864.png  \n",
            "  inflating: data_80k/output_images/78865.png  \n",
            "  inflating: data_80k/output_images/78866.png  \n",
            "  inflating: data_80k/output_images/78867.png  \n",
            "  inflating: data_80k/output_images/78868.png  \n",
            "  inflating: data_80k/output_images/78869.png  \n",
            "  inflating: data_80k/output_images/7887.png  \n",
            "  inflating: data_80k/output_images/78870.png  \n",
            "  inflating: data_80k/output_images/78871.png  \n",
            "  inflating: data_80k/output_images/78872.png  \n",
            "  inflating: data_80k/output_images/78873.png  \n",
            "  inflating: data_80k/output_images/78874.png  \n",
            "  inflating: data_80k/output_images/78875.png  \n",
            "  inflating: data_80k/output_images/78876.png  \n",
            "  inflating: data_80k/output_images/78877.png  \n",
            "  inflating: data_80k/output_images/78878.png  \n",
            "  inflating: data_80k/output_images/78879.png  \n",
            "  inflating: data_80k/output_images/7888.png  \n",
            "  inflating: data_80k/output_images/78880.png  \n",
            "  inflating: data_80k/output_images/78881.png  \n",
            "  inflating: data_80k/output_images/78882.png  \n",
            "  inflating: data_80k/output_images/78883.png  \n",
            "  inflating: data_80k/output_images/78884.png  \n",
            "  inflating: data_80k/output_images/78885.png  \n",
            "  inflating: data_80k/output_images/78886.png  \n",
            "  inflating: data_80k/output_images/78887.png  \n",
            "  inflating: data_80k/output_images/78888.png  \n",
            "  inflating: data_80k/output_images/78889.png  \n",
            "  inflating: data_80k/output_images/7889.png  \n",
            "  inflating: data_80k/output_images/78890.png  \n",
            "  inflating: data_80k/output_images/78891.png  \n",
            "  inflating: data_80k/output_images/78892.png  \n",
            "  inflating: data_80k/output_images/78893.png  \n",
            "  inflating: data_80k/output_images/78894.png  \n",
            "  inflating: data_80k/output_images/78895.png  \n",
            "  inflating: data_80k/output_images/78896.png  \n",
            "  inflating: data_80k/output_images/78897.png  \n",
            "  inflating: data_80k/output_images/78898.png  \n",
            "  inflating: data_80k/output_images/78899.png  \n",
            "  inflating: data_80k/output_images/789.png  \n",
            "  inflating: data_80k/output_images/7890.png  \n",
            "  inflating: data_80k/output_images/78900.png  \n",
            "  inflating: data_80k/output_images/78901.png  \n",
            "  inflating: data_80k/output_images/78902.png  \n",
            "  inflating: data_80k/output_images/78903.png  \n",
            "  inflating: data_80k/output_images/78904.png  \n",
            "  inflating: data_80k/output_images/78905.png  \n",
            "  inflating: data_80k/output_images/78906.png  \n",
            "  inflating: data_80k/output_images/78907.png  \n",
            "  inflating: data_80k/output_images/78908.png  \n",
            "  inflating: data_80k/output_images/78909.png  \n",
            "  inflating: data_80k/output_images/7891.png  \n",
            "  inflating: data_80k/output_images/78910.png  \n",
            "  inflating: data_80k/output_images/78911.png  \n",
            "  inflating: data_80k/output_images/78912.png  \n",
            "  inflating: data_80k/output_images/78913.png  \n",
            "  inflating: data_80k/output_images/78914.png  \n",
            "  inflating: data_80k/output_images/78915.png  \n",
            "  inflating: data_80k/output_images/78916.png  \n",
            "  inflating: data_80k/output_images/78917.png  \n",
            "  inflating: data_80k/output_images/78918.png  \n",
            "  inflating: data_80k/output_images/78919.png  \n",
            "  inflating: data_80k/output_images/7892.png  \n",
            "  inflating: data_80k/output_images/78920.png  \n",
            "  inflating: data_80k/output_images/78921.png  \n",
            "  inflating: data_80k/output_images/78922.png  \n",
            "  inflating: data_80k/output_images/78923.png  \n",
            "  inflating: data_80k/output_images/78924.png  \n",
            "  inflating: data_80k/output_images/78925.png  \n",
            "  inflating: data_80k/output_images/78926.png  \n",
            "  inflating: data_80k/output_images/78927.png  \n",
            "  inflating: data_80k/output_images/78928.png  \n",
            "  inflating: data_80k/output_images/78929.png  \n",
            "  inflating: data_80k/output_images/7893.png  \n",
            "  inflating: data_80k/output_images/78930.png  \n",
            "  inflating: data_80k/output_images/78931.png  \n",
            "  inflating: data_80k/output_images/78932.png  \n",
            "  inflating: data_80k/output_images/78933.png  \n",
            "  inflating: data_80k/output_images/78934.png  \n",
            "  inflating: data_80k/output_images/78935.png  \n",
            "  inflating: data_80k/output_images/78936.png  \n",
            "  inflating: data_80k/output_images/78937.png  \n",
            "  inflating: data_80k/output_images/78938.png  \n",
            "  inflating: data_80k/output_images/78939.png  \n",
            "  inflating: data_80k/output_images/7894.png  \n",
            "  inflating: data_80k/output_images/78940.png  \n",
            "  inflating: data_80k/output_images/78941.png  \n",
            "  inflating: data_80k/output_images/78942.png  \n",
            "  inflating: data_80k/output_images/78943.png  \n",
            "  inflating: data_80k/output_images/78944.png  \n",
            "  inflating: data_80k/output_images/78945.png  \n",
            "  inflating: data_80k/output_images/78946.png  \n",
            "  inflating: data_80k/output_images/78947.png  \n",
            "  inflating: data_80k/output_images/78948.png  \n",
            "  inflating: data_80k/output_images/78949.png  \n",
            "  inflating: data_80k/output_images/7895.png  \n",
            "  inflating: data_80k/output_images/78950.png  \n",
            "  inflating: data_80k/output_images/78951.png  \n",
            "  inflating: data_80k/output_images/78952.png  \n",
            "  inflating: data_80k/output_images/78953.png  \n",
            "  inflating: data_80k/output_images/78954.png  \n",
            "  inflating: data_80k/output_images/78955.png  \n",
            "  inflating: data_80k/output_images/78956.png  \n",
            "  inflating: data_80k/output_images/78957.png  \n",
            "  inflating: data_80k/output_images/78958.png  \n",
            "  inflating: data_80k/output_images/78959.png  \n",
            "  inflating: data_80k/output_images/7896.png  \n",
            "  inflating: data_80k/output_images/78960.png  \n",
            "  inflating: data_80k/output_images/78961.png  \n",
            "  inflating: data_80k/output_images/78962.png  \n",
            "  inflating: data_80k/output_images/78963.png  \n",
            "  inflating: data_80k/output_images/78964.png  \n",
            "  inflating: data_80k/output_images/78965.png  \n",
            "  inflating: data_80k/output_images/78966.png  \n",
            "  inflating: data_80k/output_images/78967.png  \n",
            "  inflating: data_80k/output_images/78968.png  \n",
            "  inflating: data_80k/output_images/78969.png  \n",
            "  inflating: data_80k/output_images/7897.png  \n",
            "  inflating: data_80k/output_images/78970.png  \n",
            "  inflating: data_80k/output_images/78971.png  \n",
            "  inflating: data_80k/output_images/78972.png  \n",
            "  inflating: data_80k/output_images/78973.png  \n",
            "  inflating: data_80k/output_images/78974.png  \n",
            "  inflating: data_80k/output_images/78975.png  \n",
            "  inflating: data_80k/output_images/78976.png  \n",
            "  inflating: data_80k/output_images/78977.png  \n",
            "  inflating: data_80k/output_images/78978.png  \n",
            "  inflating: data_80k/output_images/78979.png  \n",
            "  inflating: data_80k/output_images/7898.png  \n",
            "  inflating: data_80k/output_images/78980.png  \n",
            "  inflating: data_80k/output_images/78981.png  \n",
            "  inflating: data_80k/output_images/78982.png  \n",
            "  inflating: data_80k/output_images/78983.png  \n",
            "  inflating: data_80k/output_images/78984.png  \n",
            "  inflating: data_80k/output_images/78985.png  \n",
            "  inflating: data_80k/output_images/78986.png  \n",
            "  inflating: data_80k/output_images/78987.png  \n",
            "  inflating: data_80k/output_images/78988.png  \n",
            "  inflating: data_80k/output_images/78989.png  \n",
            "  inflating: data_80k/output_images/7899.png  \n",
            "  inflating: data_80k/output_images/78990.png  \n",
            "  inflating: data_80k/output_images/78991.png  \n",
            "  inflating: data_80k/output_images/78992.png  \n",
            "  inflating: data_80k/output_images/78993.png  \n",
            "  inflating: data_80k/output_images/78994.png  \n",
            "  inflating: data_80k/output_images/78995.png  \n",
            "  inflating: data_80k/output_images/78996.png  \n",
            "  inflating: data_80k/output_images/78997.png  \n",
            "  inflating: data_80k/output_images/78998.png  \n",
            "  inflating: data_80k/output_images/78999.png  \n",
            "  inflating: data_80k/output_images/79.png  \n",
            "  inflating: data_80k/output_images/790.png  \n",
            "  inflating: data_80k/output_images/7900.png  \n",
            "  inflating: data_80k/output_images/79000.png  \n",
            "  inflating: data_80k/output_images/79001.png  \n",
            "  inflating: data_80k/output_images/79002.png  \n",
            "  inflating: data_80k/output_images/79003.png  \n",
            "  inflating: data_80k/output_images/79004.png  \n",
            "  inflating: data_80k/output_images/79005.png  \n",
            "  inflating: data_80k/output_images/79006.png  \n",
            "  inflating: data_80k/output_images/79007.png  \n",
            "  inflating: data_80k/output_images/79008.png  \n",
            "  inflating: data_80k/output_images/79009.png  \n",
            "  inflating: data_80k/output_images/7901.png  \n",
            "  inflating: data_80k/output_images/79010.png  \n",
            "  inflating: data_80k/output_images/79011.png  \n",
            "  inflating: data_80k/output_images/79012.png  \n",
            "  inflating: data_80k/output_images/79013.png  \n",
            "  inflating: data_80k/output_images/79014.png  \n",
            "  inflating: data_80k/output_images/79015.png  \n",
            "  inflating: data_80k/output_images/79016.png  \n",
            "  inflating: data_80k/output_images/79017.png  \n",
            "  inflating: data_80k/output_images/79018.png  \n",
            "  inflating: data_80k/output_images/79019.png  \n",
            "  inflating: data_80k/output_images/7902.png  \n",
            "  inflating: data_80k/output_images/79020.png  \n",
            "  inflating: data_80k/output_images/79021.png  \n",
            "  inflating: data_80k/output_images/79022.png  \n",
            "  inflating: data_80k/output_images/79023.png  \n",
            "  inflating: data_80k/output_images/79024.png  \n",
            "  inflating: data_80k/output_images/79025.png  \n",
            "  inflating: data_80k/output_images/79026.png  \n",
            "  inflating: data_80k/output_images/79027.png  \n",
            "  inflating: data_80k/output_images/79028.png  \n",
            "  inflating: data_80k/output_images/79029.png  \n",
            "  inflating: data_80k/output_images/7903.png  \n",
            "  inflating: data_80k/output_images/79030.png  \n",
            "  inflating: data_80k/output_images/79031.png  \n",
            "  inflating: data_80k/output_images/79032.png  \n",
            "  inflating: data_80k/output_images/79033.png  \n",
            "  inflating: data_80k/output_images/79034.png  \n",
            "  inflating: data_80k/output_images/79035.png  \n",
            "  inflating: data_80k/output_images/79036.png  \n",
            "  inflating: data_80k/output_images/79037.png  \n",
            "  inflating: data_80k/output_images/79038.png  \n",
            "  inflating: data_80k/output_images/79039.png  \n",
            "  inflating: data_80k/output_images/7904.png  \n",
            "  inflating: data_80k/output_images/79040.png  \n",
            "  inflating: data_80k/output_images/79041.png  \n",
            "  inflating: data_80k/output_images/79042.png  \n",
            "  inflating: data_80k/output_images/79043.png  \n",
            "  inflating: data_80k/output_images/79044.png  \n",
            "  inflating: data_80k/output_images/79045.png  \n",
            "  inflating: data_80k/output_images/79046.png  \n",
            "  inflating: data_80k/output_images/79047.png  \n",
            "  inflating: data_80k/output_images/79048.png  \n",
            "  inflating: data_80k/output_images/79049.png  \n",
            "  inflating: data_80k/output_images/7905.png  \n",
            "  inflating: data_80k/output_images/79050.png  \n",
            "  inflating: data_80k/output_images/79051.png  \n",
            "  inflating: data_80k/output_images/79052.png  \n",
            "  inflating: data_80k/output_images/79053.png  \n",
            "  inflating: data_80k/output_images/79054.png  \n",
            "  inflating: data_80k/output_images/79055.png  \n",
            "  inflating: data_80k/output_images/79056.png  \n",
            "  inflating: data_80k/output_images/79057.png  \n",
            "  inflating: data_80k/output_images/79058.png  \n",
            "  inflating: data_80k/output_images/79059.png  \n",
            "  inflating: data_80k/output_images/7906.png  \n",
            "  inflating: data_80k/output_images/79060.png  \n",
            "  inflating: data_80k/output_images/79061.png  \n",
            "  inflating: data_80k/output_images/79062.png  \n",
            "  inflating: data_80k/output_images/79063.png  \n",
            "  inflating: data_80k/output_images/79064.png  \n",
            "  inflating: data_80k/output_images/79065.png  \n",
            "  inflating: data_80k/output_images/79066.png  \n",
            "  inflating: data_80k/output_images/79067.png  \n",
            "  inflating: data_80k/output_images/79068.png  \n",
            "  inflating: data_80k/output_images/79069.png  \n",
            "  inflating: data_80k/output_images/7907.png  \n",
            "  inflating: data_80k/output_images/79070.png  \n",
            "  inflating: data_80k/output_images/79071.png  \n",
            "  inflating: data_80k/output_images/79072.png  \n",
            "  inflating: data_80k/output_images/79073.png  \n",
            "  inflating: data_80k/output_images/79074.png  \n",
            "  inflating: data_80k/output_images/79075.png  \n",
            "  inflating: data_80k/output_images/79076.png  \n",
            "  inflating: data_80k/output_images/79077.png  \n",
            "  inflating: data_80k/output_images/79078.png  \n",
            "  inflating: data_80k/output_images/79079.png  \n",
            "  inflating: data_80k/output_images/7908.png  \n",
            "  inflating: data_80k/output_images/79080.png  \n",
            "  inflating: data_80k/output_images/79081.png  \n",
            "  inflating: data_80k/output_images/79082.png  \n",
            "  inflating: data_80k/output_images/79083.png  \n",
            "  inflating: data_80k/output_images/79084.png  \n",
            "  inflating: data_80k/output_images/79085.png  \n",
            "  inflating: data_80k/output_images/79086.png  \n",
            "  inflating: data_80k/output_images/79087.png  \n",
            "  inflating: data_80k/output_images/79088.png  \n",
            "  inflating: data_80k/output_images/79089.png  \n",
            "  inflating: data_80k/output_images/7909.png  \n",
            "  inflating: data_80k/output_images/79090.png  \n",
            "  inflating: data_80k/output_images/79091.png  \n",
            "  inflating: data_80k/output_images/79092.png  \n",
            "  inflating: data_80k/output_images/79093.png  \n",
            "  inflating: data_80k/output_images/79094.png  \n",
            "  inflating: data_80k/output_images/79095.png  \n",
            "  inflating: data_80k/output_images/79096.png  \n",
            "  inflating: data_80k/output_images/79097.png  \n",
            "  inflating: data_80k/output_images/79098.png  \n",
            "  inflating: data_80k/output_images/79099.png  \n",
            "  inflating: data_80k/output_images/791.png  \n",
            "  inflating: data_80k/output_images/7910.png  \n",
            "  inflating: data_80k/output_images/79100.png  \n",
            "  inflating: data_80k/output_images/79101.png  \n",
            "  inflating: data_80k/output_images/79102.png  \n",
            "  inflating: data_80k/output_images/79103.png  \n",
            "  inflating: data_80k/output_images/79104.png  \n",
            "  inflating: data_80k/output_images/79105.png  \n",
            "  inflating: data_80k/output_images/79106.png  \n",
            "  inflating: data_80k/output_images/79107.png  \n",
            "  inflating: data_80k/output_images/79108.png  \n",
            "  inflating: data_80k/output_images/79109.png  \n",
            "  inflating: data_80k/output_images/7911.png  \n",
            "  inflating: data_80k/output_images/79110.png  \n",
            "  inflating: data_80k/output_images/79111.png  \n",
            "  inflating: data_80k/output_images/79112.png  \n",
            "  inflating: data_80k/output_images/79113.png  \n",
            "  inflating: data_80k/output_images/79114.png  \n",
            "  inflating: data_80k/output_images/79115.png  \n",
            "  inflating: data_80k/output_images/79116.png  \n",
            "  inflating: data_80k/output_images/79117.png  \n",
            "  inflating: data_80k/output_images/79118.png  \n",
            "  inflating: data_80k/output_images/79119.png  \n",
            "  inflating: data_80k/output_images/7912.png  \n",
            "  inflating: data_80k/output_images/79120.png  \n",
            "  inflating: data_80k/output_images/79121.png  \n",
            "  inflating: data_80k/output_images/79122.png  \n",
            "  inflating: data_80k/output_images/79123.png  \n",
            "  inflating: data_80k/output_images/79124.png  \n",
            "  inflating: data_80k/output_images/79125.png  \n",
            "  inflating: data_80k/output_images/79126.png  \n",
            "  inflating: data_80k/output_images/79127.png  \n",
            "  inflating: data_80k/output_images/79128.png  \n",
            "  inflating: data_80k/output_images/79129.png  \n",
            "  inflating: data_80k/output_images/7913.png  \n",
            "  inflating: data_80k/output_images/79130.png  \n",
            "  inflating: data_80k/output_images/79131.png  \n",
            "  inflating: data_80k/output_images/79132.png  \n",
            "  inflating: data_80k/output_images/79133.png  \n",
            "  inflating: data_80k/output_images/79134.png  \n",
            "  inflating: data_80k/output_images/79135.png  \n",
            "  inflating: data_80k/output_images/79136.png  \n",
            "  inflating: data_80k/output_images/79137.png  \n",
            "  inflating: data_80k/output_images/79138.png  \n",
            "  inflating: data_80k/output_images/79139.png  \n",
            "  inflating: data_80k/output_images/7914.png  \n",
            "  inflating: data_80k/output_images/79140.png  \n",
            "  inflating: data_80k/output_images/79141.png  \n",
            "  inflating: data_80k/output_images/79142.png  \n",
            "  inflating: data_80k/output_images/79143.png  \n",
            "  inflating: data_80k/output_images/79144.png  \n",
            "  inflating: data_80k/output_images/79145.png  \n",
            "  inflating: data_80k/output_images/79146.png  \n",
            "  inflating: data_80k/output_images/79147.png  \n",
            "  inflating: data_80k/output_images/79148.png  \n",
            "  inflating: data_80k/output_images/79149.png  \n",
            "  inflating: data_80k/output_images/7915.png  \n",
            "  inflating: data_80k/output_images/79150.png  \n",
            "  inflating: data_80k/output_images/79151.png  \n",
            "  inflating: data_80k/output_images/79152.png  \n",
            "  inflating: data_80k/output_images/79153.png  \n",
            "  inflating: data_80k/output_images/79154.png  \n",
            "  inflating: data_80k/output_images/79155.png  \n",
            "  inflating: data_80k/output_images/79156.png  \n",
            "  inflating: data_80k/output_images/79157.png  \n",
            "  inflating: data_80k/output_images/79158.png  \n",
            "  inflating: data_80k/output_images/79159.png  \n",
            "  inflating: data_80k/output_images/7916.png  \n",
            "  inflating: data_80k/output_images/79160.png  \n",
            "  inflating: data_80k/output_images/79161.png  \n",
            "  inflating: data_80k/output_images/79162.png  \n",
            "  inflating: data_80k/output_images/79163.png  \n",
            "  inflating: data_80k/output_images/79164.png  \n",
            "  inflating: data_80k/output_images/79165.png  \n",
            "  inflating: data_80k/output_images/79166.png  \n",
            "  inflating: data_80k/output_images/79167.png  \n",
            "  inflating: data_80k/output_images/79168.png  \n",
            "  inflating: data_80k/output_images/79169.png  \n",
            "  inflating: data_80k/output_images/7917.png  \n",
            "  inflating: data_80k/output_images/79170.png  \n",
            "  inflating: data_80k/output_images/79171.png  \n",
            "  inflating: data_80k/output_images/79172.png  \n",
            "  inflating: data_80k/output_images/79173.png  \n",
            "  inflating: data_80k/output_images/79174.png  \n",
            "  inflating: data_80k/output_images/79175.png  \n",
            "  inflating: data_80k/output_images/79176.png  \n",
            "  inflating: data_80k/output_images/79177.png  \n",
            "  inflating: data_80k/output_images/79178.png  \n",
            "  inflating: data_80k/output_images/79179.png  \n",
            "  inflating: data_80k/output_images/7918.png  \n",
            "  inflating: data_80k/output_images/79180.png  \n",
            "  inflating: data_80k/output_images/79181.png  \n",
            "  inflating: data_80k/output_images/79182.png  \n",
            "  inflating: data_80k/output_images/79183.png  \n",
            "  inflating: data_80k/output_images/79184.png  \n",
            "  inflating: data_80k/output_images/79185.png  \n",
            "  inflating: data_80k/output_images/79186.png  \n",
            "  inflating: data_80k/output_images/79187.png  \n",
            "  inflating: data_80k/output_images/79188.png  \n",
            "  inflating: data_80k/output_images/79189.png  \n",
            "  inflating: data_80k/output_images/7919.png  \n",
            "  inflating: data_80k/output_images/79190.png  \n",
            "  inflating: data_80k/output_images/79191.png  \n",
            "  inflating: data_80k/output_images/79192.png  \n",
            "  inflating: data_80k/output_images/79193.png  \n",
            "  inflating: data_80k/output_images/79194.png  \n",
            "  inflating: data_80k/output_images/79195.png  \n",
            "  inflating: data_80k/output_images/79196.png  \n",
            "  inflating: data_80k/output_images/79197.png  \n",
            "  inflating: data_80k/output_images/79198.png  \n",
            "  inflating: data_80k/output_images/79199.png  \n",
            "  inflating: data_80k/output_images/792.png  \n",
            "  inflating: data_80k/output_images/7920.png  \n",
            "  inflating: data_80k/output_images/79200.png  \n",
            "  inflating: data_80k/output_images/79201.png  \n",
            "  inflating: data_80k/output_images/79202.png  \n",
            "  inflating: data_80k/output_images/79203.png  \n",
            "  inflating: data_80k/output_images/79204.png  \n",
            "  inflating: data_80k/output_images/79205.png  \n",
            "  inflating: data_80k/output_images/79206.png  \n",
            "  inflating: data_80k/output_images/79207.png  \n",
            "  inflating: data_80k/output_images/79208.png  \n",
            "  inflating: data_80k/output_images/79209.png  \n",
            "  inflating: data_80k/output_images/7921.png  \n",
            "  inflating: data_80k/output_images/79210.png  \n",
            "  inflating: data_80k/output_images/79211.png  \n",
            "  inflating: data_80k/output_images/79212.png  \n",
            "  inflating: data_80k/output_images/79213.png  \n",
            "  inflating: data_80k/output_images/79214.png  \n",
            "  inflating: data_80k/output_images/79215.png  \n",
            "  inflating: data_80k/output_images/79216.png  \n",
            "  inflating: data_80k/output_images/79217.png  \n",
            "  inflating: data_80k/output_images/79218.png  \n",
            "  inflating: data_80k/output_images/79219.png  \n",
            "  inflating: data_80k/output_images/7922.png  \n",
            "  inflating: data_80k/output_images/79220.png  \n",
            "  inflating: data_80k/output_images/79221.png  \n",
            "  inflating: data_80k/output_images/79222.png  \n",
            "  inflating: data_80k/output_images/79223.png  \n",
            "  inflating: data_80k/output_images/79224.png  \n",
            "  inflating: data_80k/output_images/79225.png  \n",
            "  inflating: data_80k/output_images/79226.png  \n",
            "  inflating: data_80k/output_images/79227.png  \n",
            "  inflating: data_80k/output_images/79228.png  \n",
            "  inflating: data_80k/output_images/79229.png  \n",
            "  inflating: data_80k/output_images/7923.png  \n",
            "  inflating: data_80k/output_images/79230.png  \n",
            "  inflating: data_80k/output_images/79231.png  \n",
            "  inflating: data_80k/output_images/79232.png  \n",
            "  inflating: data_80k/output_images/79233.png  \n",
            "  inflating: data_80k/output_images/79234.png  \n",
            "  inflating: data_80k/output_images/79235.png  \n",
            "  inflating: data_80k/output_images/79236.png  \n",
            "  inflating: data_80k/output_images/79237.png  \n",
            "  inflating: data_80k/output_images/79238.png  \n",
            "  inflating: data_80k/output_images/79239.png  \n",
            "  inflating: data_80k/output_images/7924.png  \n",
            "  inflating: data_80k/output_images/79240.png  \n",
            "  inflating: data_80k/output_images/79241.png  \n",
            "  inflating: data_80k/output_images/79242.png  \n",
            "  inflating: data_80k/output_images/79243.png  \n",
            "  inflating: data_80k/output_images/79244.png  \n",
            "  inflating: data_80k/output_images/79245.png  \n",
            "  inflating: data_80k/output_images/79246.png  \n",
            "  inflating: data_80k/output_images/79247.png  \n",
            "  inflating: data_80k/output_images/79248.png  \n",
            "  inflating: data_80k/output_images/79249.png  \n",
            "  inflating: data_80k/output_images/7925.png  \n",
            "  inflating: data_80k/output_images/79250.png  \n",
            "  inflating: data_80k/output_images/79251.png  \n",
            "  inflating: data_80k/output_images/79252.png  \n",
            "  inflating: data_80k/output_images/79253.png  \n",
            "  inflating: data_80k/output_images/79254.png  \n",
            "  inflating: data_80k/output_images/79255.png  \n",
            "  inflating: data_80k/output_images/79256.png  \n",
            "  inflating: data_80k/output_images/79257.png  \n",
            "  inflating: data_80k/output_images/79258.png  \n",
            "  inflating: data_80k/output_images/79259.png  \n",
            "  inflating: data_80k/output_images/7926.png  \n",
            "  inflating: data_80k/output_images/79260.png  \n",
            "  inflating: data_80k/output_images/79261.png  \n",
            "  inflating: data_80k/output_images/79262.png  \n",
            "  inflating: data_80k/output_images/79263.png  \n",
            "  inflating: data_80k/output_images/79264.png  \n",
            "  inflating: data_80k/output_images/79265.png  \n",
            "  inflating: data_80k/output_images/79266.png  \n",
            "  inflating: data_80k/output_images/79267.png  \n",
            "  inflating: data_80k/output_images/79268.png  \n",
            "  inflating: data_80k/output_images/79269.png  \n",
            "  inflating: data_80k/output_images/7927.png  \n",
            "  inflating: data_80k/output_images/79270.png  \n",
            "  inflating: data_80k/output_images/79271.png  \n",
            "  inflating: data_80k/output_images/79272.png  \n",
            "  inflating: data_80k/output_images/79273.png  \n",
            "  inflating: data_80k/output_images/79274.png  \n",
            "  inflating: data_80k/output_images/79275.png  \n",
            "  inflating: data_80k/output_images/79276.png  \n",
            "  inflating: data_80k/output_images/79277.png  \n",
            "  inflating: data_80k/output_images/79278.png  \n",
            "  inflating: data_80k/output_images/79279.png  \n",
            "  inflating: data_80k/output_images/7928.png  \n",
            "  inflating: data_80k/output_images/79280.png  \n",
            "  inflating: data_80k/output_images/79281.png  \n",
            "  inflating: data_80k/output_images/79282.png  \n",
            "  inflating: data_80k/output_images/79283.png  \n",
            "  inflating: data_80k/output_images/79284.png  \n",
            "  inflating: data_80k/output_images/79285.png  \n",
            "  inflating: data_80k/output_images/79286.png  \n",
            "  inflating: data_80k/output_images/79287.png  \n",
            "  inflating: data_80k/output_images/79288.png  \n",
            "  inflating: data_80k/output_images/79289.png  \n",
            "  inflating: data_80k/output_images/7929.png  \n",
            "  inflating: data_80k/output_images/79290.png  \n",
            "  inflating: data_80k/output_images/79291.png  \n",
            "  inflating: data_80k/output_images/79292.png  \n",
            "  inflating: data_80k/output_images/79293.png  \n",
            "  inflating: data_80k/output_images/79294.png  \n",
            "  inflating: data_80k/output_images/79295.png  \n",
            "  inflating: data_80k/output_images/79296.png  \n",
            "  inflating: data_80k/output_images/79297.png  \n",
            "  inflating: data_80k/output_images/79298.png  \n",
            "  inflating: data_80k/output_images/79299.png  \n",
            "  inflating: data_80k/output_images/793.png  \n",
            "  inflating: data_80k/output_images/7930.png  \n",
            "  inflating: data_80k/output_images/79300.png  \n",
            "  inflating: data_80k/output_images/79301.png  \n",
            "  inflating: data_80k/output_images/79302.png  \n",
            "  inflating: data_80k/output_images/79303.png  \n",
            "  inflating: data_80k/output_images/79304.png  \n",
            "  inflating: data_80k/output_images/79305.png  \n",
            "  inflating: data_80k/output_images/79306.png  \n",
            "  inflating: data_80k/output_images/79307.png  \n",
            "  inflating: data_80k/output_images/79308.png  \n",
            "  inflating: data_80k/output_images/79309.png  \n",
            "  inflating: data_80k/output_images/7931.png  \n",
            "  inflating: data_80k/output_images/79310.png  \n",
            "  inflating: data_80k/output_images/79311.png  \n",
            "  inflating: data_80k/output_images/79312.png  \n",
            "  inflating: data_80k/output_images/79313.png  \n",
            "  inflating: data_80k/output_images/79314.png  \n",
            "  inflating: data_80k/output_images/79315.png  \n",
            "  inflating: data_80k/output_images/79316.png  \n",
            "  inflating: data_80k/output_images/79317.png  \n",
            "  inflating: data_80k/output_images/79318.png  \n",
            "  inflating: data_80k/output_images/79319.png  \n",
            "  inflating: data_80k/output_images/7932.png  \n",
            "  inflating: data_80k/output_images/79320.png  \n",
            "  inflating: data_80k/output_images/79321.png  \n",
            "  inflating: data_80k/output_images/79322.png  \n",
            "  inflating: data_80k/output_images/79323.png  \n",
            "  inflating: data_80k/output_images/79324.png  \n",
            "  inflating: data_80k/output_images/79325.png  \n",
            "  inflating: data_80k/output_images/79326.png  \n",
            "  inflating: data_80k/output_images/79327.png  \n",
            "  inflating: data_80k/output_images/79328.png  \n",
            "  inflating: data_80k/output_images/79329.png  \n",
            "  inflating: data_80k/output_images/7933.png  \n",
            "  inflating: data_80k/output_images/79330.png  \n",
            "  inflating: data_80k/output_images/79331.png  \n",
            "  inflating: data_80k/output_images/79332.png  \n",
            "  inflating: data_80k/output_images/79333.png  \n",
            "  inflating: data_80k/output_images/79334.png  \n",
            "  inflating: data_80k/output_images/79335.png  \n",
            "  inflating: data_80k/output_images/79336.png  \n",
            "  inflating: data_80k/output_images/79337.png  \n",
            "  inflating: data_80k/output_images/79338.png  \n",
            "  inflating: data_80k/output_images/79339.png  \n",
            "  inflating: data_80k/output_images/7934.png  \n",
            "  inflating: data_80k/output_images/79340.png  \n",
            "  inflating: data_80k/output_images/79341.png  \n",
            "  inflating: data_80k/output_images/79342.png  \n",
            "  inflating: data_80k/output_images/79343.png  \n",
            "  inflating: data_80k/output_images/79344.png  \n",
            "  inflating: data_80k/output_images/79345.png  \n",
            "  inflating: data_80k/output_images/79346.png  \n",
            "  inflating: data_80k/output_images/79347.png  \n",
            "  inflating: data_80k/output_images/79348.png  \n",
            "  inflating: data_80k/output_images/79349.png  \n",
            "  inflating: data_80k/output_images/7935.png  \n",
            "  inflating: data_80k/output_images/79350.png  \n",
            "  inflating: data_80k/output_images/79351.png  \n",
            "  inflating: data_80k/output_images/79352.png  \n",
            "  inflating: data_80k/output_images/79353.png  \n",
            "  inflating: data_80k/output_images/79354.png  \n",
            "  inflating: data_80k/output_images/79355.png  \n",
            "  inflating: data_80k/output_images/79356.png  \n",
            "  inflating: data_80k/output_images/79357.png  \n",
            "  inflating: data_80k/output_images/79358.png  \n",
            "  inflating: data_80k/output_images/79359.png  \n",
            "  inflating: data_80k/output_images/7936.png  \n",
            "  inflating: data_80k/output_images/79360.png  \n",
            "  inflating: data_80k/output_images/79361.png  \n",
            "  inflating: data_80k/output_images/79362.png  \n",
            "  inflating: data_80k/output_images/79363.png  \n",
            "  inflating: data_80k/output_images/79364.png  \n",
            "  inflating: data_80k/output_images/79365.png  \n",
            "  inflating: data_80k/output_images/79366.png  \n",
            "  inflating: data_80k/output_images/79367.png  \n",
            "  inflating: data_80k/output_images/79368.png  \n",
            "  inflating: data_80k/output_images/79369.png  \n",
            "  inflating: data_80k/output_images/7937.png  \n",
            "  inflating: data_80k/output_images/79370.png  \n",
            "  inflating: data_80k/output_images/79371.png  \n",
            "  inflating: data_80k/output_images/79372.png  \n",
            "  inflating: data_80k/output_images/79373.png  \n",
            "  inflating: data_80k/output_images/79374.png  \n",
            "  inflating: data_80k/output_images/79375.png  \n",
            "  inflating: data_80k/output_images/79376.png  \n",
            "  inflating: data_80k/output_images/79377.png  \n",
            "  inflating: data_80k/output_images/79378.png  \n",
            "  inflating: data_80k/output_images/79379.png  \n",
            "  inflating: data_80k/output_images/7938.png  \n",
            "  inflating: data_80k/output_images/79380.png  \n",
            "  inflating: data_80k/output_images/79381.png  \n",
            "  inflating: data_80k/output_images/79382.png  \n",
            "  inflating: data_80k/output_images/79383.png  \n",
            "  inflating: data_80k/output_images/79384.png  \n",
            "  inflating: data_80k/output_images/79385.png  \n",
            "  inflating: data_80k/output_images/79386.png  \n",
            "  inflating: data_80k/output_images/79387.png  \n",
            "  inflating: data_80k/output_images/79388.png  \n",
            "  inflating: data_80k/output_images/79389.png  \n",
            "  inflating: data_80k/output_images/7939.png  \n",
            "  inflating: data_80k/output_images/79390.png  \n",
            "  inflating: data_80k/output_images/79391.png  \n",
            "  inflating: data_80k/output_images/79392.png  \n",
            "  inflating: data_80k/output_images/79393.png  \n",
            "  inflating: data_80k/output_images/79394.png  \n",
            "  inflating: data_80k/output_images/79395.png  \n",
            "  inflating: data_80k/output_images/79396.png  \n",
            "  inflating: data_80k/output_images/79397.png  \n",
            "  inflating: data_80k/output_images/79398.png  \n",
            "  inflating: data_80k/output_images/79399.png  \n",
            "  inflating: data_80k/output_images/794.png  \n",
            "  inflating: data_80k/output_images/7940.png  \n",
            "  inflating: data_80k/output_images/79400.png  \n",
            "  inflating: data_80k/output_images/79401.png  \n",
            "  inflating: data_80k/output_images/79402.png  \n",
            "  inflating: data_80k/output_images/79403.png  \n",
            "  inflating: data_80k/output_images/79404.png  \n",
            "  inflating: data_80k/output_images/79405.png  \n",
            "  inflating: data_80k/output_images/79406.png  \n",
            "  inflating: data_80k/output_images/79407.png  \n",
            "  inflating: data_80k/output_images/79408.png  \n",
            "  inflating: data_80k/output_images/79409.png  \n",
            "  inflating: data_80k/output_images/7941.png  \n",
            "  inflating: data_80k/output_images/79410.png  \n",
            "  inflating: data_80k/output_images/79411.png  \n",
            "  inflating: data_80k/output_images/79412.png  \n",
            "  inflating: data_80k/output_images/79413.png  \n",
            "  inflating: data_80k/output_images/79414.png  \n",
            "  inflating: data_80k/output_images/79415.png  \n",
            "  inflating: data_80k/output_images/79416.png  \n",
            "  inflating: data_80k/output_images/79417.png  \n",
            "  inflating: data_80k/output_images/79418.png  \n",
            "  inflating: data_80k/output_images/79419.png  \n",
            "  inflating: data_80k/output_images/7942.png  \n",
            "  inflating: data_80k/output_images/79420.png  \n",
            "  inflating: data_80k/output_images/79421.png  \n",
            "  inflating: data_80k/output_images/79422.png  \n",
            "  inflating: data_80k/output_images/79423.png  \n",
            "  inflating: data_80k/output_images/79424.png  \n",
            "  inflating: data_80k/output_images/79425.png  \n",
            "  inflating: data_80k/output_images/79426.png  \n",
            "  inflating: data_80k/output_images/79427.png  \n",
            "  inflating: data_80k/output_images/79428.png  \n",
            "  inflating: data_80k/output_images/79429.png  \n",
            "  inflating: data_80k/output_images/7943.png  \n",
            "  inflating: data_80k/output_images/79430.png  \n",
            "  inflating: data_80k/output_images/79431.png  \n",
            "  inflating: data_80k/output_images/79432.png  \n",
            "  inflating: data_80k/output_images/79433.png  \n",
            "  inflating: data_80k/output_images/79434.png  \n",
            "  inflating: data_80k/output_images/79435.png  \n",
            "  inflating: data_80k/output_images/79436.png  \n",
            "  inflating: data_80k/output_images/79437.png  \n",
            "  inflating: data_80k/output_images/79438.png  \n",
            "  inflating: data_80k/output_images/79439.png  \n",
            "  inflating: data_80k/output_images/7944.png  \n",
            "  inflating: data_80k/output_images/79440.png  \n",
            "  inflating: data_80k/output_images/79441.png  \n",
            "  inflating: data_80k/output_images/79442.png  \n",
            "  inflating: data_80k/output_images/79443.png  \n",
            "  inflating: data_80k/output_images/79444.png  \n",
            "  inflating: data_80k/output_images/79445.png  \n",
            "  inflating: data_80k/output_images/79446.png  \n",
            "  inflating: data_80k/output_images/79447.png  \n",
            "  inflating: data_80k/output_images/79448.png  \n",
            "  inflating: data_80k/output_images/79449.png  \n",
            "  inflating: data_80k/output_images/7945.png  \n",
            "  inflating: data_80k/output_images/79450.png  \n",
            "  inflating: data_80k/output_images/79451.png  \n",
            "  inflating: data_80k/output_images/79452.png  \n",
            "  inflating: data_80k/output_images/79453.png  \n",
            "  inflating: data_80k/output_images/79454.png  \n",
            "  inflating: data_80k/output_images/79455.png  \n",
            "  inflating: data_80k/output_images/79456.png  \n",
            "  inflating: data_80k/output_images/79457.png  \n",
            "  inflating: data_80k/output_images/79458.png  \n",
            "  inflating: data_80k/output_images/79459.png  \n",
            "  inflating: data_80k/output_images/7946.png  \n",
            "  inflating: data_80k/output_images/79460.png  \n",
            "  inflating: data_80k/output_images/79461.png  \n",
            "  inflating: data_80k/output_images/79462.png  \n",
            "  inflating: data_80k/output_images/79463.png  \n",
            "  inflating: data_80k/output_images/79464.png  \n",
            "  inflating: data_80k/output_images/79465.png  \n",
            "  inflating: data_80k/output_images/79466.png  \n",
            "  inflating: data_80k/output_images/79467.png  \n",
            "  inflating: data_80k/output_images/79468.png  \n",
            "  inflating: data_80k/output_images/79469.png  \n",
            "  inflating: data_80k/output_images/7947.png  \n",
            "  inflating: data_80k/output_images/79470.png  \n",
            "  inflating: data_80k/output_images/79471.png  \n",
            "  inflating: data_80k/output_images/79472.png  \n",
            "  inflating: data_80k/output_images/79473.png  \n",
            "  inflating: data_80k/output_images/79474.png  \n",
            "  inflating: data_80k/output_images/79475.png  \n",
            "  inflating: data_80k/output_images/79476.png  \n",
            "  inflating: data_80k/output_images/79477.png  \n",
            "  inflating: data_80k/output_images/79478.png  \n",
            "  inflating: data_80k/output_images/79479.png  \n",
            "  inflating: data_80k/output_images/7948.png  \n",
            "  inflating: data_80k/output_images/79480.png  \n",
            "  inflating: data_80k/output_images/79481.png  \n",
            "  inflating: data_80k/output_images/79482.png  \n",
            "  inflating: data_80k/output_images/79483.png  \n",
            "  inflating: data_80k/output_images/79484.png  \n",
            "  inflating: data_80k/output_images/79485.png  \n",
            "  inflating: data_80k/output_images/79486.png  \n",
            "  inflating: data_80k/output_images/79487.png  \n",
            "  inflating: data_80k/output_images/79488.png  \n",
            "  inflating: data_80k/output_images/79489.png  \n",
            "  inflating: data_80k/output_images/7949.png  \n",
            "  inflating: data_80k/output_images/79490.png  \n",
            "  inflating: data_80k/output_images/79491.png  \n",
            "  inflating: data_80k/output_images/79492.png  \n",
            "  inflating: data_80k/output_images/79493.png  \n",
            "  inflating: data_80k/output_images/79494.png  \n",
            "  inflating: data_80k/output_images/79495.png  \n",
            "  inflating: data_80k/output_images/79496.png  \n",
            "  inflating: data_80k/output_images/79497.png  \n",
            "  inflating: data_80k/output_images/79498.png  \n",
            "  inflating: data_80k/output_images/79499.png  \n",
            "  inflating: data_80k/output_images/795.png  \n",
            "  inflating: data_80k/output_images/7950.png  \n",
            "  inflating: data_80k/output_images/79500.png  \n",
            "  inflating: data_80k/output_images/79501.png  \n",
            "  inflating: data_80k/output_images/79502.png  \n",
            "  inflating: data_80k/output_images/79503.png  \n",
            "  inflating: data_80k/output_images/79504.png  \n",
            "  inflating: data_80k/output_images/79505.png  \n",
            "  inflating: data_80k/output_images/79506.png  \n",
            "  inflating: data_80k/output_images/79507.png  \n",
            "  inflating: data_80k/output_images/79508.png  \n",
            "  inflating: data_80k/output_images/79509.png  \n",
            "  inflating: data_80k/output_images/7951.png  \n",
            "  inflating: data_80k/output_images/79510.png  \n",
            "  inflating: data_80k/output_images/79511.png  \n",
            "  inflating: data_80k/output_images/79512.png  \n",
            "  inflating: data_80k/output_images/79513.png  \n",
            "  inflating: data_80k/output_images/79514.png  \n",
            "  inflating: data_80k/output_images/79515.png  \n",
            "  inflating: data_80k/output_images/79516.png  \n",
            "  inflating: data_80k/output_images/79517.png  \n",
            "  inflating: data_80k/output_images/79518.png  \n",
            "  inflating: data_80k/output_images/79519.png  \n",
            "  inflating: data_80k/output_images/7952.png  \n",
            "  inflating: data_80k/output_images/79520.png  \n",
            "  inflating: data_80k/output_images/79521.png  \n",
            "  inflating: data_80k/output_images/79522.png  \n",
            "  inflating: data_80k/output_images/79523.png  \n",
            "  inflating: data_80k/output_images/79524.png  \n",
            "  inflating: data_80k/output_images/79525.png  \n",
            "  inflating: data_80k/output_images/79526.png  \n",
            "  inflating: data_80k/output_images/79527.png  \n",
            "  inflating: data_80k/output_images/79528.png  \n",
            "  inflating: data_80k/output_images/79529.png  \n",
            "  inflating: data_80k/output_images/7953.png  \n",
            "  inflating: data_80k/output_images/79530.png  \n",
            "  inflating: data_80k/output_images/79531.png  \n",
            "  inflating: data_80k/output_images/79532.png  \n",
            "  inflating: data_80k/output_images/79533.png  \n",
            "  inflating: data_80k/output_images/79534.png  \n",
            "  inflating: data_80k/output_images/79535.png  \n",
            "  inflating: data_80k/output_images/79536.png  \n",
            "  inflating: data_80k/output_images/79537.png  \n",
            "  inflating: data_80k/output_images/79538.png  \n",
            "  inflating: data_80k/output_images/79539.png  \n",
            "  inflating: data_80k/output_images/7954.png  \n",
            "  inflating: data_80k/output_images/79540.png  \n",
            "  inflating: data_80k/output_images/79541.png  \n",
            "  inflating: data_80k/output_images/79542.png  \n",
            "  inflating: data_80k/output_images/79543.png  \n",
            "  inflating: data_80k/output_images/79544.png  \n",
            "  inflating: data_80k/output_images/79545.png  \n",
            "  inflating: data_80k/output_images/79546.png  \n",
            "  inflating: data_80k/output_images/79547.png  \n",
            "  inflating: data_80k/output_images/79548.png  \n",
            "  inflating: data_80k/output_images/79549.png  \n",
            "  inflating: data_80k/output_images/7955.png  \n",
            "  inflating: data_80k/output_images/79550.png  \n",
            "  inflating: data_80k/output_images/79551.png  \n",
            "  inflating: data_80k/output_images/79552.png  \n",
            "  inflating: data_80k/output_images/79553.png  \n",
            "  inflating: data_80k/output_images/79554.png  \n",
            "  inflating: data_80k/output_images/79555.png  \n",
            "  inflating: data_80k/output_images/79556.png  \n",
            "  inflating: data_80k/output_images/79557.png  \n",
            "  inflating: data_80k/output_images/79558.png  \n",
            "  inflating: data_80k/output_images/79559.png  \n",
            "  inflating: data_80k/output_images/7956.png  \n",
            "  inflating: data_80k/output_images/79560.png  \n",
            "  inflating: data_80k/output_images/79561.png  \n",
            "  inflating: data_80k/output_images/79562.png  \n",
            "  inflating: data_80k/output_images/79563.png  \n",
            "  inflating: data_80k/output_images/79564.png  \n",
            "  inflating: data_80k/output_images/79565.png  \n",
            "  inflating: data_80k/output_images/79566.png  \n",
            "  inflating: data_80k/output_images/79567.png  \n",
            "  inflating: data_80k/output_images/79568.png  \n",
            "  inflating: data_80k/output_images/79569.png  \n",
            "  inflating: data_80k/output_images/7957.png  \n",
            "  inflating: data_80k/output_images/79570.png  \n",
            "  inflating: data_80k/output_images/79571.png  \n",
            "  inflating: data_80k/output_images/79572.png  \n",
            "  inflating: data_80k/output_images/79573.png  \n",
            "  inflating: data_80k/output_images/79574.png  \n",
            "  inflating: data_80k/output_images/79575.png  \n",
            "  inflating: data_80k/output_images/79576.png  \n",
            "  inflating: data_80k/output_images/79577.png  \n",
            "  inflating: data_80k/output_images/79578.png  \n",
            "  inflating: data_80k/output_images/79579.png  \n",
            "  inflating: data_80k/output_images/7958.png  \n",
            "  inflating: data_80k/output_images/79580.png  \n",
            "  inflating: data_80k/output_images/79581.png  \n",
            "  inflating: data_80k/output_images/79582.png  \n",
            "  inflating: data_80k/output_images/79583.png  \n",
            "  inflating: data_80k/output_images/79584.png  \n",
            "  inflating: data_80k/output_images/79585.png  \n",
            "  inflating: data_80k/output_images/79586.png  \n",
            "  inflating: data_80k/output_images/79587.png  \n",
            "  inflating: data_80k/output_images/79588.png  \n",
            "  inflating: data_80k/output_images/79589.png  \n",
            "  inflating: data_80k/output_images/7959.png  \n",
            "  inflating: data_80k/output_images/79590.png  \n",
            "  inflating: data_80k/output_images/79591.png  \n",
            "  inflating: data_80k/output_images/79592.png  \n",
            "  inflating: data_80k/output_images/79593.png  \n",
            "  inflating: data_80k/output_images/79594.png  \n",
            "  inflating: data_80k/output_images/79595.png  \n",
            "  inflating: data_80k/output_images/79596.png  \n",
            "  inflating: data_80k/output_images/79597.png  \n",
            "  inflating: data_80k/output_images/79598.png  \n",
            "  inflating: data_80k/output_images/79599.png  \n",
            "  inflating: data_80k/output_images/796.png  \n",
            "  inflating: data_80k/output_images/7960.png  \n",
            "  inflating: data_80k/output_images/79600.png  \n",
            "  inflating: data_80k/output_images/79601.png  \n",
            "  inflating: data_80k/output_images/79602.png  \n",
            "  inflating: data_80k/output_images/79603.png  \n",
            "  inflating: data_80k/output_images/79604.png  \n",
            "  inflating: data_80k/output_images/79605.png  \n",
            "  inflating: data_80k/output_images/79606.png  \n",
            "  inflating: data_80k/output_images/79607.png  \n",
            "  inflating: data_80k/output_images/79608.png  \n",
            "  inflating: data_80k/output_images/79609.png  \n",
            "  inflating: data_80k/output_images/7961.png  \n",
            "  inflating: data_80k/output_images/79610.png  \n",
            "  inflating: data_80k/output_images/79611.png  \n",
            "  inflating: data_80k/output_images/79612.png  \n",
            "  inflating: data_80k/output_images/79613.png  \n",
            "  inflating: data_80k/output_images/79614.png  \n",
            "  inflating: data_80k/output_images/79615.png  \n",
            "  inflating: data_80k/output_images/79616.png  \n",
            "  inflating: data_80k/output_images/79617.png  \n",
            "  inflating: data_80k/output_images/79618.png  \n",
            "  inflating: data_80k/output_images/79619.png  \n",
            "  inflating: data_80k/output_images/7962.png  \n",
            "  inflating: data_80k/output_images/79620.png  \n",
            "  inflating: data_80k/output_images/79621.png  \n",
            "  inflating: data_80k/output_images/79622.png  \n",
            "  inflating: data_80k/output_images/79623.png  \n",
            "  inflating: data_80k/output_images/79624.png  \n",
            "  inflating: data_80k/output_images/79625.png  \n",
            "  inflating: data_80k/output_images/79626.png  \n",
            "  inflating: data_80k/output_images/79627.png  \n",
            "  inflating: data_80k/output_images/79628.png  \n",
            "  inflating: data_80k/output_images/79629.png  \n",
            "  inflating: data_80k/output_images/7963.png  \n",
            "  inflating: data_80k/output_images/79630.png  \n",
            "  inflating: data_80k/output_images/79631.png  \n",
            "  inflating: data_80k/output_images/79632.png  \n",
            "  inflating: data_80k/output_images/79633.png  \n",
            "  inflating: data_80k/output_images/79634.png  \n",
            "  inflating: data_80k/output_images/79635.png  \n",
            "  inflating: data_80k/output_images/79636.png  \n",
            "  inflating: data_80k/output_images/79637.png  \n",
            "  inflating: data_80k/output_images/79638.png  \n",
            "  inflating: data_80k/output_images/79639.png  \n",
            "  inflating: data_80k/output_images/7964.png  \n",
            "  inflating: data_80k/output_images/79640.png  \n",
            "  inflating: data_80k/output_images/79641.png  \n",
            "  inflating: data_80k/output_images/79642.png  \n",
            "  inflating: data_80k/output_images/79643.png  \n",
            "  inflating: data_80k/output_images/79644.png  \n",
            "  inflating: data_80k/output_images/79645.png  \n",
            "  inflating: data_80k/output_images/79646.png  \n",
            "  inflating: data_80k/output_images/79647.png  \n",
            "  inflating: data_80k/output_images/79648.png  \n",
            "  inflating: data_80k/output_images/79649.png  \n",
            "  inflating: data_80k/output_images/7965.png  \n",
            "  inflating: data_80k/output_images/79650.png  \n",
            "  inflating: data_80k/output_images/79651.png  \n",
            "  inflating: data_80k/output_images/79652.png  \n",
            "  inflating: data_80k/output_images/79653.png  \n",
            "  inflating: data_80k/output_images/79654.png  \n",
            "  inflating: data_80k/output_images/79655.png  \n",
            "  inflating: data_80k/output_images/79656.png  \n",
            "  inflating: data_80k/output_images/79657.png  \n",
            "  inflating: data_80k/output_images/79658.png  \n",
            "  inflating: data_80k/output_images/79659.png  \n",
            "  inflating: data_80k/output_images/7966.png  \n",
            "  inflating: data_80k/output_images/79660.png  \n",
            "  inflating: data_80k/output_images/79661.png  \n",
            "  inflating: data_80k/output_images/79662.png  \n",
            "  inflating: data_80k/output_images/79663.png  \n",
            "  inflating: data_80k/output_images/79664.png  \n",
            "  inflating: data_80k/output_images/79665.png  \n",
            "  inflating: data_80k/output_images/79666.png  \n",
            "  inflating: data_80k/output_images/79667.png  \n",
            "  inflating: data_80k/output_images/79668.png  \n",
            "  inflating: data_80k/output_images/79669.png  \n",
            "  inflating: data_80k/output_images/7967.png  \n",
            "  inflating: data_80k/output_images/79670.png  \n",
            "  inflating: data_80k/output_images/79671.png  \n",
            "  inflating: data_80k/output_images/79672.png  \n",
            "  inflating: data_80k/output_images/79673.png  \n",
            "  inflating: data_80k/output_images/79674.png  \n",
            "  inflating: data_80k/output_images/79675.png  \n",
            "  inflating: data_80k/output_images/79676.png  \n",
            "  inflating: data_80k/output_images/79677.png  \n",
            "  inflating: data_80k/output_images/79678.png  \n",
            "  inflating: data_80k/output_images/79679.png  \n",
            "  inflating: data_80k/output_images/7968.png  \n",
            "  inflating: data_80k/output_images/79680.png  \n",
            "  inflating: data_80k/output_images/79681.png  \n",
            "  inflating: data_80k/output_images/79682.png  \n",
            "  inflating: data_80k/output_images/79683.png  \n",
            "  inflating: data_80k/output_images/79684.png  \n",
            "  inflating: data_80k/output_images/79685.png  \n",
            "  inflating: data_80k/output_images/79686.png  \n",
            "  inflating: data_80k/output_images/79687.png  \n",
            "  inflating: data_80k/output_images/79688.png  \n",
            "  inflating: data_80k/output_images/79689.png  \n",
            "  inflating: data_80k/output_images/7969.png  \n",
            "  inflating: data_80k/output_images/79690.png  \n",
            "  inflating: data_80k/output_images/79691.png  \n",
            "  inflating: data_80k/output_images/79692.png  \n",
            "  inflating: data_80k/output_images/79693.png  \n",
            "  inflating: data_80k/output_images/79694.png  \n",
            "  inflating: data_80k/output_images/79695.png  \n",
            "  inflating: data_80k/output_images/79696.png  \n",
            "  inflating: data_80k/output_images/79697.png  \n",
            "  inflating: data_80k/output_images/79698.png  \n",
            "  inflating: data_80k/output_images/79699.png  \n",
            "  inflating: data_80k/output_images/797.png  \n",
            "  inflating: data_80k/output_images/7970.png  \n",
            "  inflating: data_80k/output_images/79700.png  \n",
            "  inflating: data_80k/output_images/79701.png  \n",
            "  inflating: data_80k/output_images/79702.png  \n",
            "  inflating: data_80k/output_images/79703.png  \n",
            "  inflating: data_80k/output_images/79704.png  \n",
            "  inflating: data_80k/output_images/79705.png  \n",
            "  inflating: data_80k/output_images/79706.png  \n",
            "  inflating: data_80k/output_images/79707.png  \n",
            "  inflating: data_80k/output_images/79708.png  \n",
            "  inflating: data_80k/output_images/79709.png  \n",
            "  inflating: data_80k/output_images/7971.png  \n",
            "  inflating: data_80k/output_images/79710.png  \n",
            "  inflating: data_80k/output_images/79711.png  \n",
            "  inflating: data_80k/output_images/79712.png  \n",
            "  inflating: data_80k/output_images/79713.png  \n",
            "  inflating: data_80k/output_images/79714.png  \n",
            "  inflating: data_80k/output_images/79715.png  \n",
            "  inflating: data_80k/output_images/79716.png  \n",
            "  inflating: data_80k/output_images/79717.png  \n",
            "  inflating: data_80k/output_images/79718.png  \n",
            "  inflating: data_80k/output_images/79719.png  \n",
            "  inflating: data_80k/output_images/7972.png  \n",
            "  inflating: data_80k/output_images/79720.png  \n",
            "  inflating: data_80k/output_images/79721.png  \n",
            "  inflating: data_80k/output_images/79722.png  \n",
            "  inflating: data_80k/output_images/79723.png  \n",
            "  inflating: data_80k/output_images/79724.png  \n",
            "  inflating: data_80k/output_images/79725.png  \n",
            "  inflating: data_80k/output_images/79726.png  \n",
            "  inflating: data_80k/output_images/79727.png  \n",
            "  inflating: data_80k/output_images/79728.png  \n",
            "  inflating: data_80k/output_images/79729.png  \n",
            "  inflating: data_80k/output_images/7973.png  \n",
            "  inflating: data_80k/output_images/79730.png  \n",
            "  inflating: data_80k/output_images/79731.png  \n",
            "  inflating: data_80k/output_images/79732.png  \n",
            "  inflating: data_80k/output_images/79733.png  \n",
            "  inflating: data_80k/output_images/79734.png  \n",
            "  inflating: data_80k/output_images/79735.png  \n",
            "  inflating: data_80k/output_images/79736.png  \n",
            "  inflating: data_80k/output_images/79737.png  \n",
            "  inflating: data_80k/output_images/79738.png  \n",
            "  inflating: data_80k/output_images/79739.png  \n",
            "  inflating: data_80k/output_images/7974.png  \n",
            "  inflating: data_80k/output_images/79740.png  \n",
            "  inflating: data_80k/output_images/79741.png  \n",
            "  inflating: data_80k/output_images/79742.png  \n",
            "  inflating: data_80k/output_images/79743.png  \n",
            "  inflating: data_80k/output_images/79744.png  \n",
            "  inflating: data_80k/output_images/79745.png  \n",
            "  inflating: data_80k/output_images/79746.png  \n",
            "  inflating: data_80k/output_images/79747.png  \n",
            "  inflating: data_80k/output_images/79748.png  \n",
            "  inflating: data_80k/output_images/79749.png  \n",
            "  inflating: data_80k/output_images/7975.png  \n",
            "  inflating: data_80k/output_images/79750.png  \n",
            "  inflating: data_80k/output_images/79751.png  \n",
            "  inflating: data_80k/output_images/79752.png  \n",
            "  inflating: data_80k/output_images/79753.png  \n",
            "  inflating: data_80k/output_images/79754.png  \n",
            "  inflating: data_80k/output_images/79755.png  \n",
            "  inflating: data_80k/output_images/79756.png  \n",
            "  inflating: data_80k/output_images/79757.png  \n",
            "  inflating: data_80k/output_images/79758.png  \n",
            "  inflating: data_80k/output_images/79759.png  \n",
            "  inflating: data_80k/output_images/7976.png  \n",
            "  inflating: data_80k/output_images/79760.png  \n",
            "  inflating: data_80k/output_images/79761.png  \n",
            "  inflating: data_80k/output_images/79762.png  \n",
            "  inflating: data_80k/output_images/79763.png  \n",
            "  inflating: data_80k/output_images/79764.png  \n",
            "  inflating: data_80k/output_images/79765.png  \n",
            "  inflating: data_80k/output_images/79766.png  \n",
            "  inflating: data_80k/output_images/79767.png  \n",
            "  inflating: data_80k/output_images/79768.png  \n",
            "  inflating: data_80k/output_images/79769.png  \n",
            "  inflating: data_80k/output_images/7977.png  \n",
            "  inflating: data_80k/output_images/79770.png  \n",
            "  inflating: data_80k/output_images/79771.png  \n",
            "  inflating: data_80k/output_images/79772.png  \n",
            "  inflating: data_80k/output_images/79773.png  \n",
            "  inflating: data_80k/output_images/79774.png  \n",
            "  inflating: data_80k/output_images/79775.png  \n",
            "  inflating: data_80k/output_images/79776.png  \n",
            "  inflating: data_80k/output_images/79777.png  \n",
            "  inflating: data_80k/output_images/79778.png  \n",
            "  inflating: data_80k/output_images/79779.png  \n",
            "  inflating: data_80k/output_images/7978.png  \n",
            "  inflating: data_80k/output_images/79780.png  \n",
            "  inflating: data_80k/output_images/79781.png  \n",
            "  inflating: data_80k/output_images/79782.png  \n",
            "  inflating: data_80k/output_images/79783.png  \n",
            "  inflating: data_80k/output_images/79784.png  \n",
            "  inflating: data_80k/output_images/79785.png  \n",
            "  inflating: data_80k/output_images/79786.png  \n",
            "  inflating: data_80k/output_images/79787.png  \n",
            "  inflating: data_80k/output_images/79788.png  \n",
            "  inflating: data_80k/output_images/79789.png  \n",
            "  inflating: data_80k/output_images/7979.png  \n",
            "  inflating: data_80k/output_images/79790.png  \n",
            "  inflating: data_80k/output_images/79791.png  \n",
            "  inflating: data_80k/output_images/79792.png  \n",
            "  inflating: data_80k/output_images/79793.png  \n",
            "  inflating: data_80k/output_images/79794.png  \n",
            "  inflating: data_80k/output_images/79795.png  \n",
            "  inflating: data_80k/output_images/79796.png  \n",
            "  inflating: data_80k/output_images/79797.png  \n",
            "  inflating: data_80k/output_images/79798.png  \n",
            "  inflating: data_80k/output_images/79799.png  \n",
            "  inflating: data_80k/output_images/798.png  \n",
            "  inflating: data_80k/output_images/7980.png  \n",
            "  inflating: data_80k/output_images/79800.png  \n",
            "  inflating: data_80k/output_images/79801.png  \n",
            "  inflating: data_80k/output_images/79802.png  \n",
            "  inflating: data_80k/output_images/79803.png  \n",
            "  inflating: data_80k/output_images/79804.png  \n",
            "  inflating: data_80k/output_images/79805.png  \n",
            "  inflating: data_80k/output_images/79806.png  \n",
            "  inflating: data_80k/output_images/79807.png  \n",
            "  inflating: data_80k/output_images/79808.png  \n",
            "  inflating: data_80k/output_images/79809.png  \n",
            "  inflating: data_80k/output_images/7981.png  \n",
            "  inflating: data_80k/output_images/79810.png  \n",
            "  inflating: data_80k/output_images/79811.png  \n",
            "  inflating: data_80k/output_images/79812.png  \n",
            "  inflating: data_80k/output_images/79813.png  \n",
            "  inflating: data_80k/output_images/79814.png  \n",
            "  inflating: data_80k/output_images/79815.png  \n",
            "  inflating: data_80k/output_images/79816.png  \n",
            "  inflating: data_80k/output_images/79817.png  \n",
            "  inflating: data_80k/output_images/79818.png  \n",
            "  inflating: data_80k/output_images/79819.png  \n",
            "  inflating: data_80k/output_images/7982.png  \n",
            "  inflating: data_80k/output_images/79820.png  \n",
            "  inflating: data_80k/output_images/79821.png  \n",
            "  inflating: data_80k/output_images/79822.png  \n",
            "  inflating: data_80k/output_images/79823.png  \n",
            "  inflating: data_80k/output_images/79824.png  \n",
            "  inflating: data_80k/output_images/79825.png  \n",
            "  inflating: data_80k/output_images/79826.png  \n",
            "  inflating: data_80k/output_images/79827.png  \n",
            "  inflating: data_80k/output_images/79828.png  \n",
            "  inflating: data_80k/output_images/79829.png  \n",
            "  inflating: data_80k/output_images/7983.png  \n",
            "  inflating: data_80k/output_images/79830.png  \n",
            "  inflating: data_80k/output_images/79831.png  \n",
            "  inflating: data_80k/output_images/79832.png  \n",
            "  inflating: data_80k/output_images/79833.png  \n",
            "  inflating: data_80k/output_images/79834.png  \n",
            "  inflating: data_80k/output_images/79835.png  \n",
            "  inflating: data_80k/output_images/79836.png  \n",
            "  inflating: data_80k/output_images/79837.png  \n",
            "  inflating: data_80k/output_images/79838.png  \n",
            "  inflating: data_80k/output_images/79839.png  \n",
            "  inflating: data_80k/output_images/7984.png  \n",
            "  inflating: data_80k/output_images/79840.png  \n",
            "  inflating: data_80k/output_images/79841.png  \n",
            "  inflating: data_80k/output_images/79842.png  \n",
            "  inflating: data_80k/output_images/79843.png  \n",
            "  inflating: data_80k/output_images/79844.png  \n",
            "  inflating: data_80k/output_images/79845.png  \n",
            "  inflating: data_80k/output_images/79846.png  \n",
            "  inflating: data_80k/output_images/79847.png  \n",
            "  inflating: data_80k/output_images/79848.png  \n",
            "  inflating: data_80k/output_images/79849.png  \n",
            "  inflating: data_80k/output_images/7985.png  \n",
            "  inflating: data_80k/output_images/79850.png  \n",
            "  inflating: data_80k/output_images/79851.png  \n",
            "  inflating: data_80k/output_images/79852.png  \n",
            "  inflating: data_80k/output_images/79853.png  \n",
            "  inflating: data_80k/output_images/79854.png  \n",
            "  inflating: data_80k/output_images/79855.png  \n",
            "  inflating: data_80k/output_images/79856.png  \n",
            "  inflating: data_80k/output_images/79857.png  \n",
            "  inflating: data_80k/output_images/79858.png  \n",
            "  inflating: data_80k/output_images/79859.png  \n",
            "  inflating: data_80k/output_images/7986.png  \n",
            "  inflating: data_80k/output_images/79860.png  \n",
            "  inflating: data_80k/output_images/79861.png  \n",
            "  inflating: data_80k/output_images/79862.png  \n",
            "  inflating: data_80k/output_images/79863.png  \n",
            "  inflating: data_80k/output_images/79864.png  \n",
            "  inflating: data_80k/output_images/79865.png  \n",
            "  inflating: data_80k/output_images/79866.png  \n",
            "  inflating: data_80k/output_images/79867.png  \n",
            "  inflating: data_80k/output_images/79868.png  \n",
            "  inflating: data_80k/output_images/79869.png  \n",
            "  inflating: data_80k/output_images/7987.png  \n",
            "  inflating: data_80k/output_images/79870.png  \n",
            "  inflating: data_80k/output_images/79871.png  \n",
            "  inflating: data_80k/output_images/79872.png  \n",
            "  inflating: data_80k/output_images/79873.png  \n",
            "  inflating: data_80k/output_images/79874.png  \n",
            "  inflating: data_80k/output_images/79875.png  \n",
            "  inflating: data_80k/output_images/79876.png  \n",
            "  inflating: data_80k/output_images/79877.png  \n",
            "  inflating: data_80k/output_images/79878.png  \n",
            "  inflating: data_80k/output_images/79879.png  \n",
            "  inflating: data_80k/output_images/7988.png  \n",
            "  inflating: data_80k/output_images/79880.png  \n",
            "  inflating: data_80k/output_images/79881.png  \n",
            "  inflating: data_80k/output_images/79882.png  \n",
            "  inflating: data_80k/output_images/79883.png  \n",
            "  inflating: data_80k/output_images/79884.png  \n",
            "  inflating: data_80k/output_images/79885.png  \n",
            "  inflating: data_80k/output_images/79886.png  \n",
            "  inflating: data_80k/output_images/79887.png  \n",
            "  inflating: data_80k/output_images/79888.png  \n",
            "  inflating: data_80k/output_images/79889.png  \n",
            "  inflating: data_80k/output_images/7989.png  \n",
            "  inflating: data_80k/output_images/79890.png  \n",
            "  inflating: data_80k/output_images/79891.png  \n",
            "  inflating: data_80k/output_images/79892.png  \n",
            "  inflating: data_80k/output_images/79893.png  \n",
            "  inflating: data_80k/output_images/79894.png  \n",
            "  inflating: data_80k/output_images/79895.png  \n",
            "  inflating: data_80k/output_images/79896.png  \n",
            "  inflating: data_80k/output_images/79897.png  \n",
            "  inflating: data_80k/output_images/79898.png  \n",
            "  inflating: data_80k/output_images/79899.png  \n",
            "  inflating: data_80k/output_images/799.png  \n",
            "  inflating: data_80k/output_images/7990.png  \n",
            "  inflating: data_80k/output_images/79900.png  \n",
            "  inflating: data_80k/output_images/79901.png  \n",
            "  inflating: data_80k/output_images/79902.png  \n",
            "  inflating: data_80k/output_images/79903.png  \n",
            "  inflating: data_80k/output_images/79904.png  \n",
            "  inflating: data_80k/output_images/79905.png  \n",
            "  inflating: data_80k/output_images/79906.png  \n",
            "  inflating: data_80k/output_images/79907.png  \n",
            "  inflating: data_80k/output_images/79908.png  \n",
            "  inflating: data_80k/output_images/79909.png  \n",
            "  inflating: data_80k/output_images/7991.png  \n",
            "  inflating: data_80k/output_images/79910.png  \n",
            "  inflating: data_80k/output_images/79911.png  \n",
            "  inflating: data_80k/output_images/79912.png  \n",
            "  inflating: data_80k/output_images/79913.png  \n",
            "  inflating: data_80k/output_images/79914.png  \n",
            "  inflating: data_80k/output_images/79915.png  \n",
            "  inflating: data_80k/output_images/79916.png  \n",
            "  inflating: data_80k/output_images/79917.png  \n",
            "  inflating: data_80k/output_images/79918.png  \n",
            "  inflating: data_80k/output_images/79919.png  \n",
            "  inflating: data_80k/output_images/7992.png  \n",
            "  inflating: data_80k/output_images/79920.png  \n",
            "  inflating: data_80k/output_images/79921.png  \n",
            "  inflating: data_80k/output_images/79922.png  \n",
            "  inflating: data_80k/output_images/79923.png  \n",
            "  inflating: data_80k/output_images/79924.png  \n",
            "  inflating: data_80k/output_images/79925.png  \n",
            "  inflating: data_80k/output_images/79926.png  \n",
            "  inflating: data_80k/output_images/79927.png  \n",
            "  inflating: data_80k/output_images/79928.png  \n",
            "  inflating: data_80k/output_images/79929.png  \n",
            "  inflating: data_80k/output_images/7993.png  \n",
            "  inflating: data_80k/output_images/79930.png  \n",
            "  inflating: data_80k/output_images/79931.png  \n",
            "  inflating: data_80k/output_images/79932.png  \n",
            "  inflating: data_80k/output_images/79933.png  \n",
            "  inflating: data_80k/output_images/79934.png  \n",
            "  inflating: data_80k/output_images/79935.png  \n",
            "  inflating: data_80k/output_images/79936.png  \n",
            "  inflating: data_80k/output_images/79937.png  \n",
            "  inflating: data_80k/output_images/79938.png  \n",
            "  inflating: data_80k/output_images/79939.png  \n",
            "  inflating: data_80k/output_images/7994.png  \n",
            "  inflating: data_80k/output_images/79940.png  \n",
            "  inflating: data_80k/output_images/79941.png  \n",
            "  inflating: data_80k/output_images/79942.png  \n",
            "  inflating: data_80k/output_images/79943.png  \n",
            "  inflating: data_80k/output_images/79944.png  \n",
            "  inflating: data_80k/output_images/79945.png  \n",
            "  inflating: data_80k/output_images/79946.png  \n",
            "  inflating: data_80k/output_images/79947.png  \n",
            "  inflating: data_80k/output_images/79948.png  \n",
            "  inflating: data_80k/output_images/79949.png  \n",
            "  inflating: data_80k/output_images/7995.png  \n",
            "  inflating: data_80k/output_images/79950.png  \n",
            "  inflating: data_80k/output_images/79951.png  \n",
            "  inflating: data_80k/output_images/79952.png  \n",
            "  inflating: data_80k/output_images/79953.png  \n",
            "  inflating: data_80k/output_images/79954.png  \n",
            "  inflating: data_80k/output_images/79955.png  \n",
            "  inflating: data_80k/output_images/79956.png  \n",
            "  inflating: data_80k/output_images/79957.png  \n",
            "  inflating: data_80k/output_images/79958.png  \n",
            "  inflating: data_80k/output_images/79959.png  \n",
            "  inflating: data_80k/output_images/7996.png  \n",
            "  inflating: data_80k/output_images/79960.png  \n",
            "  inflating: data_80k/output_images/79961.png  \n",
            "  inflating: data_80k/output_images/79962.png  \n",
            "  inflating: data_80k/output_images/79963.png  \n",
            "  inflating: data_80k/output_images/79964.png  \n",
            "  inflating: data_80k/output_images/79965.png  \n",
            "  inflating: data_80k/output_images/79966.png  \n",
            "  inflating: data_80k/output_images/79967.png  \n",
            "  inflating: data_80k/output_images/79968.png  \n",
            "  inflating: data_80k/output_images/79969.png  \n",
            "  inflating: data_80k/output_images/7997.png  \n",
            "  inflating: data_80k/output_images/79970.png  \n",
            "  inflating: data_80k/output_images/79971.png  \n",
            "  inflating: data_80k/output_images/79972.png  \n",
            "  inflating: data_80k/output_images/79973.png  \n",
            "  inflating: data_80k/output_images/79974.png  \n",
            "  inflating: data_80k/output_images/79975.png  \n",
            "  inflating: data_80k/output_images/79976.png  \n",
            "  inflating: data_80k/output_images/79977.png  \n",
            "  inflating: data_80k/output_images/79978.png  \n",
            "  inflating: data_80k/output_images/79979.png  \n",
            "  inflating: data_80k/output_images/7998.png  \n",
            "  inflating: data_80k/output_images/79980.png  \n",
            "  inflating: data_80k/output_images/79981.png  \n",
            "  inflating: data_80k/output_images/79982.png  \n",
            "  inflating: data_80k/output_images/79983.png  \n",
            "  inflating: data_80k/output_images/79984.png  \n",
            "  inflating: data_80k/output_images/79985.png  \n",
            "  inflating: data_80k/output_images/79986.png  \n",
            "  inflating: data_80k/output_images/79987.png  \n",
            "  inflating: data_80k/output_images/79988.png  \n",
            "  inflating: data_80k/output_images/79989.png  \n",
            "  inflating: data_80k/output_images/7999.png  \n",
            "  inflating: data_80k/output_images/79990.png  \n",
            "  inflating: data_80k/output_images/79991.png  \n",
            "  inflating: data_80k/output_images/79992.png  \n",
            "  inflating: data_80k/output_images/79993.png  \n",
            "  inflating: data_80k/output_images/79994.png  \n",
            "  inflating: data_80k/output_images/79995.png  \n",
            "  inflating: data_80k/output_images/79996.png  \n",
            "  inflating: data_80k/output_images/79997.png  \n",
            "  inflating: data_80k/output_images/79998.png  \n",
            "  inflating: data_80k/output_images/79999.png  \n",
            "  inflating: data_80k/output_images/8.png  \n",
            "  inflating: data_80k/output_images/80.png  \n",
            "  inflating: data_80k/output_images/800.png  \n",
            "  inflating: data_80k/output_images/8000.png  \n",
            "  inflating: data_80k/output_images/80000.png  \n",
            "  inflating: data_80k/output_images/8001.png  \n",
            "  inflating: data_80k/output_images/8002.png  \n",
            "  inflating: data_80k/output_images/8003.png  \n",
            "  inflating: data_80k/output_images/8004.png  \n",
            "  inflating: data_80k/output_images/8005.png  \n",
            "  inflating: data_80k/output_images/8006.png  \n",
            "  inflating: data_80k/output_images/8007.png  \n",
            "  inflating: data_80k/output_images/8008.png  \n",
            "  inflating: data_80k/output_images/8009.png  \n",
            "  inflating: data_80k/output_images/801.png  \n",
            "  inflating: data_80k/output_images/8010.png  \n",
            "  inflating: data_80k/output_images/8011.png  \n",
            "  inflating: data_80k/output_images/8012.png  \n",
            "  inflating: data_80k/output_images/8013.png  \n",
            "  inflating: data_80k/output_images/8014.png  \n",
            "  inflating: data_80k/output_images/8015.png  \n",
            "  inflating: data_80k/output_images/8016.png  \n",
            "  inflating: data_80k/output_images/8017.png  \n",
            "  inflating: data_80k/output_images/8018.png  \n",
            "  inflating: data_80k/output_images/8019.png  \n",
            "  inflating: data_80k/output_images/802.png  \n",
            "  inflating: data_80k/output_images/8020.png  \n",
            "  inflating: data_80k/output_images/8021.png  \n",
            "  inflating: data_80k/output_images/8022.png  \n",
            "  inflating: data_80k/output_images/8023.png  \n",
            "  inflating: data_80k/output_images/8024.png  \n",
            "  inflating: data_80k/output_images/8025.png  \n",
            "  inflating: data_80k/output_images/8026.png  \n",
            "  inflating: data_80k/output_images/8027.png  \n",
            "  inflating: data_80k/output_images/8028.png  \n",
            "  inflating: data_80k/output_images/8029.png  \n",
            "  inflating: data_80k/output_images/803.png  \n",
            "  inflating: data_80k/output_images/8030.png  \n",
            "  inflating: data_80k/output_images/8031.png  \n",
            "  inflating: data_80k/output_images/8032.png  \n",
            "  inflating: data_80k/output_images/8033.png  \n",
            "  inflating: data_80k/output_images/8034.png  \n",
            "  inflating: data_80k/output_images/8035.png  \n",
            "  inflating: data_80k/output_images/8036.png  \n",
            "  inflating: data_80k/output_images/8037.png  \n",
            "  inflating: data_80k/output_images/8038.png  \n",
            "  inflating: data_80k/output_images/8039.png  \n",
            "  inflating: data_80k/output_images/804.png  \n",
            "  inflating: data_80k/output_images/8040.png  \n",
            "  inflating: data_80k/output_images/8041.png  \n",
            "  inflating: data_80k/output_images/8042.png  \n",
            "  inflating: data_80k/output_images/8043.png  \n",
            "  inflating: data_80k/output_images/8044.png  \n",
            "  inflating: data_80k/output_images/8045.png  \n",
            "  inflating: data_80k/output_images/8046.png  \n",
            "  inflating: data_80k/output_images/8047.png  \n",
            "  inflating: data_80k/output_images/8048.png  \n",
            "  inflating: data_80k/output_images/8049.png  \n",
            "  inflating: data_80k/output_images/805.png  \n",
            "  inflating: data_80k/output_images/8050.png  \n",
            "  inflating: data_80k/output_images/8051.png  \n",
            "  inflating: data_80k/output_images/8052.png  \n",
            "  inflating: data_80k/output_images/8053.png  \n",
            "  inflating: data_80k/output_images/8054.png  \n",
            "  inflating: data_80k/output_images/8055.png  \n",
            "  inflating: data_80k/output_images/8056.png  \n",
            "  inflating: data_80k/output_images/8057.png  \n",
            "  inflating: data_80k/output_images/8058.png  \n",
            "  inflating: data_80k/output_images/8059.png  \n",
            "  inflating: data_80k/output_images/806.png  \n",
            "  inflating: data_80k/output_images/8060.png  \n",
            "  inflating: data_80k/output_images/8061.png  \n",
            "  inflating: data_80k/output_images/8062.png  \n",
            "  inflating: data_80k/output_images/8063.png  \n",
            "  inflating: data_80k/output_images/8064.png  \n",
            "  inflating: data_80k/output_images/8065.png  \n",
            "  inflating: data_80k/output_images/8066.png  \n",
            "  inflating: data_80k/output_images/8067.png  \n",
            "  inflating: data_80k/output_images/8068.png  \n",
            "  inflating: data_80k/output_images/8069.png  \n",
            "  inflating: data_80k/output_images/807.png  \n",
            "  inflating: data_80k/output_images/8070.png  \n",
            "  inflating: data_80k/output_images/8071.png  \n",
            "  inflating: data_80k/output_images/8072.png  \n",
            "  inflating: data_80k/output_images/8073.png  \n",
            "  inflating: data_80k/output_images/8074.png  \n",
            "  inflating: data_80k/output_images/8075.png  \n",
            "  inflating: data_80k/output_images/8076.png  \n",
            "  inflating: data_80k/output_images/8077.png  \n",
            "  inflating: data_80k/output_images/8078.png  \n",
            "  inflating: data_80k/output_images/8079.png  \n",
            "  inflating: data_80k/output_images/808.png  \n",
            "  inflating: data_80k/output_images/8080.png  \n",
            "  inflating: data_80k/output_images/8081.png  \n",
            "  inflating: data_80k/output_images/8082.png  \n",
            "  inflating: data_80k/output_images/8083.png  \n",
            "  inflating: data_80k/output_images/8084.png  \n",
            "  inflating: data_80k/output_images/8085.png  \n",
            "  inflating: data_80k/output_images/8086.png  \n",
            "  inflating: data_80k/output_images/8087.png  \n",
            "  inflating: data_80k/output_images/8088.png  \n",
            "  inflating: data_80k/output_images/8089.png  \n",
            "  inflating: data_80k/output_images/809.png  \n",
            "  inflating: data_80k/output_images/8090.png  \n",
            "  inflating: data_80k/output_images/8091.png  \n",
            "  inflating: data_80k/output_images/8092.png  \n",
            "  inflating: data_80k/output_images/8093.png  \n",
            "  inflating: data_80k/output_images/8094.png  \n",
            "  inflating: data_80k/output_images/8095.png  \n",
            "  inflating: data_80k/output_images/8096.png  \n",
            "  inflating: data_80k/output_images/8097.png  \n",
            "  inflating: data_80k/output_images/8098.png  \n",
            "  inflating: data_80k/output_images/8099.png  \n",
            "  inflating: data_80k/output_images/81.png  \n",
            "  inflating: data_80k/output_images/810.png  \n",
            "  inflating: data_80k/output_images/8100.png  \n",
            "  inflating: data_80k/output_images/8101.png  \n",
            "  inflating: data_80k/output_images/8102.png  \n",
            "  inflating: data_80k/output_images/8103.png  \n",
            "  inflating: data_80k/output_images/8104.png  \n",
            "  inflating: data_80k/output_images/8105.png  \n",
            "  inflating: data_80k/output_images/8106.png  \n",
            "  inflating: data_80k/output_images/8107.png  \n",
            "  inflating: data_80k/output_images/8108.png  \n",
            "  inflating: data_80k/output_images/8109.png  \n",
            "  inflating: data_80k/output_images/811.png  \n",
            "  inflating: data_80k/output_images/8110.png  \n",
            "  inflating: data_80k/output_images/8111.png  \n",
            "  inflating: data_80k/output_images/8112.png  \n",
            "  inflating: data_80k/output_images/8113.png  \n",
            "  inflating: data_80k/output_images/8114.png  \n",
            "  inflating: data_80k/output_images/8115.png  \n",
            "  inflating: data_80k/output_images/8116.png  \n",
            "  inflating: data_80k/output_images/8117.png  \n",
            "  inflating: data_80k/output_images/8118.png  \n",
            "  inflating: data_80k/output_images/8119.png  \n",
            "  inflating: data_80k/output_images/812.png  \n",
            "  inflating: data_80k/output_images/8120.png  \n",
            "  inflating: data_80k/output_images/8121.png  \n",
            "  inflating: data_80k/output_images/8122.png  \n",
            "  inflating: data_80k/output_images/8123.png  \n",
            "  inflating: data_80k/output_images/8124.png  \n",
            "  inflating: data_80k/output_images/8125.png  \n",
            "  inflating: data_80k/output_images/8126.png  \n",
            "  inflating: data_80k/output_images/8127.png  \n",
            "  inflating: data_80k/output_images/8128.png  \n",
            "  inflating: data_80k/output_images/8129.png  \n",
            "  inflating: data_80k/output_images/813.png  \n",
            "  inflating: data_80k/output_images/8130.png  \n",
            "  inflating: data_80k/output_images/8131.png  \n",
            "  inflating: data_80k/output_images/8132.png  \n",
            "  inflating: data_80k/output_images/8133.png  \n",
            "  inflating: data_80k/output_images/8134.png  \n",
            "  inflating: data_80k/output_images/8135.png  \n",
            "  inflating: data_80k/output_images/8136.png  \n",
            "  inflating: data_80k/output_images/8137.png  \n",
            "  inflating: data_80k/output_images/8138.png  \n",
            "  inflating: data_80k/output_images/8139.png  \n",
            "  inflating: data_80k/output_images/814.png  \n",
            "  inflating: data_80k/output_images/8140.png  \n",
            "  inflating: data_80k/output_images/8141.png  \n",
            "  inflating: data_80k/output_images/8142.png  \n",
            "  inflating: data_80k/output_images/8143.png  \n",
            "  inflating: data_80k/output_images/8144.png  \n",
            "  inflating: data_80k/output_images/8145.png  \n",
            "  inflating: data_80k/output_images/8146.png  \n",
            "  inflating: data_80k/output_images/8147.png  \n",
            "  inflating: data_80k/output_images/8148.png  \n",
            "  inflating: data_80k/output_images/8149.png  \n",
            "  inflating: data_80k/output_images/815.png  \n",
            "  inflating: data_80k/output_images/8150.png  \n",
            "  inflating: data_80k/output_images/8151.png  \n",
            "  inflating: data_80k/output_images/8152.png  \n",
            "  inflating: data_80k/output_images/8153.png  \n",
            "  inflating: data_80k/output_images/8154.png  \n",
            "  inflating: data_80k/output_images/8155.png  \n",
            "  inflating: data_80k/output_images/8156.png  \n",
            "  inflating: data_80k/output_images/8157.png  \n",
            "  inflating: data_80k/output_images/8158.png  \n",
            "  inflating: data_80k/output_images/8159.png  \n",
            "  inflating: data_80k/output_images/816.png  \n",
            "  inflating: data_80k/output_images/8160.png  \n",
            "  inflating: data_80k/output_images/8161.png  \n",
            "  inflating: data_80k/output_images/8162.png  \n",
            "  inflating: data_80k/output_images/8163.png  \n",
            "  inflating: data_80k/output_images/8164.png  \n",
            "  inflating: data_80k/output_images/8165.png  \n",
            "  inflating: data_80k/output_images/8166.png  \n",
            "  inflating: data_80k/output_images/8167.png  \n",
            "  inflating: data_80k/output_images/8168.png  \n",
            "  inflating: data_80k/output_images/8169.png  \n",
            "  inflating: data_80k/output_images/817.png  \n",
            "  inflating: data_80k/output_images/8170.png  \n",
            "  inflating: data_80k/output_images/8171.png  \n",
            "  inflating: data_80k/output_images/8172.png  \n",
            "  inflating: data_80k/output_images/8173.png  \n",
            "  inflating: data_80k/output_images/8174.png  \n",
            "  inflating: data_80k/output_images/8175.png  \n",
            "  inflating: data_80k/output_images/8176.png  \n",
            "  inflating: data_80k/output_images/8177.png  \n",
            "  inflating: data_80k/output_images/8178.png  \n",
            "  inflating: data_80k/output_images/8179.png  \n",
            "  inflating: data_80k/output_images/818.png  \n",
            "  inflating: data_80k/output_images/8180.png  \n",
            "  inflating: data_80k/output_images/8181.png  \n",
            "  inflating: data_80k/output_images/8182.png  \n",
            "  inflating: data_80k/output_images/8183.png  \n",
            "  inflating: data_80k/output_images/8184.png  \n",
            "  inflating: data_80k/output_images/8185.png  \n",
            "  inflating: data_80k/output_images/8186.png  \n",
            "  inflating: data_80k/output_images/8187.png  \n",
            "  inflating: data_80k/output_images/8188.png  \n",
            "  inflating: data_80k/output_images/8189.png  \n",
            "  inflating: data_80k/output_images/819.png  \n",
            "  inflating: data_80k/output_images/8190.png  \n",
            "  inflating: data_80k/output_images/8191.png  \n",
            "  inflating: data_80k/output_images/8192.png  \n",
            "  inflating: data_80k/output_images/8193.png  \n",
            "  inflating: data_80k/output_images/8194.png  \n",
            "  inflating: data_80k/output_images/8195.png  \n",
            "  inflating: data_80k/output_images/8196.png  \n",
            "  inflating: data_80k/output_images/8197.png  \n",
            "  inflating: data_80k/output_images/8198.png  \n",
            "  inflating: data_80k/output_images/8199.png  \n",
            "  inflating: data_80k/output_images/82.png  \n",
            "  inflating: data_80k/output_images/820.png  \n",
            "  inflating: data_80k/output_images/8200.png  \n",
            "  inflating: data_80k/output_images/8201.png  \n",
            "  inflating: data_80k/output_images/8202.png  \n",
            "  inflating: data_80k/output_images/8203.png  \n",
            "  inflating: data_80k/output_images/8204.png  \n",
            "  inflating: data_80k/output_images/8205.png  \n",
            "  inflating: data_80k/output_images/8206.png  \n",
            "  inflating: data_80k/output_images/8207.png  \n",
            "  inflating: data_80k/output_images/8208.png  \n",
            "  inflating: data_80k/output_images/8209.png  \n",
            "  inflating: data_80k/output_images/821.png  \n",
            "  inflating: data_80k/output_images/8210.png  \n",
            "  inflating: data_80k/output_images/8211.png  \n",
            "  inflating: data_80k/output_images/8212.png  \n",
            "  inflating: data_80k/output_images/8213.png  \n",
            "  inflating: data_80k/output_images/8214.png  \n",
            "  inflating: data_80k/output_images/8215.png  \n",
            "  inflating: data_80k/output_images/8216.png  \n",
            "  inflating: data_80k/output_images/8217.png  \n",
            "  inflating: data_80k/output_images/8218.png  \n",
            "  inflating: data_80k/output_images/8219.png  \n",
            "  inflating: data_80k/output_images/822.png  \n",
            "  inflating: data_80k/output_images/8220.png  \n",
            "  inflating: data_80k/output_images/8221.png  \n",
            "  inflating: data_80k/output_images/8222.png  \n",
            "  inflating: data_80k/output_images/8223.png  \n",
            "  inflating: data_80k/output_images/8224.png  \n",
            "  inflating: data_80k/output_images/8225.png  \n",
            "  inflating: data_80k/output_images/8226.png  \n",
            "  inflating: data_80k/output_images/8227.png  \n",
            "  inflating: data_80k/output_images/8228.png  \n",
            "  inflating: data_80k/output_images/8229.png  \n",
            "  inflating: data_80k/output_images/823.png  \n",
            "  inflating: data_80k/output_images/8230.png  \n",
            "  inflating: data_80k/output_images/8231.png  \n",
            "  inflating: data_80k/output_images/8232.png  \n",
            "  inflating: data_80k/output_images/8233.png  \n",
            "  inflating: data_80k/output_images/8234.png  \n",
            "  inflating: data_80k/output_images/8235.png  \n",
            "  inflating: data_80k/output_images/8236.png  \n",
            "  inflating: data_80k/output_images/8237.png  \n",
            "  inflating: data_80k/output_images/8238.png  \n",
            "  inflating: data_80k/output_images/8239.png  \n",
            "  inflating: data_80k/output_images/824.png  \n",
            "  inflating: data_80k/output_images/8240.png  \n",
            "  inflating: data_80k/output_images/8241.png  \n",
            "  inflating: data_80k/output_images/8242.png  \n",
            "  inflating: data_80k/output_images/8243.png  \n",
            "  inflating: data_80k/output_images/8244.png  \n",
            "  inflating: data_80k/output_images/8245.png  \n",
            "  inflating: data_80k/output_images/8246.png  \n",
            "  inflating: data_80k/output_images/8247.png  \n",
            "  inflating: data_80k/output_images/8248.png  \n",
            "  inflating: data_80k/output_images/8249.png  \n",
            "  inflating: data_80k/output_images/825.png  \n",
            "  inflating: data_80k/output_images/8250.png  \n",
            "  inflating: data_80k/output_images/8251.png  \n",
            "  inflating: data_80k/output_images/8252.png  \n",
            "  inflating: data_80k/output_images/8253.png  \n",
            "  inflating: data_80k/output_images/8254.png  \n",
            "  inflating: data_80k/output_images/8255.png  \n",
            "  inflating: data_80k/output_images/8256.png  \n",
            "  inflating: data_80k/output_images/8257.png  \n",
            "  inflating: data_80k/output_images/8258.png  \n",
            "  inflating: data_80k/output_images/8259.png  \n",
            "  inflating: data_80k/output_images/826.png  \n",
            "  inflating: data_80k/output_images/8260.png  \n",
            "  inflating: data_80k/output_images/8261.png  \n",
            "  inflating: data_80k/output_images/8262.png  \n",
            "  inflating: data_80k/output_images/8263.png  \n",
            "  inflating: data_80k/output_images/8264.png  \n",
            "  inflating: data_80k/output_images/8265.png  \n",
            "  inflating: data_80k/output_images/8266.png  \n",
            "  inflating: data_80k/output_images/8267.png  \n",
            "  inflating: data_80k/output_images/8268.png  \n",
            "  inflating: data_80k/output_images/8269.png  \n",
            "  inflating: data_80k/output_images/827.png  \n",
            "  inflating: data_80k/output_images/8270.png  \n",
            "  inflating: data_80k/output_images/8271.png  \n",
            "  inflating: data_80k/output_images/8272.png  \n",
            "  inflating: data_80k/output_images/8273.png  \n",
            "  inflating: data_80k/output_images/8274.png  \n",
            "  inflating: data_80k/output_images/8275.png  \n",
            "  inflating: data_80k/output_images/8276.png  \n",
            "  inflating: data_80k/output_images/8277.png  \n",
            "  inflating: data_80k/output_images/8278.png  \n",
            "  inflating: data_80k/output_images/8279.png  \n",
            "  inflating: data_80k/output_images/828.png  \n",
            "  inflating: data_80k/output_images/8280.png  \n",
            "  inflating: data_80k/output_images/8281.png  \n",
            "  inflating: data_80k/output_images/8282.png  \n",
            "  inflating: data_80k/output_images/8283.png  \n",
            "  inflating: data_80k/output_images/8284.png  \n",
            "  inflating: data_80k/output_images/8285.png  \n",
            "  inflating: data_80k/output_images/8286.png  \n",
            "  inflating: data_80k/output_images/8287.png  \n",
            "  inflating: data_80k/output_images/8288.png  \n",
            "  inflating: data_80k/output_images/8289.png  \n",
            "  inflating: data_80k/output_images/829.png  \n",
            "  inflating: data_80k/output_images/8290.png  \n",
            "  inflating: data_80k/output_images/8291.png  \n",
            "  inflating: data_80k/output_images/8292.png  \n",
            "  inflating: data_80k/output_images/8293.png  \n",
            "  inflating: data_80k/output_images/8294.png  \n",
            "  inflating: data_80k/output_images/8295.png  \n",
            "  inflating: data_80k/output_images/8296.png  \n",
            "  inflating: data_80k/output_images/8297.png  \n",
            "  inflating: data_80k/output_images/8298.png  \n",
            "  inflating: data_80k/output_images/8299.png  \n",
            "  inflating: data_80k/output_images/83.png  \n",
            "  inflating: data_80k/output_images/830.png  \n",
            "  inflating: data_80k/output_images/8300.png  \n",
            "  inflating: data_80k/output_images/8301.png  \n",
            "  inflating: data_80k/output_images/8302.png  \n",
            "  inflating: data_80k/output_images/8303.png  \n",
            "  inflating: data_80k/output_images/8304.png  \n",
            "  inflating: data_80k/output_images/8305.png  \n",
            "  inflating: data_80k/output_images/8306.png  \n",
            "  inflating: data_80k/output_images/8307.png  \n",
            "  inflating: data_80k/output_images/8308.png  \n",
            "  inflating: data_80k/output_images/8309.png  \n",
            "  inflating: data_80k/output_images/831.png  \n",
            "  inflating: data_80k/output_images/8310.png  \n",
            "  inflating: data_80k/output_images/8311.png  \n",
            "  inflating: data_80k/output_images/8312.png  \n",
            "  inflating: data_80k/output_images/8313.png  \n",
            "  inflating: data_80k/output_images/8314.png  \n",
            "  inflating: data_80k/output_images/8315.png  \n",
            "  inflating: data_80k/output_images/8316.png  \n",
            "  inflating: data_80k/output_images/8317.png  \n",
            "  inflating: data_80k/output_images/8318.png  \n",
            "  inflating: data_80k/output_images/8319.png  \n",
            "  inflating: data_80k/output_images/832.png  \n",
            "  inflating: data_80k/output_images/8320.png  \n",
            "  inflating: data_80k/output_images/8321.png  \n",
            "  inflating: data_80k/output_images/8322.png  \n",
            "  inflating: data_80k/output_images/8323.png  \n",
            "  inflating: data_80k/output_images/8324.png  \n",
            "  inflating: data_80k/output_images/8325.png  \n",
            "  inflating: data_80k/output_images/8326.png  \n",
            "  inflating: data_80k/output_images/8327.png  \n",
            "  inflating: data_80k/output_images/8328.png  \n",
            "  inflating: data_80k/output_images/8329.png  \n",
            "  inflating: data_80k/output_images/833.png  \n",
            "  inflating: data_80k/output_images/8330.png  \n",
            "  inflating: data_80k/output_images/8331.png  \n",
            "  inflating: data_80k/output_images/8332.png  \n",
            "  inflating: data_80k/output_images/8333.png  \n",
            "  inflating: data_80k/output_images/8334.png  \n",
            "  inflating: data_80k/output_images/8335.png  \n",
            "  inflating: data_80k/output_images/8336.png  \n",
            "  inflating: data_80k/output_images/8337.png  \n",
            "  inflating: data_80k/output_images/8338.png  \n",
            "  inflating: data_80k/output_images/8339.png  \n",
            "  inflating: data_80k/output_images/834.png  \n",
            "  inflating: data_80k/output_images/8340.png  \n",
            "  inflating: data_80k/output_images/8341.png  \n",
            "  inflating: data_80k/output_images/8342.png  \n",
            "  inflating: data_80k/output_images/8343.png  \n",
            "  inflating: data_80k/output_images/8344.png  \n",
            "  inflating: data_80k/output_images/8345.png  \n",
            "  inflating: data_80k/output_images/8346.png  \n",
            "  inflating: data_80k/output_images/8347.png  \n",
            "  inflating: data_80k/output_images/8348.png  \n",
            "  inflating: data_80k/output_images/8349.png  \n",
            "  inflating: data_80k/output_images/835.png  \n",
            "  inflating: data_80k/output_images/8350.png  \n",
            "  inflating: data_80k/output_images/8351.png  \n",
            "  inflating: data_80k/output_images/8352.png  \n",
            "  inflating: data_80k/output_images/8353.png  \n",
            "  inflating: data_80k/output_images/8354.png  \n",
            "  inflating: data_80k/output_images/8355.png  \n",
            "  inflating: data_80k/output_images/8356.png  \n",
            "  inflating: data_80k/output_images/8357.png  \n",
            "  inflating: data_80k/output_images/8358.png  \n",
            "  inflating: data_80k/output_images/8359.png  \n",
            "  inflating: data_80k/output_images/836.png  \n",
            "  inflating: data_80k/output_images/8360.png  \n",
            "  inflating: data_80k/output_images/8361.png  \n",
            "  inflating: data_80k/output_images/8362.png  \n",
            "  inflating: data_80k/output_images/8363.png  \n",
            "  inflating: data_80k/output_images/8364.png  \n",
            "  inflating: data_80k/output_images/8365.png  \n",
            "  inflating: data_80k/output_images/8366.png  \n",
            "  inflating: data_80k/output_images/8367.png  \n",
            "  inflating: data_80k/output_images/8368.png  \n",
            "  inflating: data_80k/output_images/8369.png  \n",
            "  inflating: data_80k/output_images/837.png  \n",
            "  inflating: data_80k/output_images/8370.png  \n",
            "  inflating: data_80k/output_images/8371.png  \n",
            "  inflating: data_80k/output_images/8372.png  \n",
            "  inflating: data_80k/output_images/8373.png  \n",
            "  inflating: data_80k/output_images/8374.png  \n",
            "  inflating: data_80k/output_images/8375.png  \n",
            "  inflating: data_80k/output_images/8376.png  \n",
            "  inflating: data_80k/output_images/8377.png  \n",
            "  inflating: data_80k/output_images/8378.png  \n",
            "  inflating: data_80k/output_images/8379.png  \n",
            "  inflating: data_80k/output_images/838.png  \n",
            "  inflating: data_80k/output_images/8380.png  \n",
            "  inflating: data_80k/output_images/8381.png  \n",
            "  inflating: data_80k/output_images/8382.png  \n",
            "  inflating: data_80k/output_images/8383.png  \n",
            "  inflating: data_80k/output_images/8384.png  \n",
            "  inflating: data_80k/output_images/8385.png  \n",
            "  inflating: data_80k/output_images/8386.png  \n",
            "  inflating: data_80k/output_images/8387.png  \n",
            "  inflating: data_80k/output_images/8388.png  \n",
            "  inflating: data_80k/output_images/8389.png  \n",
            "  inflating: data_80k/output_images/839.png  \n",
            "  inflating: data_80k/output_images/8390.png  \n",
            "  inflating: data_80k/output_images/8391.png  \n",
            "  inflating: data_80k/output_images/8392.png  \n",
            "  inflating: data_80k/output_images/8393.png  \n",
            "  inflating: data_80k/output_images/8394.png  \n",
            "  inflating: data_80k/output_images/8395.png  \n",
            "  inflating: data_80k/output_images/8396.png  \n",
            "  inflating: data_80k/output_images/8397.png  \n",
            "  inflating: data_80k/output_images/8398.png  \n",
            "  inflating: data_80k/output_images/8399.png  \n",
            "  inflating: data_80k/output_images/84.png  \n",
            "  inflating: data_80k/output_images/840.png  \n",
            "  inflating: data_80k/output_images/8400.png  \n",
            "  inflating: data_80k/output_images/8401.png  \n",
            "  inflating: data_80k/output_images/8402.png  \n",
            "  inflating: data_80k/output_images/8403.png  \n",
            "  inflating: data_80k/output_images/8404.png  \n",
            "  inflating: data_80k/output_images/8405.png  \n",
            "  inflating: data_80k/output_images/8406.png  \n",
            "  inflating: data_80k/output_images/8407.png  \n",
            "  inflating: data_80k/output_images/8408.png  \n",
            "  inflating: data_80k/output_images/8409.png  \n",
            "  inflating: data_80k/output_images/841.png  \n",
            "  inflating: data_80k/output_images/8410.png  \n",
            "  inflating: data_80k/output_images/8411.png  \n",
            "  inflating: data_80k/output_images/8412.png  \n",
            "  inflating: data_80k/output_images/8413.png  \n",
            "  inflating: data_80k/output_images/8414.png  \n",
            "  inflating: data_80k/output_images/8415.png  \n",
            "  inflating: data_80k/output_images/8416.png  \n",
            "  inflating: data_80k/output_images/8417.png  \n",
            "  inflating: data_80k/output_images/8418.png  \n",
            "  inflating: data_80k/output_images/8419.png  \n",
            "  inflating: data_80k/output_images/842.png  \n",
            "  inflating: data_80k/output_images/8420.png  \n",
            "  inflating: data_80k/output_images/8421.png  \n",
            "  inflating: data_80k/output_images/8422.png  \n",
            "  inflating: data_80k/output_images/8423.png  \n",
            "  inflating: data_80k/output_images/8424.png  \n",
            "  inflating: data_80k/output_images/8425.png  \n",
            "  inflating: data_80k/output_images/8426.png  \n",
            "  inflating: data_80k/output_images/8427.png  \n",
            "  inflating: data_80k/output_images/8428.png  \n",
            "  inflating: data_80k/output_images/8429.png  \n",
            "  inflating: data_80k/output_images/843.png  \n",
            "  inflating: data_80k/output_images/8430.png  \n",
            "  inflating: data_80k/output_images/8431.png  \n",
            "  inflating: data_80k/output_images/8432.png  \n",
            "  inflating: data_80k/output_images/8433.png  \n",
            "  inflating: data_80k/output_images/8434.png  \n",
            "  inflating: data_80k/output_images/8435.png  \n",
            "  inflating: data_80k/output_images/8436.png  \n",
            "  inflating: data_80k/output_images/8437.png  \n",
            "  inflating: data_80k/output_images/8438.png  \n",
            "  inflating: data_80k/output_images/8439.png  \n",
            "  inflating: data_80k/output_images/844.png  \n",
            "  inflating: data_80k/output_images/8440.png  \n",
            "  inflating: data_80k/output_images/8441.png  \n",
            "  inflating: data_80k/output_images/8442.png  \n",
            "  inflating: data_80k/output_images/8443.png  \n",
            "  inflating: data_80k/output_images/8444.png  \n",
            "  inflating: data_80k/output_images/8445.png  \n",
            "  inflating: data_80k/output_images/8446.png  \n",
            "  inflating: data_80k/output_images/8447.png  \n",
            "  inflating: data_80k/output_images/8448.png  \n",
            "  inflating: data_80k/output_images/8449.png  \n",
            "  inflating: data_80k/output_images/845.png  \n",
            "  inflating: data_80k/output_images/8450.png  \n",
            "  inflating: data_80k/output_images/8451.png  \n",
            "  inflating: data_80k/output_images/8452.png  \n",
            "  inflating: data_80k/output_images/8453.png  \n",
            "  inflating: data_80k/output_images/8454.png  \n",
            "  inflating: data_80k/output_images/8455.png  \n",
            "  inflating: data_80k/output_images/8456.png  \n",
            "  inflating: data_80k/output_images/8457.png  \n",
            "  inflating: data_80k/output_images/8458.png  \n",
            "  inflating: data_80k/output_images/8459.png  \n",
            "  inflating: data_80k/output_images/846.png  \n",
            "  inflating: data_80k/output_images/8460.png  \n",
            "  inflating: data_80k/output_images/8461.png  \n",
            "  inflating: data_80k/output_images/8462.png  \n",
            "  inflating: data_80k/output_images/8463.png  \n",
            "  inflating: data_80k/output_images/8464.png  \n",
            "  inflating: data_80k/output_images/8465.png  \n",
            "  inflating: data_80k/output_images/8466.png  \n",
            "  inflating: data_80k/output_images/8467.png  \n",
            "  inflating: data_80k/output_images/8468.png  \n",
            "  inflating: data_80k/output_images/8469.png  \n",
            "  inflating: data_80k/output_images/847.png  \n",
            "  inflating: data_80k/output_images/8470.png  \n",
            "  inflating: data_80k/output_images/8471.png  \n",
            "  inflating: data_80k/output_images/8472.png  \n",
            "  inflating: data_80k/output_images/8473.png  \n",
            "  inflating: data_80k/output_images/8474.png  \n",
            "  inflating: data_80k/output_images/8475.png  \n",
            "  inflating: data_80k/output_images/8476.png  \n",
            "  inflating: data_80k/output_images/8477.png  \n",
            "  inflating: data_80k/output_images/8478.png  \n",
            "  inflating: data_80k/output_images/8479.png  \n",
            "  inflating: data_80k/output_images/848.png  \n",
            "  inflating: data_80k/output_images/8480.png  \n",
            "  inflating: data_80k/output_images/8481.png  \n",
            "  inflating: data_80k/output_images/8482.png  \n",
            "  inflating: data_80k/output_images/8483.png  \n",
            "  inflating: data_80k/output_images/8484.png  \n",
            "  inflating: data_80k/output_images/8485.png  \n",
            "  inflating: data_80k/output_images/8486.png  \n",
            "  inflating: data_80k/output_images/8487.png  \n",
            "  inflating: data_80k/output_images/8488.png  \n",
            "  inflating: data_80k/output_images/8489.png  \n",
            "  inflating: data_80k/output_images/849.png  \n",
            "  inflating: data_80k/output_images/8490.png  \n",
            "  inflating: data_80k/output_images/8491.png  \n",
            "  inflating: data_80k/output_images/8492.png  \n",
            "  inflating: data_80k/output_images/8493.png  \n",
            "  inflating: data_80k/output_images/8494.png  \n",
            "  inflating: data_80k/output_images/8495.png  \n",
            "  inflating: data_80k/output_images/8496.png  \n",
            "  inflating: data_80k/output_images/8497.png  \n",
            "  inflating: data_80k/output_images/8498.png  \n",
            "  inflating: data_80k/output_images/8499.png  \n",
            "  inflating: data_80k/output_images/85.png  \n",
            "  inflating: data_80k/output_images/850.png  \n",
            "  inflating: data_80k/output_images/8500.png  \n",
            "  inflating: data_80k/output_images/8501.png  \n",
            "  inflating: data_80k/output_images/8502.png  \n",
            "  inflating: data_80k/output_images/8503.png  \n",
            "  inflating: data_80k/output_images/8504.png  \n",
            "  inflating: data_80k/output_images/8505.png  \n",
            "  inflating: data_80k/output_images/8506.png  \n",
            "  inflating: data_80k/output_images/8507.png  \n",
            "  inflating: data_80k/output_images/8508.png  \n",
            "  inflating: data_80k/output_images/8509.png  \n",
            "  inflating: data_80k/output_images/851.png  \n",
            "  inflating: data_80k/output_images/8510.png  \n",
            "  inflating: data_80k/output_images/8511.png  \n",
            "  inflating: data_80k/output_images/8512.png  \n",
            "  inflating: data_80k/output_images/8513.png  \n",
            "  inflating: data_80k/output_images/8514.png  \n",
            "  inflating: data_80k/output_images/8515.png  \n",
            "  inflating: data_80k/output_images/8516.png  \n",
            "  inflating: data_80k/output_images/8517.png  \n",
            "  inflating: data_80k/output_images/8518.png  \n",
            "  inflating: data_80k/output_images/8519.png  \n",
            "  inflating: data_80k/output_images/852.png  \n",
            "  inflating: data_80k/output_images/8520.png  \n",
            "  inflating: data_80k/output_images/8521.png  \n",
            "  inflating: data_80k/output_images/8522.png  \n",
            "  inflating: data_80k/output_images/8523.png  \n",
            "  inflating: data_80k/output_images/8524.png  \n",
            "  inflating: data_80k/output_images/8525.png  \n",
            "  inflating: data_80k/output_images/8526.png  \n",
            "  inflating: data_80k/output_images/8527.png  \n",
            "  inflating: data_80k/output_images/8528.png  \n",
            "  inflating: data_80k/output_images/8529.png  \n",
            "  inflating: data_80k/output_images/853.png  \n",
            "  inflating: data_80k/output_images/8530.png  \n",
            "  inflating: data_80k/output_images/8531.png  \n",
            "  inflating: data_80k/output_images/8532.png  \n",
            "  inflating: data_80k/output_images/8533.png  \n",
            "  inflating: data_80k/output_images/8534.png  \n",
            "  inflating: data_80k/output_images/8535.png  \n",
            "  inflating: data_80k/output_images/8536.png  \n",
            "  inflating: data_80k/output_images/8537.png  \n",
            "  inflating: data_80k/output_images/8538.png  \n",
            "  inflating: data_80k/output_images/8539.png  \n",
            "  inflating: data_80k/output_images/854.png  \n",
            "  inflating: data_80k/output_images/8540.png  \n",
            "  inflating: data_80k/output_images/8541.png  \n",
            "  inflating: data_80k/output_images/8542.png  \n",
            "  inflating: data_80k/output_images/8543.png  \n",
            "  inflating: data_80k/output_images/8544.png  \n",
            "  inflating: data_80k/output_images/8545.png  \n",
            "  inflating: data_80k/output_images/8546.png  \n",
            "  inflating: data_80k/output_images/8547.png  \n",
            "  inflating: data_80k/output_images/8548.png  \n",
            "  inflating: data_80k/output_images/8549.png  \n",
            "  inflating: data_80k/output_images/855.png  \n",
            "  inflating: data_80k/output_images/8550.png  \n",
            "  inflating: data_80k/output_images/8551.png  \n",
            "  inflating: data_80k/output_images/8552.png  \n",
            "  inflating: data_80k/output_images/8553.png  \n",
            "  inflating: data_80k/output_images/8554.png  \n",
            "  inflating: data_80k/output_images/8555.png  \n",
            "  inflating: data_80k/output_images/8556.png  \n",
            "  inflating: data_80k/output_images/8557.png  \n",
            "  inflating: data_80k/output_images/8558.png  \n",
            "  inflating: data_80k/output_images/8559.png  \n",
            "  inflating: data_80k/output_images/856.png  \n",
            "  inflating: data_80k/output_images/8560.png  \n",
            "  inflating: data_80k/output_images/8561.png  \n",
            "  inflating: data_80k/output_images/8562.png  \n",
            "  inflating: data_80k/output_images/8563.png  \n",
            "  inflating: data_80k/output_images/8564.png  \n",
            "  inflating: data_80k/output_images/8565.png  \n",
            "  inflating: data_80k/output_images/8566.png  \n",
            "  inflating: data_80k/output_images/8567.png  \n",
            "  inflating: data_80k/output_images/8568.png  \n",
            "  inflating: data_80k/output_images/8569.png  \n",
            "  inflating: data_80k/output_images/857.png  \n",
            "  inflating: data_80k/output_images/8570.png  \n",
            "  inflating: data_80k/output_images/8571.png  \n",
            "  inflating: data_80k/output_images/8572.png  \n",
            "  inflating: data_80k/output_images/8573.png  \n",
            "  inflating: data_80k/output_images/8574.png  \n",
            "  inflating: data_80k/output_images/8575.png  \n",
            "  inflating: data_80k/output_images/8576.png  \n",
            "  inflating: data_80k/output_images/8577.png  \n",
            "  inflating: data_80k/output_images/8578.png  \n",
            "  inflating: data_80k/output_images/8579.png  \n",
            "  inflating: data_80k/output_images/858.png  \n",
            "  inflating: data_80k/output_images/8580.png  \n",
            "  inflating: data_80k/output_images/8581.png  \n",
            "  inflating: data_80k/output_images/8582.png  \n",
            "  inflating: data_80k/output_images/8583.png  \n",
            "  inflating: data_80k/output_images/8584.png  \n",
            "  inflating: data_80k/output_images/8585.png  \n",
            "  inflating: data_80k/output_images/8586.png  \n",
            "  inflating: data_80k/output_images/8587.png  \n",
            "  inflating: data_80k/output_images/8588.png  \n",
            "  inflating: data_80k/output_images/8589.png  \n",
            "  inflating: data_80k/output_images/859.png  \n",
            "  inflating: data_80k/output_images/8590.png  \n",
            "  inflating: data_80k/output_images/8591.png  \n",
            "  inflating: data_80k/output_images/8592.png  \n",
            "  inflating: data_80k/output_images/8593.png  \n",
            "  inflating: data_80k/output_images/8594.png  \n",
            "  inflating: data_80k/output_images/8595.png  \n",
            "  inflating: data_80k/output_images/8596.png  \n",
            "  inflating: data_80k/output_images/8597.png  \n",
            "  inflating: data_80k/output_images/8598.png  \n",
            "  inflating: data_80k/output_images/8599.png  \n",
            "  inflating: data_80k/output_images/86.png  \n",
            "  inflating: data_80k/output_images/860.png  \n",
            "  inflating: data_80k/output_images/8600.png  \n",
            "  inflating: data_80k/output_images/8601.png  \n",
            "  inflating: data_80k/output_images/8602.png  \n",
            "  inflating: data_80k/output_images/8603.png  \n",
            "  inflating: data_80k/output_images/8604.png  \n",
            "  inflating: data_80k/output_images/8605.png  \n",
            "  inflating: data_80k/output_images/8606.png  \n",
            "  inflating: data_80k/output_images/8607.png  \n",
            "  inflating: data_80k/output_images/8608.png  \n",
            "  inflating: data_80k/output_images/8609.png  \n",
            "  inflating: data_80k/output_images/861.png  \n",
            "  inflating: data_80k/output_images/8610.png  \n",
            "  inflating: data_80k/output_images/8611.png  \n",
            "  inflating: data_80k/output_images/8612.png  \n",
            "  inflating: data_80k/output_images/8613.png  \n",
            "  inflating: data_80k/output_images/8614.png  \n",
            "  inflating: data_80k/output_images/8615.png  \n",
            "  inflating: data_80k/output_images/8616.png  \n",
            "  inflating: data_80k/output_images/8617.png  \n",
            "  inflating: data_80k/output_images/8618.png  \n",
            "  inflating: data_80k/output_images/8619.png  \n",
            "  inflating: data_80k/output_images/862.png  \n",
            "  inflating: data_80k/output_images/8620.png  \n",
            "  inflating: data_80k/output_images/8621.png  \n",
            "  inflating: data_80k/output_images/8622.png  \n",
            "  inflating: data_80k/output_images/8623.png  \n",
            "  inflating: data_80k/output_images/8624.png  \n",
            "  inflating: data_80k/output_images/8625.png  \n",
            "  inflating: data_80k/output_images/8626.png  \n",
            "  inflating: data_80k/output_images/8627.png  \n",
            "  inflating: data_80k/output_images/8628.png  \n",
            "  inflating: data_80k/output_images/8629.png  \n",
            "  inflating: data_80k/output_images/863.png  \n",
            "  inflating: data_80k/output_images/8630.png  \n",
            "  inflating: data_80k/output_images/8631.png  \n",
            "  inflating: data_80k/output_images/8632.png  \n",
            "  inflating: data_80k/output_images/8633.png  \n",
            "  inflating: data_80k/output_images/8634.png  \n",
            "  inflating: data_80k/output_images/8635.png  \n",
            "  inflating: data_80k/output_images/8636.png  \n",
            "  inflating: data_80k/output_images/8637.png  \n",
            "  inflating: data_80k/output_images/8638.png  \n",
            "  inflating: data_80k/output_images/8639.png  \n",
            "  inflating: data_80k/output_images/864.png  \n",
            "  inflating: data_80k/output_images/8640.png  \n",
            "  inflating: data_80k/output_images/8641.png  \n",
            "  inflating: data_80k/output_images/8642.png  \n",
            "  inflating: data_80k/output_images/8643.png  \n",
            "  inflating: data_80k/output_images/8644.png  \n",
            "  inflating: data_80k/output_images/8645.png  \n",
            "  inflating: data_80k/output_images/8646.png  \n",
            "  inflating: data_80k/output_images/8647.png  \n",
            "  inflating: data_80k/output_images/8648.png  \n",
            "  inflating: data_80k/output_images/8649.png  \n",
            "  inflating: data_80k/output_images/865.png  \n",
            "  inflating: data_80k/output_images/8650.png  \n",
            "  inflating: data_80k/output_images/8651.png  \n",
            "  inflating: data_80k/output_images/8652.png  \n",
            "  inflating: data_80k/output_images/8653.png  \n",
            "  inflating: data_80k/output_images/8654.png  \n",
            "  inflating: data_80k/output_images/8655.png  \n",
            "  inflating: data_80k/output_images/8656.png  \n",
            "  inflating: data_80k/output_images/8657.png  \n",
            "  inflating: data_80k/output_images/8658.png  \n",
            "  inflating: data_80k/output_images/8659.png  \n",
            "  inflating: data_80k/output_images/866.png  \n",
            "  inflating: data_80k/output_images/8660.png  \n",
            "  inflating: data_80k/output_images/8661.png  \n",
            "  inflating: data_80k/output_images/8662.png  \n",
            "  inflating: data_80k/output_images/8663.png  \n",
            "  inflating: data_80k/output_images/8664.png  \n",
            "  inflating: data_80k/output_images/8665.png  \n",
            "  inflating: data_80k/output_images/8666.png  \n",
            "  inflating: data_80k/output_images/8667.png  \n",
            "  inflating: data_80k/output_images/8668.png  \n",
            "  inflating: data_80k/output_images/8669.png  \n",
            "  inflating: data_80k/output_images/867.png  \n",
            "  inflating: data_80k/output_images/8670.png  \n",
            "  inflating: data_80k/output_images/8671.png  \n",
            "  inflating: data_80k/output_images/8672.png  \n",
            "  inflating: data_80k/output_images/8673.png  \n",
            "  inflating: data_80k/output_images/8674.png  \n",
            "  inflating: data_80k/output_images/8675.png  \n",
            "  inflating: data_80k/output_images/8676.png  \n",
            "  inflating: data_80k/output_images/8677.png  \n",
            "  inflating: data_80k/output_images/8678.png  \n",
            "  inflating: data_80k/output_images/8679.png  \n",
            "  inflating: data_80k/output_images/868.png  \n",
            "  inflating: data_80k/output_images/8680.png  \n",
            "  inflating: data_80k/output_images/8681.png  \n",
            "  inflating: data_80k/output_images/8682.png  \n",
            "  inflating: data_80k/output_images/8683.png  \n",
            "  inflating: data_80k/output_images/8684.png  \n",
            "  inflating: data_80k/output_images/8685.png  \n",
            "  inflating: data_80k/output_images/8686.png  \n",
            "  inflating: data_80k/output_images/8687.png  \n",
            "  inflating: data_80k/output_images/8688.png  \n",
            "  inflating: data_80k/output_images/8689.png  \n",
            "  inflating: data_80k/output_images/869.png  \n",
            "  inflating: data_80k/output_images/8690.png  \n",
            "  inflating: data_80k/output_images/8691.png  \n",
            "  inflating: data_80k/output_images/8692.png  \n",
            "  inflating: data_80k/output_images/8693.png  \n",
            "  inflating: data_80k/output_images/8694.png  \n",
            "  inflating: data_80k/output_images/8695.png  \n",
            "  inflating: data_80k/output_images/8696.png  \n",
            "  inflating: data_80k/output_images/8697.png  \n",
            "  inflating: data_80k/output_images/8698.png  \n",
            "  inflating: data_80k/output_images/8699.png  \n",
            "  inflating: data_80k/output_images/87.png  \n",
            "  inflating: data_80k/output_images/870.png  \n",
            "  inflating: data_80k/output_images/8700.png  \n",
            "  inflating: data_80k/output_images/8701.png  \n",
            "  inflating: data_80k/output_images/8702.png  \n",
            "  inflating: data_80k/output_images/8703.png  \n",
            "  inflating: data_80k/output_images/8704.png  \n",
            "  inflating: data_80k/output_images/8705.png  \n",
            "  inflating: data_80k/output_images/8706.png  \n",
            "  inflating: data_80k/output_images/8707.png  \n",
            "  inflating: data_80k/output_images/8708.png  \n",
            "  inflating: data_80k/output_images/8709.png  \n",
            "  inflating: data_80k/output_images/871.png  \n",
            "  inflating: data_80k/output_images/8710.png  \n",
            "  inflating: data_80k/output_images/8711.png  \n",
            "  inflating: data_80k/output_images/8712.png  \n",
            "  inflating: data_80k/output_images/8713.png  \n",
            "  inflating: data_80k/output_images/8714.png  \n",
            "  inflating: data_80k/output_images/8715.png  \n",
            "  inflating: data_80k/output_images/8716.png  \n",
            "  inflating: data_80k/output_images/8717.png  \n",
            "  inflating: data_80k/output_images/8718.png  \n",
            "  inflating: data_80k/output_images/8719.png  \n",
            "  inflating: data_80k/output_images/872.png  \n",
            "  inflating: data_80k/output_images/8720.png  \n",
            "  inflating: data_80k/output_images/8721.png  \n",
            "  inflating: data_80k/output_images/8722.png  \n",
            "  inflating: data_80k/output_images/8723.png  \n",
            "  inflating: data_80k/output_images/8724.png  \n",
            "  inflating: data_80k/output_images/8725.png  \n",
            "  inflating: data_80k/output_images/8726.png  \n",
            "  inflating: data_80k/output_images/8727.png  \n",
            "  inflating: data_80k/output_images/8728.png  \n",
            "  inflating: data_80k/output_images/8729.png  \n",
            "  inflating: data_80k/output_images/873.png  \n",
            "  inflating: data_80k/output_images/8730.png  \n",
            "  inflating: data_80k/output_images/8731.png  \n",
            "  inflating: data_80k/output_images/8732.png  \n",
            "  inflating: data_80k/output_images/8733.png  \n",
            "  inflating: data_80k/output_images/8734.png  \n",
            "  inflating: data_80k/output_images/8735.png  \n",
            "  inflating: data_80k/output_images/8736.png  \n",
            "  inflating: data_80k/output_images/8737.png  \n",
            "  inflating: data_80k/output_images/8738.png  \n",
            "  inflating: data_80k/output_images/8739.png  \n",
            "  inflating: data_80k/output_images/874.png  \n",
            "  inflating: data_80k/output_images/8740.png  \n",
            "  inflating: data_80k/output_images/8741.png  \n",
            "  inflating: data_80k/output_images/8742.png  \n",
            "  inflating: data_80k/output_images/8743.png  \n",
            "  inflating: data_80k/output_images/8744.png  \n",
            "  inflating: data_80k/output_images/8745.png  \n",
            "  inflating: data_80k/output_images/8746.png  \n",
            "  inflating: data_80k/output_images/8747.png  \n",
            "  inflating: data_80k/output_images/8748.png  \n",
            "  inflating: data_80k/output_images/8749.png  \n",
            "  inflating: data_80k/output_images/875.png  \n",
            "  inflating: data_80k/output_images/8750.png  \n",
            "  inflating: data_80k/output_images/8751.png  \n",
            "  inflating: data_80k/output_images/8752.png  \n",
            "  inflating: data_80k/output_images/8753.png  \n",
            "  inflating: data_80k/output_images/8754.png  \n",
            "  inflating: data_80k/output_images/8755.png  \n",
            "  inflating: data_80k/output_images/8756.png  \n",
            "  inflating: data_80k/output_images/8757.png  \n",
            "  inflating: data_80k/output_images/8758.png  \n",
            "  inflating: data_80k/output_images/8759.png  \n",
            "  inflating: data_80k/output_images/876.png  \n",
            "  inflating: data_80k/output_images/8760.png  \n",
            "  inflating: data_80k/output_images/8761.png  \n",
            "  inflating: data_80k/output_images/8762.png  \n",
            "  inflating: data_80k/output_images/8763.png  \n",
            "  inflating: data_80k/output_images/8764.png  \n",
            "  inflating: data_80k/output_images/8765.png  \n",
            "  inflating: data_80k/output_images/8766.png  \n",
            "  inflating: data_80k/output_images/8767.png  \n",
            "  inflating: data_80k/output_images/8768.png  \n",
            "  inflating: data_80k/output_images/8769.png  \n",
            "  inflating: data_80k/output_images/877.png  \n",
            "  inflating: data_80k/output_images/8770.png  \n",
            "  inflating: data_80k/output_images/8771.png  \n",
            "  inflating: data_80k/output_images/8772.png  \n",
            "  inflating: data_80k/output_images/8773.png  \n",
            "  inflating: data_80k/output_images/8774.png  \n",
            "  inflating: data_80k/output_images/8775.png  \n",
            "  inflating: data_80k/output_images/8776.png  \n",
            "  inflating: data_80k/output_images/8777.png  \n",
            "  inflating: data_80k/output_images/8778.png  \n",
            "  inflating: data_80k/output_images/8779.png  \n",
            "  inflating: data_80k/output_images/878.png  \n",
            "  inflating: data_80k/output_images/8780.png  \n",
            "  inflating: data_80k/output_images/8781.png  \n",
            "  inflating: data_80k/output_images/8782.png  \n",
            "  inflating: data_80k/output_images/8783.png  \n",
            "  inflating: data_80k/output_images/8784.png  \n",
            "  inflating: data_80k/output_images/8785.png  \n",
            "  inflating: data_80k/output_images/8786.png  \n",
            "  inflating: data_80k/output_images/8787.png  \n",
            "  inflating: data_80k/output_images/8788.png  \n",
            "  inflating: data_80k/output_images/8789.png  \n",
            "  inflating: data_80k/output_images/879.png  \n",
            "  inflating: data_80k/output_images/8790.png  \n",
            "  inflating: data_80k/output_images/8791.png  \n",
            "  inflating: data_80k/output_images/8792.png  \n",
            "  inflating: data_80k/output_images/8793.png  \n",
            "  inflating: data_80k/output_images/8794.png  \n",
            "  inflating: data_80k/output_images/8795.png  \n",
            "  inflating: data_80k/output_images/8796.png  \n",
            "  inflating: data_80k/output_images/8797.png  \n",
            "  inflating: data_80k/output_images/8798.png  \n",
            "  inflating: data_80k/output_images/8799.png  \n",
            "  inflating: data_80k/output_images/88.png  \n",
            "  inflating: data_80k/output_images/880.png  \n",
            "  inflating: data_80k/output_images/8800.png  \n",
            "  inflating: data_80k/output_images/8801.png  \n",
            "  inflating: data_80k/output_images/8802.png  \n",
            "  inflating: data_80k/output_images/8803.png  \n",
            "  inflating: data_80k/output_images/8804.png  \n",
            "  inflating: data_80k/output_images/8805.png  \n",
            "  inflating: data_80k/output_images/8806.png  \n",
            "  inflating: data_80k/output_images/8807.png  \n",
            "  inflating: data_80k/output_images/8808.png  \n",
            "  inflating: data_80k/output_images/8809.png  \n",
            "  inflating: data_80k/output_images/881.png  \n",
            "  inflating: data_80k/output_images/8810.png  \n",
            "  inflating: data_80k/output_images/8811.png  \n",
            "  inflating: data_80k/output_images/8812.png  \n",
            "  inflating: data_80k/output_images/8813.png  \n",
            "  inflating: data_80k/output_images/8814.png  \n",
            "  inflating: data_80k/output_images/8815.png  \n",
            "  inflating: data_80k/output_images/8816.png  \n",
            "  inflating: data_80k/output_images/8817.png  \n",
            "  inflating: data_80k/output_images/8818.png  \n",
            "  inflating: data_80k/output_images/8819.png  \n",
            "  inflating: data_80k/output_images/882.png  \n",
            "  inflating: data_80k/output_images/8820.png  \n",
            "  inflating: data_80k/output_images/8821.png  \n",
            "  inflating: data_80k/output_images/8822.png  \n",
            "  inflating: data_80k/output_images/8823.png  \n",
            "  inflating: data_80k/output_images/8824.png  \n",
            "  inflating: data_80k/output_images/8825.png  \n",
            "  inflating: data_80k/output_images/8826.png  \n",
            "  inflating: data_80k/output_images/8827.png  \n",
            "  inflating: data_80k/output_images/8828.png  \n",
            "  inflating: data_80k/output_images/8829.png  \n",
            "  inflating: data_80k/output_images/883.png  \n",
            "  inflating: data_80k/output_images/8830.png  \n",
            "  inflating: data_80k/output_images/8831.png  \n",
            "  inflating: data_80k/output_images/8832.png  \n",
            "  inflating: data_80k/output_images/8833.png  \n",
            "  inflating: data_80k/output_images/8834.png  \n",
            "  inflating: data_80k/output_images/8835.png  \n",
            "  inflating: data_80k/output_images/8836.png  \n",
            "  inflating: data_80k/output_images/8837.png  \n",
            "  inflating: data_80k/output_images/8838.png  \n",
            "  inflating: data_80k/output_images/8839.png  \n",
            "  inflating: data_80k/output_images/884.png  \n",
            "  inflating: data_80k/output_images/8840.png  \n",
            "  inflating: data_80k/output_images/8841.png  \n",
            "  inflating: data_80k/output_images/8842.png  \n",
            "  inflating: data_80k/output_images/8843.png  \n",
            "  inflating: data_80k/output_images/8844.png  \n",
            "  inflating: data_80k/output_images/8845.png  \n",
            "  inflating: data_80k/output_images/8846.png  \n",
            "  inflating: data_80k/output_images/8847.png  \n",
            "  inflating: data_80k/output_images/8848.png  \n",
            "  inflating: data_80k/output_images/8849.png  \n",
            "  inflating: data_80k/output_images/885.png  \n",
            "  inflating: data_80k/output_images/8850.png  \n",
            "  inflating: data_80k/output_images/8851.png  \n",
            "  inflating: data_80k/output_images/8852.png  \n",
            "  inflating: data_80k/output_images/8853.png  \n",
            "  inflating: data_80k/output_images/8854.png  \n",
            "  inflating: data_80k/output_images/8855.png  \n",
            "  inflating: data_80k/output_images/8856.png  \n",
            "  inflating: data_80k/output_images/8857.png  \n",
            "  inflating: data_80k/output_images/8858.png  \n",
            "  inflating: data_80k/output_images/8859.png  \n",
            "  inflating: data_80k/output_images/886.png  \n",
            "  inflating: data_80k/output_images/8860.png  \n",
            "  inflating: data_80k/output_images/8861.png  \n",
            "  inflating: data_80k/output_images/8862.png  \n",
            "  inflating: data_80k/output_images/8863.png  \n",
            "  inflating: data_80k/output_images/8864.png  \n",
            "  inflating: data_80k/output_images/8865.png  \n",
            "  inflating: data_80k/output_images/8866.png  \n",
            "  inflating: data_80k/output_images/8867.png  \n",
            "  inflating: data_80k/output_images/8868.png  \n",
            "  inflating: data_80k/output_images/8869.png  \n",
            "  inflating: data_80k/output_images/887.png  \n",
            "  inflating: data_80k/output_images/8870.png  \n",
            "  inflating: data_80k/output_images/8871.png  \n",
            "  inflating: data_80k/output_images/8872.png  \n",
            "  inflating: data_80k/output_images/8873.png  \n",
            "  inflating: data_80k/output_images/8874.png  \n",
            "  inflating: data_80k/output_images/8875.png  \n",
            "  inflating: data_80k/output_images/8876.png  \n",
            "  inflating: data_80k/output_images/8877.png  \n",
            "  inflating: data_80k/output_images/8878.png  \n",
            "  inflating: data_80k/output_images/8879.png  \n",
            "  inflating: data_80k/output_images/888.png  \n",
            "  inflating: data_80k/output_images/8880.png  \n",
            "  inflating: data_80k/output_images/8881.png  \n",
            "  inflating: data_80k/output_images/8882.png  \n",
            "  inflating: data_80k/output_images/8883.png  \n",
            "  inflating: data_80k/output_images/8884.png  \n",
            "  inflating: data_80k/output_images/8885.png  \n",
            "  inflating: data_80k/output_images/8886.png  \n",
            "  inflating: data_80k/output_images/8887.png  \n",
            "  inflating: data_80k/output_images/8888.png  \n",
            "  inflating: data_80k/output_images/8889.png  \n",
            "  inflating: data_80k/output_images/889.png  \n",
            "  inflating: data_80k/output_images/8890.png  \n",
            "  inflating: data_80k/output_images/8891.png  \n",
            "  inflating: data_80k/output_images/8892.png  \n",
            "  inflating: data_80k/output_images/8893.png  \n",
            "  inflating: data_80k/output_images/8894.png  \n",
            "  inflating: data_80k/output_images/8895.png  \n",
            "  inflating: data_80k/output_images/8896.png  \n",
            "  inflating: data_80k/output_images/8897.png  \n",
            "  inflating: data_80k/output_images/8898.png  \n",
            "  inflating: data_80k/output_images/8899.png  \n",
            "  inflating: data_80k/output_images/89.png  \n",
            "  inflating: data_80k/output_images/890.png  \n",
            "  inflating: data_80k/output_images/8900.png  \n",
            "  inflating: data_80k/output_images/8901.png  \n",
            "  inflating: data_80k/output_images/8902.png  \n",
            "  inflating: data_80k/output_images/8903.png  \n",
            "  inflating: data_80k/output_images/8904.png  \n",
            "  inflating: data_80k/output_images/8905.png  \n",
            "  inflating: data_80k/output_images/8906.png  \n",
            "  inflating: data_80k/output_images/8907.png  \n",
            "  inflating: data_80k/output_images/8908.png  \n",
            "  inflating: data_80k/output_images/8909.png  \n",
            "  inflating: data_80k/output_images/891.png  \n",
            "  inflating: data_80k/output_images/8910.png  \n",
            "  inflating: data_80k/output_images/8911.png  \n",
            "  inflating: data_80k/output_images/8912.png  \n",
            "  inflating: data_80k/output_images/8913.png  \n",
            "  inflating: data_80k/output_images/8914.png  \n",
            "  inflating: data_80k/output_images/8915.png  \n",
            "  inflating: data_80k/output_images/8916.png  \n",
            "  inflating: data_80k/output_images/8917.png  \n",
            "  inflating: data_80k/output_images/8918.png  \n",
            "  inflating: data_80k/output_images/8919.png  \n",
            "  inflating: data_80k/output_images/892.png  \n",
            "  inflating: data_80k/output_images/8920.png  \n",
            "  inflating: data_80k/output_images/8921.png  \n",
            "  inflating: data_80k/output_images/8922.png  \n",
            "  inflating: data_80k/output_images/8923.png  \n",
            "  inflating: data_80k/output_images/8924.png  \n",
            "  inflating: data_80k/output_images/8925.png  \n",
            "  inflating: data_80k/output_images/8926.png  \n",
            "  inflating: data_80k/output_images/8927.png  \n",
            "  inflating: data_80k/output_images/8928.png  \n",
            "  inflating: data_80k/output_images/8929.png  \n",
            "  inflating: data_80k/output_images/893.png  \n",
            "  inflating: data_80k/output_images/8930.png  \n",
            "  inflating: data_80k/output_images/8931.png  \n",
            "  inflating: data_80k/output_images/8932.png  \n",
            "  inflating: data_80k/output_images/8933.png  \n",
            "  inflating: data_80k/output_images/8934.png  \n",
            "  inflating: data_80k/output_images/8935.png  \n",
            "  inflating: data_80k/output_images/8936.png  \n",
            "  inflating: data_80k/output_images/8937.png  \n",
            "  inflating: data_80k/output_images/8938.png  \n",
            "  inflating: data_80k/output_images/8939.png  \n",
            "  inflating: data_80k/output_images/894.png  \n",
            "  inflating: data_80k/output_images/8940.png  \n",
            "  inflating: data_80k/output_images/8941.png  \n",
            "  inflating: data_80k/output_images/8942.png  \n",
            "  inflating: data_80k/output_images/8943.png  \n",
            "  inflating: data_80k/output_images/8944.png  \n",
            "  inflating: data_80k/output_images/8945.png  \n",
            "  inflating: data_80k/output_images/8946.png  \n",
            "  inflating: data_80k/output_images/8947.png  \n",
            "  inflating: data_80k/output_images/8948.png  \n",
            "  inflating: data_80k/output_images/8949.png  \n",
            "  inflating: data_80k/output_images/895.png  \n",
            "  inflating: data_80k/output_images/8950.png  \n",
            "  inflating: data_80k/output_images/8951.png  \n",
            "  inflating: data_80k/output_images/8952.png  \n",
            "  inflating: data_80k/output_images/8953.png  \n",
            "  inflating: data_80k/output_images/8954.png  \n",
            "  inflating: data_80k/output_images/8955.png  \n",
            "  inflating: data_80k/output_images/8956.png  \n",
            "  inflating: data_80k/output_images/8957.png  \n",
            "  inflating: data_80k/output_images/8958.png  \n",
            "  inflating: data_80k/output_images/8959.png  \n",
            "  inflating: data_80k/output_images/896.png  \n",
            "  inflating: data_80k/output_images/8960.png  \n",
            "  inflating: data_80k/output_images/8961.png  \n",
            "  inflating: data_80k/output_images/8962.png  \n",
            "  inflating: data_80k/output_images/8963.png  \n",
            "  inflating: data_80k/output_images/8964.png  \n",
            "  inflating: data_80k/output_images/8965.png  \n",
            "  inflating: data_80k/output_images/8966.png  \n",
            "  inflating: data_80k/output_images/8967.png  \n",
            "  inflating: data_80k/output_images/8968.png  \n",
            "  inflating: data_80k/output_images/8969.png  \n",
            "  inflating: data_80k/output_images/897.png  \n",
            "  inflating: data_80k/output_images/8970.png  \n",
            "  inflating: data_80k/output_images/8971.png  \n",
            "  inflating: data_80k/output_images/8972.png  \n",
            "  inflating: data_80k/output_images/8973.png  \n",
            "  inflating: data_80k/output_images/8974.png  \n",
            "  inflating: data_80k/output_images/8975.png  \n",
            "  inflating: data_80k/output_images/8976.png  \n",
            "  inflating: data_80k/output_images/8977.png  \n",
            "  inflating: data_80k/output_images/8978.png  \n",
            "  inflating: data_80k/output_images/8979.png  \n",
            "  inflating: data_80k/output_images/898.png  \n",
            "  inflating: data_80k/output_images/8980.png  \n",
            "  inflating: data_80k/output_images/8981.png  \n",
            "  inflating: data_80k/output_images/8982.png  \n",
            "  inflating: data_80k/output_images/8983.png  \n",
            "  inflating: data_80k/output_images/8984.png  \n",
            "  inflating: data_80k/output_images/8985.png  \n",
            "  inflating: data_80k/output_images/8986.png  \n",
            "  inflating: data_80k/output_images/8987.png  \n",
            "  inflating: data_80k/output_images/8988.png  \n",
            "  inflating: data_80k/output_images/8989.png  \n",
            "  inflating: data_80k/output_images/899.png  \n",
            "  inflating: data_80k/output_images/8990.png  \n",
            "  inflating: data_80k/output_images/8991.png  \n",
            "  inflating: data_80k/output_images/8992.png  \n",
            "  inflating: data_80k/output_images/8993.png  \n",
            "  inflating: data_80k/output_images/8994.png  \n",
            "  inflating: data_80k/output_images/8995.png  \n",
            "  inflating: data_80k/output_images/8996.png  \n",
            "  inflating: data_80k/output_images/8997.png  \n",
            "  inflating: data_80k/output_images/8998.png  \n",
            "  inflating: data_80k/output_images/8999.png  \n",
            "  inflating: data_80k/output_images/9.png  \n",
            "  inflating: data_80k/output_images/90.png  \n",
            "  inflating: data_80k/output_images/900.png  \n",
            "  inflating: data_80k/output_images/9000.png  \n",
            "  inflating: data_80k/output_images/9001.png  \n",
            "  inflating: data_80k/output_images/9002.png  \n",
            "  inflating: data_80k/output_images/9003.png  \n",
            "  inflating: data_80k/output_images/9004.png  \n",
            "  inflating: data_80k/output_images/9005.png  \n",
            "  inflating: data_80k/output_images/9006.png  \n",
            "  inflating: data_80k/output_images/9007.png  \n",
            "  inflating: data_80k/output_images/9008.png  \n",
            "  inflating: data_80k/output_images/9009.png  \n",
            "  inflating: data_80k/output_images/901.png  \n",
            "  inflating: data_80k/output_images/9010.png  \n",
            "  inflating: data_80k/output_images/9011.png  \n",
            "  inflating: data_80k/output_images/9012.png  \n",
            "  inflating: data_80k/output_images/9013.png  \n",
            "  inflating: data_80k/output_images/9014.png  \n",
            "  inflating: data_80k/output_images/9015.png  \n",
            "  inflating: data_80k/output_images/9016.png  \n",
            "  inflating: data_80k/output_images/9017.png  \n",
            "  inflating: data_80k/output_images/9018.png  \n",
            "  inflating: data_80k/output_images/9019.png  \n",
            "  inflating: data_80k/output_images/902.png  \n",
            "  inflating: data_80k/output_images/9020.png  \n",
            "  inflating: data_80k/output_images/9021.png  \n",
            "  inflating: data_80k/output_images/9022.png  \n",
            "  inflating: data_80k/output_images/9023.png  \n",
            "  inflating: data_80k/output_images/9024.png  \n",
            "  inflating: data_80k/output_images/9025.png  \n",
            "  inflating: data_80k/output_images/9026.png  \n",
            "  inflating: data_80k/output_images/9027.png  \n",
            "  inflating: data_80k/output_images/9028.png  \n",
            "  inflating: data_80k/output_images/9029.png  \n",
            "  inflating: data_80k/output_images/903.png  \n",
            "  inflating: data_80k/output_images/9030.png  \n",
            "  inflating: data_80k/output_images/9031.png  \n",
            "  inflating: data_80k/output_images/9032.png  \n",
            "  inflating: data_80k/output_images/9033.png  \n",
            "  inflating: data_80k/output_images/9034.png  \n",
            "  inflating: data_80k/output_images/9035.png  \n",
            "  inflating: data_80k/output_images/9036.png  \n",
            "  inflating: data_80k/output_images/9037.png  \n",
            "  inflating: data_80k/output_images/9038.png  \n",
            "  inflating: data_80k/output_images/9039.png  \n",
            "  inflating: data_80k/output_images/904.png  \n",
            "  inflating: data_80k/output_images/9040.png  \n",
            "  inflating: data_80k/output_images/9041.png  \n",
            "  inflating: data_80k/output_images/9042.png  \n",
            "  inflating: data_80k/output_images/9043.png  \n",
            "  inflating: data_80k/output_images/9044.png  \n",
            "  inflating: data_80k/output_images/9045.png  \n",
            "  inflating: data_80k/output_images/9046.png  \n",
            "  inflating: data_80k/output_images/9047.png  \n",
            "  inflating: data_80k/output_images/9048.png  \n",
            "  inflating: data_80k/output_images/9049.png  \n",
            "  inflating: data_80k/output_images/905.png  \n",
            "  inflating: data_80k/output_images/9050.png  \n",
            "  inflating: data_80k/output_images/9051.png  \n",
            "  inflating: data_80k/output_images/9052.png  \n",
            "  inflating: data_80k/output_images/9053.png  \n",
            "  inflating: data_80k/output_images/9054.png  \n",
            "  inflating: data_80k/output_images/9055.png  \n",
            "  inflating: data_80k/output_images/9056.png  \n",
            "  inflating: data_80k/output_images/9057.png  \n",
            "  inflating: data_80k/output_images/9058.png  \n",
            "  inflating: data_80k/output_images/9059.png  \n",
            "  inflating: data_80k/output_images/906.png  \n",
            "  inflating: data_80k/output_images/9060.png  \n",
            "  inflating: data_80k/output_images/9061.png  \n",
            "  inflating: data_80k/output_images/9062.png  \n",
            "  inflating: data_80k/output_images/9063.png  \n",
            "  inflating: data_80k/output_images/9064.png  \n",
            "  inflating: data_80k/output_images/9065.png  \n",
            "  inflating: data_80k/output_images/9066.png  \n",
            "  inflating: data_80k/output_images/9067.png  \n",
            "  inflating: data_80k/output_images/9068.png  \n",
            "  inflating: data_80k/output_images/9069.png  \n",
            "  inflating: data_80k/output_images/907.png  \n",
            "  inflating: data_80k/output_images/9070.png  \n",
            "  inflating: data_80k/output_images/9071.png  \n",
            "  inflating: data_80k/output_images/9072.png  \n",
            "  inflating: data_80k/output_images/9073.png  \n",
            "  inflating: data_80k/output_images/9074.png  \n",
            "  inflating: data_80k/output_images/9075.png  \n",
            "  inflating: data_80k/output_images/9076.png  \n",
            "  inflating: data_80k/output_images/9077.png  \n",
            "  inflating: data_80k/output_images/9078.png  \n",
            "  inflating: data_80k/output_images/9079.png  \n",
            "  inflating: data_80k/output_images/908.png  \n",
            "  inflating: data_80k/output_images/9080.png  \n",
            "  inflating: data_80k/output_images/9081.png  \n",
            "  inflating: data_80k/output_images/9082.png  \n",
            "  inflating: data_80k/output_images/9083.png  \n",
            "  inflating: data_80k/output_images/9084.png  \n",
            "  inflating: data_80k/output_images/9085.png  \n",
            "  inflating: data_80k/output_images/9086.png  \n",
            "  inflating: data_80k/output_images/9087.png  \n",
            "  inflating: data_80k/output_images/9088.png  \n",
            "  inflating: data_80k/output_images/9089.png  \n",
            "  inflating: data_80k/output_images/909.png  \n",
            "  inflating: data_80k/output_images/9090.png  \n",
            "  inflating: data_80k/output_images/9091.png  \n",
            "  inflating: data_80k/output_images/9092.png  \n",
            "  inflating: data_80k/output_images/9093.png  \n",
            "  inflating: data_80k/output_images/9094.png  \n",
            "  inflating: data_80k/output_images/9095.png  \n",
            "  inflating: data_80k/output_images/9096.png  \n",
            "  inflating: data_80k/output_images/9097.png  \n",
            "  inflating: data_80k/output_images/9098.png  \n",
            "  inflating: data_80k/output_images/9099.png  \n",
            "  inflating: data_80k/output_images/91.png  \n",
            "  inflating: data_80k/output_images/910.png  \n",
            "  inflating: data_80k/output_images/9100.png  \n",
            "  inflating: data_80k/output_images/9101.png  \n",
            "  inflating: data_80k/output_images/9102.png  \n",
            "  inflating: data_80k/output_images/9103.png  \n",
            "  inflating: data_80k/output_images/9104.png  \n",
            "  inflating: data_80k/output_images/9105.png  \n",
            "  inflating: data_80k/output_images/9106.png  \n",
            "  inflating: data_80k/output_images/9107.png  \n",
            "  inflating: data_80k/output_images/9108.png  \n",
            "  inflating: data_80k/output_images/9109.png  \n",
            "  inflating: data_80k/output_images/911.png  \n",
            "  inflating: data_80k/output_images/9110.png  \n",
            "  inflating: data_80k/output_images/9111.png  \n",
            "  inflating: data_80k/output_images/9112.png  \n",
            "  inflating: data_80k/output_images/9113.png  \n",
            "  inflating: data_80k/output_images/9114.png  \n",
            "  inflating: data_80k/output_images/9115.png  \n",
            "  inflating: data_80k/output_images/9116.png  \n",
            "  inflating: data_80k/output_images/9117.png  \n",
            "  inflating: data_80k/output_images/9118.png  \n",
            "  inflating: data_80k/output_images/9119.png  \n",
            "  inflating: data_80k/output_images/912.png  \n",
            "  inflating: data_80k/output_images/9120.png  \n",
            "  inflating: data_80k/output_images/9121.png  \n",
            "  inflating: data_80k/output_images/9122.png  \n",
            "  inflating: data_80k/output_images/9123.png  \n",
            "  inflating: data_80k/output_images/9124.png  \n",
            "  inflating: data_80k/output_images/9125.png  \n",
            "  inflating: data_80k/output_images/9126.png  \n",
            "  inflating: data_80k/output_images/9127.png  \n",
            "  inflating: data_80k/output_images/9128.png  \n",
            "  inflating: data_80k/output_images/9129.png  \n",
            "  inflating: data_80k/output_images/913.png  \n",
            "  inflating: data_80k/output_images/9130.png  \n",
            "  inflating: data_80k/output_images/9131.png  \n",
            "  inflating: data_80k/output_images/9132.png  \n",
            "  inflating: data_80k/output_images/9133.png  \n",
            "  inflating: data_80k/output_images/9134.png  \n",
            "  inflating: data_80k/output_images/9135.png  \n",
            "  inflating: data_80k/output_images/9136.png  \n",
            "  inflating: data_80k/output_images/9137.png  \n",
            "  inflating: data_80k/output_images/9138.png  \n",
            "  inflating: data_80k/output_images/9139.png  \n",
            "  inflating: data_80k/output_images/914.png  \n",
            "  inflating: data_80k/output_images/9140.png  \n",
            "  inflating: data_80k/output_images/9141.png  \n",
            "  inflating: data_80k/output_images/9142.png  \n",
            "  inflating: data_80k/output_images/9143.png  \n",
            "  inflating: data_80k/output_images/9144.png  \n",
            "  inflating: data_80k/output_images/9145.png  \n",
            "  inflating: data_80k/output_images/9146.png  \n",
            "  inflating: data_80k/output_images/9147.png  \n",
            "  inflating: data_80k/output_images/9148.png  \n",
            "  inflating: data_80k/output_images/9149.png  \n",
            "  inflating: data_80k/output_images/915.png  \n",
            "  inflating: data_80k/output_images/9150.png  \n",
            "  inflating: data_80k/output_images/9151.png  \n",
            "  inflating: data_80k/output_images/9152.png  \n",
            "  inflating: data_80k/output_images/9153.png  \n",
            "  inflating: data_80k/output_images/9154.png  \n",
            "  inflating: data_80k/output_images/9155.png  \n",
            "  inflating: data_80k/output_images/9156.png  \n",
            "  inflating: data_80k/output_images/9157.png  \n",
            "  inflating: data_80k/output_images/9158.png  \n",
            "  inflating: data_80k/output_images/9159.png  \n",
            "  inflating: data_80k/output_images/916.png  \n",
            "  inflating: data_80k/output_images/9160.png  \n",
            "  inflating: data_80k/output_images/9161.png  \n",
            "  inflating: data_80k/output_images/9162.png  \n",
            "  inflating: data_80k/output_images/9163.png  \n",
            "  inflating: data_80k/output_images/9164.png  \n",
            "  inflating: data_80k/output_images/9165.png  \n",
            "  inflating: data_80k/output_images/9166.png  \n",
            "  inflating: data_80k/output_images/9167.png  \n",
            "  inflating: data_80k/output_images/9168.png  \n",
            "  inflating: data_80k/output_images/9169.png  \n",
            "  inflating: data_80k/output_images/917.png  \n",
            "  inflating: data_80k/output_images/9170.png  \n",
            "  inflating: data_80k/output_images/9171.png  \n",
            "  inflating: data_80k/output_images/9172.png  \n",
            "  inflating: data_80k/output_images/9173.png  \n",
            "  inflating: data_80k/output_images/9174.png  \n",
            "  inflating: data_80k/output_images/9175.png  \n",
            "  inflating: data_80k/output_images/9176.png  \n",
            "  inflating: data_80k/output_images/9177.png  \n",
            "  inflating: data_80k/output_images/9178.png  \n",
            "  inflating: data_80k/output_images/9179.png  \n",
            "  inflating: data_80k/output_images/918.png  \n",
            "  inflating: data_80k/output_images/9180.png  \n",
            "  inflating: data_80k/output_images/9181.png  \n",
            "  inflating: data_80k/output_images/9182.png  \n",
            "  inflating: data_80k/output_images/9183.png  \n",
            "  inflating: data_80k/output_images/9184.png  \n",
            "  inflating: data_80k/output_images/9185.png  \n",
            "  inflating: data_80k/output_images/9186.png  \n",
            "  inflating: data_80k/output_images/9187.png  \n",
            "  inflating: data_80k/output_images/9188.png  \n",
            "  inflating: data_80k/output_images/9189.png  \n",
            "  inflating: data_80k/output_images/919.png  \n",
            "  inflating: data_80k/output_images/9190.png  \n",
            "  inflating: data_80k/output_images/9191.png  \n",
            "  inflating: data_80k/output_images/9192.png  \n",
            "  inflating: data_80k/output_images/9193.png  \n",
            "  inflating: data_80k/output_images/9194.png  \n",
            "  inflating: data_80k/output_images/9195.png  \n",
            "  inflating: data_80k/output_images/9196.png  \n",
            "  inflating: data_80k/output_images/9197.png  \n",
            "  inflating: data_80k/output_images/9198.png  \n",
            "  inflating: data_80k/output_images/9199.png  \n",
            "  inflating: data_80k/output_images/92.png  \n",
            "  inflating: data_80k/output_images/920.png  \n",
            "  inflating: data_80k/output_images/9200.png  \n",
            "  inflating: data_80k/output_images/9201.png  \n",
            "  inflating: data_80k/output_images/9202.png  \n",
            "  inflating: data_80k/output_images/9203.png  \n",
            "  inflating: data_80k/output_images/9204.png  \n",
            "  inflating: data_80k/output_images/9205.png  \n",
            "  inflating: data_80k/output_images/9206.png  \n",
            "  inflating: data_80k/output_images/9207.png  \n",
            "  inflating: data_80k/output_images/9208.png  \n",
            "  inflating: data_80k/output_images/9209.png  \n",
            "  inflating: data_80k/output_images/921.png  \n",
            "  inflating: data_80k/output_images/9210.png  \n",
            "  inflating: data_80k/output_images/9211.png  \n",
            "  inflating: data_80k/output_images/9212.png  \n",
            "  inflating: data_80k/output_images/9213.png  \n",
            "  inflating: data_80k/output_images/9214.png  \n",
            "  inflating: data_80k/output_images/9215.png  \n",
            "  inflating: data_80k/output_images/9216.png  \n",
            "  inflating: data_80k/output_images/9217.png  \n",
            "  inflating: data_80k/output_images/9218.png  \n",
            "  inflating: data_80k/output_images/9219.png  \n",
            "  inflating: data_80k/output_images/922.png  \n",
            "  inflating: data_80k/output_images/9220.png  \n",
            "  inflating: data_80k/output_images/9221.png  \n",
            "  inflating: data_80k/output_images/9222.png  \n",
            "  inflating: data_80k/output_images/9223.png  \n",
            "  inflating: data_80k/output_images/9224.png  \n",
            "  inflating: data_80k/output_images/9225.png  \n",
            "  inflating: data_80k/output_images/9226.png  \n",
            "  inflating: data_80k/output_images/9227.png  \n",
            "  inflating: data_80k/output_images/9228.png  \n",
            "  inflating: data_80k/output_images/9229.png  \n",
            "  inflating: data_80k/output_images/923.png  \n",
            "  inflating: data_80k/output_images/9230.png  \n",
            "  inflating: data_80k/output_images/9231.png  \n",
            "  inflating: data_80k/output_images/9232.png  \n",
            "  inflating: data_80k/output_images/9233.png  \n",
            "  inflating: data_80k/output_images/9234.png  \n",
            "  inflating: data_80k/output_images/9235.png  \n",
            "  inflating: data_80k/output_images/9236.png  \n",
            "  inflating: data_80k/output_images/9237.png  \n",
            "  inflating: data_80k/output_images/9238.png  \n",
            "  inflating: data_80k/output_images/9239.png  \n",
            "  inflating: data_80k/output_images/924.png  \n",
            "  inflating: data_80k/output_images/9240.png  \n",
            "  inflating: data_80k/output_images/9241.png  \n",
            "  inflating: data_80k/output_images/9242.png  \n",
            "  inflating: data_80k/output_images/9243.png  \n",
            "  inflating: data_80k/output_images/9244.png  \n",
            "  inflating: data_80k/output_images/9245.png  \n",
            "  inflating: data_80k/output_images/9246.png  \n",
            "  inflating: data_80k/output_images/9247.png  \n",
            "  inflating: data_80k/output_images/9248.png  \n",
            "  inflating: data_80k/output_images/9249.png  \n",
            "  inflating: data_80k/output_images/925.png  \n",
            "  inflating: data_80k/output_images/9250.png  \n",
            "  inflating: data_80k/output_images/9251.png  \n",
            "  inflating: data_80k/output_images/9252.png  \n",
            "  inflating: data_80k/output_images/9253.png  \n",
            "  inflating: data_80k/output_images/9254.png  \n",
            "  inflating: data_80k/output_images/9255.png  \n",
            "  inflating: data_80k/output_images/9256.png  \n",
            "  inflating: data_80k/output_images/9257.png  \n",
            "  inflating: data_80k/output_images/9258.png  \n",
            "  inflating: data_80k/output_images/9259.png  \n",
            "  inflating: data_80k/output_images/926.png  \n",
            "  inflating: data_80k/output_images/9260.png  \n",
            "  inflating: data_80k/output_images/9261.png  \n",
            "  inflating: data_80k/output_images/9262.png  \n",
            "  inflating: data_80k/output_images/9263.png  \n",
            "  inflating: data_80k/output_images/9264.png  \n",
            "  inflating: data_80k/output_images/9265.png  \n",
            "  inflating: data_80k/output_images/9266.png  \n",
            "  inflating: data_80k/output_images/9267.png  \n",
            "  inflating: data_80k/output_images/9268.png  \n",
            "  inflating: data_80k/output_images/9269.png  \n",
            "  inflating: data_80k/output_images/927.png  \n",
            "  inflating: data_80k/output_images/9270.png  \n",
            "  inflating: data_80k/output_images/9271.png  \n",
            "  inflating: data_80k/output_images/9272.png  \n",
            "  inflating: data_80k/output_images/9273.png  \n",
            "  inflating: data_80k/output_images/9274.png  \n",
            "  inflating: data_80k/output_images/9275.png  \n",
            "  inflating: data_80k/output_images/9276.png  \n",
            "  inflating: data_80k/output_images/9277.png  \n",
            "  inflating: data_80k/output_images/9278.png  \n",
            "  inflating: data_80k/output_images/9279.png  \n",
            "  inflating: data_80k/output_images/928.png  \n",
            "  inflating: data_80k/output_images/9280.png  \n",
            "  inflating: data_80k/output_images/9281.png  \n",
            "  inflating: data_80k/output_images/9282.png  \n",
            "  inflating: data_80k/output_images/9283.png  \n",
            "  inflating: data_80k/output_images/9284.png  \n",
            "  inflating: data_80k/output_images/9285.png  \n",
            "  inflating: data_80k/output_images/9286.png  \n",
            "  inflating: data_80k/output_images/9287.png  \n",
            "  inflating: data_80k/output_images/9288.png  \n",
            "  inflating: data_80k/output_images/9289.png  \n",
            "  inflating: data_80k/output_images/929.png  \n",
            "  inflating: data_80k/output_images/9290.png  \n",
            "  inflating: data_80k/output_images/9291.png  \n",
            "  inflating: data_80k/output_images/9292.png  \n",
            "  inflating: data_80k/output_images/9293.png  \n",
            "  inflating: data_80k/output_images/9294.png  \n",
            "  inflating: data_80k/output_images/9295.png  \n",
            "  inflating: data_80k/output_images/9296.png  \n",
            "  inflating: data_80k/output_images/9297.png  \n",
            "  inflating: data_80k/output_images/9298.png  \n",
            "  inflating: data_80k/output_images/9299.png  \n",
            "  inflating: data_80k/output_images/93.png  \n",
            "  inflating: data_80k/output_images/930.png  \n",
            "  inflating: data_80k/output_images/9300.png  \n",
            "  inflating: data_80k/output_images/9301.png  \n",
            "  inflating: data_80k/output_images/9302.png  \n",
            "  inflating: data_80k/output_images/9303.png  \n",
            "  inflating: data_80k/output_images/9304.png  \n",
            "  inflating: data_80k/output_images/9305.png  \n",
            "  inflating: data_80k/output_images/9306.png  \n",
            "  inflating: data_80k/output_images/9307.png  \n",
            "  inflating: data_80k/output_images/9308.png  \n",
            "  inflating: data_80k/output_images/9309.png  \n",
            "  inflating: data_80k/output_images/931.png  \n",
            "  inflating: data_80k/output_images/9310.png  \n",
            "  inflating: data_80k/output_images/9311.png  \n",
            "  inflating: data_80k/output_images/9312.png  \n",
            "  inflating: data_80k/output_images/9313.png  \n",
            "  inflating: data_80k/output_images/9314.png  \n",
            "  inflating: data_80k/output_images/9315.png  \n",
            "  inflating: data_80k/output_images/9316.png  \n",
            "  inflating: data_80k/output_images/9317.png  \n",
            "  inflating: data_80k/output_images/9318.png  \n",
            "  inflating: data_80k/output_images/9319.png  \n",
            "  inflating: data_80k/output_images/932.png  \n",
            "  inflating: data_80k/output_images/9320.png  \n",
            "  inflating: data_80k/output_images/9321.png  \n",
            "  inflating: data_80k/output_images/9322.png  \n",
            "  inflating: data_80k/output_images/9323.png  \n",
            "  inflating: data_80k/output_images/9324.png  \n",
            "  inflating: data_80k/output_images/9325.png  \n",
            "  inflating: data_80k/output_images/9326.png  \n",
            "  inflating: data_80k/output_images/9327.png  \n",
            "  inflating: data_80k/output_images/9328.png  \n",
            "  inflating: data_80k/output_images/9329.png  \n",
            "  inflating: data_80k/output_images/933.png  \n",
            "  inflating: data_80k/output_images/9330.png  \n",
            "  inflating: data_80k/output_images/9331.png  \n",
            "  inflating: data_80k/output_images/9332.png  \n",
            "  inflating: data_80k/output_images/9333.png  \n",
            "  inflating: data_80k/output_images/9334.png  \n",
            "  inflating: data_80k/output_images/9335.png  \n",
            "  inflating: data_80k/output_images/9336.png  \n",
            "  inflating: data_80k/output_images/9337.png  \n",
            "  inflating: data_80k/output_images/9338.png  \n",
            "  inflating: data_80k/output_images/9339.png  \n",
            "  inflating: data_80k/output_images/934.png  \n",
            "  inflating: data_80k/output_images/9340.png  \n",
            "  inflating: data_80k/output_images/9341.png  \n",
            "  inflating: data_80k/output_images/9342.png  \n",
            "  inflating: data_80k/output_images/9343.png  \n",
            "  inflating: data_80k/output_images/9344.png  \n",
            "  inflating: data_80k/output_images/9345.png  \n",
            "  inflating: data_80k/output_images/9346.png  \n",
            "  inflating: data_80k/output_images/9347.png  \n",
            "  inflating: data_80k/output_images/9348.png  \n",
            "  inflating: data_80k/output_images/9349.png  \n",
            "  inflating: data_80k/output_images/935.png  \n",
            "  inflating: data_80k/output_images/9350.png  \n",
            "  inflating: data_80k/output_images/9351.png  \n",
            "  inflating: data_80k/output_images/9352.png  \n",
            "  inflating: data_80k/output_images/9353.png  \n",
            "  inflating: data_80k/output_images/9354.png  \n",
            "  inflating: data_80k/output_images/9355.png  \n",
            "  inflating: data_80k/output_images/9356.png  \n",
            "  inflating: data_80k/output_images/9357.png  \n",
            "  inflating: data_80k/output_images/9358.png  \n",
            "  inflating: data_80k/output_images/9359.png  \n",
            "  inflating: data_80k/output_images/936.png  \n",
            "  inflating: data_80k/output_images/9360.png  \n",
            "  inflating: data_80k/output_images/9361.png  \n",
            "  inflating: data_80k/output_images/9362.png  \n",
            "  inflating: data_80k/output_images/9363.png  \n",
            "  inflating: data_80k/output_images/9364.png  \n",
            "  inflating: data_80k/output_images/9365.png  \n",
            "  inflating: data_80k/output_images/9366.png  \n",
            "  inflating: data_80k/output_images/9367.png  \n",
            "  inflating: data_80k/output_images/9368.png  \n",
            "  inflating: data_80k/output_images/9369.png  \n",
            "  inflating: data_80k/output_images/937.png  \n",
            "  inflating: data_80k/output_images/9370.png  \n",
            "  inflating: data_80k/output_images/9371.png  \n",
            "  inflating: data_80k/output_images/9372.png  \n",
            "  inflating: data_80k/output_images/9373.png  \n",
            "  inflating: data_80k/output_images/9374.png  \n",
            "  inflating: data_80k/output_images/9375.png  \n",
            "  inflating: data_80k/output_images/9376.png  \n",
            "  inflating: data_80k/output_images/9377.png  \n",
            "  inflating: data_80k/output_images/9378.png  \n",
            "  inflating: data_80k/output_images/9379.png  \n",
            "  inflating: data_80k/output_images/938.png  \n",
            "  inflating: data_80k/output_images/9380.png  \n",
            "  inflating: data_80k/output_images/9381.png  \n",
            "  inflating: data_80k/output_images/9382.png  \n",
            "  inflating: data_80k/output_images/9383.png  \n",
            "  inflating: data_80k/output_images/9384.png  \n",
            "  inflating: data_80k/output_images/9385.png  \n",
            "  inflating: data_80k/output_images/9386.png  \n",
            "  inflating: data_80k/output_images/9387.png  \n",
            "  inflating: data_80k/output_images/9388.png  \n",
            "  inflating: data_80k/output_images/9389.png  \n",
            "  inflating: data_80k/output_images/939.png  \n",
            "  inflating: data_80k/output_images/9390.png  \n",
            "  inflating: data_80k/output_images/9391.png  \n",
            "  inflating: data_80k/output_images/9392.png  \n",
            "  inflating: data_80k/output_images/9393.png  \n",
            "  inflating: data_80k/output_images/9394.png  \n",
            "  inflating: data_80k/output_images/9395.png  \n",
            "  inflating: data_80k/output_images/9396.png  \n",
            "  inflating: data_80k/output_images/9397.png  \n",
            "  inflating: data_80k/output_images/9398.png  \n",
            "  inflating: data_80k/output_images/9399.png  \n",
            "  inflating: data_80k/output_images/94.png  \n",
            "  inflating: data_80k/output_images/940.png  \n",
            "  inflating: data_80k/output_images/9400.png  \n",
            "  inflating: data_80k/output_images/9401.png  \n",
            "  inflating: data_80k/output_images/9402.png  \n",
            "  inflating: data_80k/output_images/9403.png  \n",
            "  inflating: data_80k/output_images/9404.png  \n",
            "  inflating: data_80k/output_images/9405.png  \n",
            "  inflating: data_80k/output_images/9406.png  \n",
            "  inflating: data_80k/output_images/9407.png  \n",
            "  inflating: data_80k/output_images/9408.png  \n",
            "  inflating: data_80k/output_images/9409.png  \n",
            "  inflating: data_80k/output_images/941.png  \n",
            "  inflating: data_80k/output_images/9410.png  \n",
            "  inflating: data_80k/output_images/9411.png  \n",
            "  inflating: data_80k/output_images/9412.png  \n",
            "  inflating: data_80k/output_images/9413.png  \n",
            "  inflating: data_80k/output_images/9414.png  \n",
            "  inflating: data_80k/output_images/9415.png  \n",
            "  inflating: data_80k/output_images/9416.png  \n",
            "  inflating: data_80k/output_images/9417.png  \n",
            "  inflating: data_80k/output_images/9418.png  \n",
            "  inflating: data_80k/output_images/9419.png  \n",
            "  inflating: data_80k/output_images/942.png  \n",
            "  inflating: data_80k/output_images/9420.png  \n",
            "  inflating: data_80k/output_images/9421.png  \n",
            "  inflating: data_80k/output_images/9422.png  \n",
            "  inflating: data_80k/output_images/9423.png  \n",
            "  inflating: data_80k/output_images/9424.png  \n",
            "  inflating: data_80k/output_images/9425.png  \n",
            "  inflating: data_80k/output_images/9426.png  \n",
            "  inflating: data_80k/output_images/9427.png  \n",
            "  inflating: data_80k/output_images/9428.png  \n",
            "  inflating: data_80k/output_images/9429.png  \n",
            "  inflating: data_80k/output_images/943.png  \n",
            "  inflating: data_80k/output_images/9430.png  \n",
            "  inflating: data_80k/output_images/9431.png  \n",
            "  inflating: data_80k/output_images/9432.png  \n",
            "  inflating: data_80k/output_images/9433.png  \n",
            "  inflating: data_80k/output_images/9434.png  \n",
            "  inflating: data_80k/output_images/9435.png  \n",
            "  inflating: data_80k/output_images/9436.png  \n",
            "  inflating: data_80k/output_images/9437.png  \n",
            "  inflating: data_80k/output_images/9438.png  \n",
            "  inflating: data_80k/output_images/9439.png  \n",
            "  inflating: data_80k/output_images/944.png  \n",
            "  inflating: data_80k/output_images/9440.png  \n",
            "  inflating: data_80k/output_images/9441.png  \n",
            "  inflating: data_80k/output_images/9442.png  \n",
            "  inflating: data_80k/output_images/9443.png  \n",
            "  inflating: data_80k/output_images/9444.png  \n",
            "  inflating: data_80k/output_images/9445.png  \n",
            "  inflating: data_80k/output_images/9446.png  \n",
            "  inflating: data_80k/output_images/9447.png  \n",
            "  inflating: data_80k/output_images/9448.png  \n",
            "  inflating: data_80k/output_images/9449.png  \n",
            "  inflating: data_80k/output_images/945.png  \n",
            "  inflating: data_80k/output_images/9450.png  \n",
            "  inflating: data_80k/output_images/9451.png  \n",
            "  inflating: data_80k/output_images/9452.png  \n",
            "  inflating: data_80k/output_images/9453.png  \n",
            "  inflating: data_80k/output_images/9454.png  \n",
            "  inflating: data_80k/output_images/9455.png  \n",
            "  inflating: data_80k/output_images/9456.png  \n",
            "  inflating: data_80k/output_images/9457.png  \n",
            "  inflating: data_80k/output_images/9458.png  \n",
            "  inflating: data_80k/output_images/9459.png  \n",
            "  inflating: data_80k/output_images/946.png  \n",
            "  inflating: data_80k/output_images/9460.png  \n",
            "  inflating: data_80k/output_images/9461.png  \n",
            "  inflating: data_80k/output_images/9462.png  \n",
            "  inflating: data_80k/output_images/9463.png  \n",
            "  inflating: data_80k/output_images/9464.png  \n",
            "  inflating: data_80k/output_images/9465.png  \n",
            "  inflating: data_80k/output_images/9466.png  \n",
            "  inflating: data_80k/output_images/9467.png  \n",
            "  inflating: data_80k/output_images/9468.png  \n",
            "  inflating: data_80k/output_images/9469.png  \n",
            "  inflating: data_80k/output_images/947.png  \n",
            "  inflating: data_80k/output_images/9470.png  \n",
            "  inflating: data_80k/output_images/9471.png  \n",
            "  inflating: data_80k/output_images/9472.png  \n",
            "  inflating: data_80k/output_images/9473.png  \n",
            "  inflating: data_80k/output_images/9474.png  \n",
            "  inflating: data_80k/output_images/9475.png  \n",
            "  inflating: data_80k/output_images/9476.png  \n",
            "  inflating: data_80k/output_images/9477.png  \n",
            "  inflating: data_80k/output_images/9478.png  \n",
            "  inflating: data_80k/output_images/9479.png  \n",
            "  inflating: data_80k/output_images/948.png  \n",
            "  inflating: data_80k/output_images/9480.png  \n",
            "  inflating: data_80k/output_images/9481.png  \n",
            "  inflating: data_80k/output_images/9482.png  \n",
            "  inflating: data_80k/output_images/9483.png  \n",
            "  inflating: data_80k/output_images/9484.png  \n",
            "  inflating: data_80k/output_images/9485.png  \n",
            "  inflating: data_80k/output_images/9486.png  \n",
            "  inflating: data_80k/output_images/9487.png  \n",
            "  inflating: data_80k/output_images/9488.png  \n",
            "  inflating: data_80k/output_images/9489.png  \n",
            "  inflating: data_80k/output_images/949.png  \n",
            "  inflating: data_80k/output_images/9490.png  \n",
            "  inflating: data_80k/output_images/9491.png  \n",
            "  inflating: data_80k/output_images/9492.png  \n",
            "  inflating: data_80k/output_images/9493.png  \n",
            "  inflating: data_80k/output_images/9494.png  \n",
            "  inflating: data_80k/output_images/9495.png  \n",
            "  inflating: data_80k/output_images/9496.png  \n",
            "  inflating: data_80k/output_images/9497.png  \n",
            "  inflating: data_80k/output_images/9498.png  \n",
            "  inflating: data_80k/output_images/9499.png  \n",
            "  inflating: data_80k/output_images/95.png  \n",
            "  inflating: data_80k/output_images/950.png  \n",
            "  inflating: data_80k/output_images/9500.png  \n",
            "  inflating: data_80k/output_images/9501.png  \n",
            "  inflating: data_80k/output_images/9502.png  \n",
            "  inflating: data_80k/output_images/9503.png  \n",
            "  inflating: data_80k/output_images/9504.png  \n",
            "  inflating: data_80k/output_images/9505.png  \n",
            "  inflating: data_80k/output_images/9506.png  \n",
            "  inflating: data_80k/output_images/9507.png  \n",
            "  inflating: data_80k/output_images/9508.png  \n",
            "  inflating: data_80k/output_images/9509.png  \n",
            "  inflating: data_80k/output_images/951.png  \n",
            "  inflating: data_80k/output_images/9510.png  \n",
            "  inflating: data_80k/output_images/9511.png  \n",
            "  inflating: data_80k/output_images/9512.png  \n",
            "  inflating: data_80k/output_images/9513.png  \n",
            "  inflating: data_80k/output_images/9514.png  \n",
            "  inflating: data_80k/output_images/9515.png  \n",
            "  inflating: data_80k/output_images/9516.png  \n",
            "  inflating: data_80k/output_images/9517.png  \n",
            "  inflating: data_80k/output_images/9518.png  \n",
            "  inflating: data_80k/output_images/9519.png  \n",
            "  inflating: data_80k/output_images/952.png  \n",
            "  inflating: data_80k/output_images/9520.png  \n",
            "  inflating: data_80k/output_images/9521.png  \n",
            "  inflating: data_80k/output_images/9522.png  \n",
            "  inflating: data_80k/output_images/9523.png  \n",
            "  inflating: data_80k/output_images/9524.png  \n",
            "  inflating: data_80k/output_images/9525.png  \n",
            "  inflating: data_80k/output_images/9526.png  \n",
            "  inflating: data_80k/output_images/9527.png  \n",
            "  inflating: data_80k/output_images/9528.png  \n",
            "  inflating: data_80k/output_images/9529.png  \n",
            "  inflating: data_80k/output_images/953.png  \n",
            "  inflating: data_80k/output_images/9530.png  \n",
            "  inflating: data_80k/output_images/9531.png  \n",
            "  inflating: data_80k/output_images/9532.png  \n",
            "  inflating: data_80k/output_images/9533.png  \n",
            "  inflating: data_80k/output_images/9534.png  \n",
            "  inflating: data_80k/output_images/9535.png  \n",
            "  inflating: data_80k/output_images/9536.png  \n",
            "  inflating: data_80k/output_images/9537.png  \n",
            "  inflating: data_80k/output_images/9538.png  \n",
            "  inflating: data_80k/output_images/9539.png  \n",
            "  inflating: data_80k/output_images/954.png  \n",
            "  inflating: data_80k/output_images/9540.png  \n",
            "  inflating: data_80k/output_images/9541.png  \n",
            "  inflating: data_80k/output_images/9542.png  \n",
            "  inflating: data_80k/output_images/9543.png  \n",
            "  inflating: data_80k/output_images/9544.png  \n",
            "  inflating: data_80k/output_images/9545.png  \n",
            "  inflating: data_80k/output_images/9546.png  \n",
            "  inflating: data_80k/output_images/9547.png  \n",
            "  inflating: data_80k/output_images/9548.png  \n",
            "  inflating: data_80k/output_images/9549.png  \n",
            "  inflating: data_80k/output_images/955.png  \n",
            "  inflating: data_80k/output_images/9550.png  \n",
            "  inflating: data_80k/output_images/9551.png  \n",
            "  inflating: data_80k/output_images/9552.png  \n",
            "  inflating: data_80k/output_images/9553.png  \n",
            "  inflating: data_80k/output_images/9554.png  \n",
            "  inflating: data_80k/output_images/9555.png  \n",
            "  inflating: data_80k/output_images/9556.png  \n",
            "  inflating: data_80k/output_images/9557.png  \n",
            "  inflating: data_80k/output_images/9558.png  \n",
            "  inflating: data_80k/output_images/9559.png  \n",
            "  inflating: data_80k/output_images/956.png  \n",
            "  inflating: data_80k/output_images/9560.png  \n",
            "  inflating: data_80k/output_images/9561.png  \n",
            "  inflating: data_80k/output_images/9562.png  \n",
            "  inflating: data_80k/output_images/9563.png  \n",
            "  inflating: data_80k/output_images/9564.png  \n",
            "  inflating: data_80k/output_images/9565.png  \n",
            "  inflating: data_80k/output_images/9566.png  \n",
            "  inflating: data_80k/output_images/9567.png  \n",
            "  inflating: data_80k/output_images/9568.png  \n",
            "  inflating: data_80k/output_images/9569.png  \n",
            "  inflating: data_80k/output_images/957.png  \n",
            "  inflating: data_80k/output_images/9570.png  \n",
            "  inflating: data_80k/output_images/9571.png  \n",
            "  inflating: data_80k/output_images/9572.png  \n",
            "  inflating: data_80k/output_images/9573.png  \n",
            "  inflating: data_80k/output_images/9574.png  \n",
            "  inflating: data_80k/output_images/9575.png  \n",
            "  inflating: data_80k/output_images/9576.png  \n",
            "  inflating: data_80k/output_images/9577.png  \n",
            "  inflating: data_80k/output_images/9578.png  \n",
            "  inflating: data_80k/output_images/9579.png  \n",
            "  inflating: data_80k/output_images/958.png  \n",
            "  inflating: data_80k/output_images/9580.png  \n",
            "  inflating: data_80k/output_images/9581.png  \n",
            "  inflating: data_80k/output_images/9582.png  \n",
            "  inflating: data_80k/output_images/9583.png  \n",
            "  inflating: data_80k/output_images/9584.png  \n",
            "  inflating: data_80k/output_images/9585.png  \n",
            "  inflating: data_80k/output_images/9586.png  \n",
            "  inflating: data_80k/output_images/9587.png  \n",
            "  inflating: data_80k/output_images/9588.png  \n",
            "  inflating: data_80k/output_images/9589.png  \n",
            "  inflating: data_80k/output_images/959.png  \n",
            "  inflating: data_80k/output_images/9590.png  \n",
            "  inflating: data_80k/output_images/9591.png  \n",
            "  inflating: data_80k/output_images/9592.png  \n",
            "  inflating: data_80k/output_images/9593.png  \n",
            "  inflating: data_80k/output_images/9594.png  \n",
            "  inflating: data_80k/output_images/9595.png  \n",
            "  inflating: data_80k/output_images/9596.png  \n",
            "  inflating: data_80k/output_images/9597.png  \n",
            "  inflating: data_80k/output_images/9598.png  \n",
            "  inflating: data_80k/output_images/9599.png  \n",
            "  inflating: data_80k/output_images/96.png  \n",
            "  inflating: data_80k/output_images/960.png  \n",
            "  inflating: data_80k/output_images/9600.png  \n",
            "  inflating: data_80k/output_images/9601.png  \n",
            "  inflating: data_80k/output_images/9602.png  \n",
            "  inflating: data_80k/output_images/9603.png  \n",
            "  inflating: data_80k/output_images/9604.png  \n",
            "  inflating: data_80k/output_images/9605.png  \n",
            "  inflating: data_80k/output_images/9606.png  \n",
            "  inflating: data_80k/output_images/9607.png  \n",
            "  inflating: data_80k/output_images/9608.png  \n",
            "  inflating: data_80k/output_images/9609.png  \n",
            "  inflating: data_80k/output_images/961.png  \n",
            "  inflating: data_80k/output_images/9610.png  \n",
            "  inflating: data_80k/output_images/9611.png  \n",
            "  inflating: data_80k/output_images/9612.png  \n",
            "  inflating: data_80k/output_images/9613.png  \n",
            "  inflating: data_80k/output_images/9614.png  \n",
            "  inflating: data_80k/output_images/9615.png  \n",
            "  inflating: data_80k/output_images/9616.png  \n",
            "  inflating: data_80k/output_images/9617.png  \n",
            "  inflating: data_80k/output_images/9618.png  \n",
            "  inflating: data_80k/output_images/9619.png  \n",
            "  inflating: data_80k/output_images/962.png  \n",
            "  inflating: data_80k/output_images/9620.png  \n",
            "  inflating: data_80k/output_images/9621.png  \n",
            "  inflating: data_80k/output_images/9622.png  \n",
            "  inflating: data_80k/output_images/9623.png  \n",
            "  inflating: data_80k/output_images/9624.png  \n",
            "  inflating: data_80k/output_images/9625.png  \n",
            "  inflating: data_80k/output_images/9626.png  \n",
            "  inflating: data_80k/output_images/9627.png  \n",
            "  inflating: data_80k/output_images/9628.png  \n",
            "  inflating: data_80k/output_images/9629.png  \n",
            "  inflating: data_80k/output_images/963.png  \n",
            "  inflating: data_80k/output_images/9630.png  \n",
            "  inflating: data_80k/output_images/9631.png  \n",
            "  inflating: data_80k/output_images/9632.png  \n",
            "  inflating: data_80k/output_images/9633.png  \n",
            "  inflating: data_80k/output_images/9634.png  \n",
            "  inflating: data_80k/output_images/9635.png  \n",
            "  inflating: data_80k/output_images/9636.png  \n",
            "  inflating: data_80k/output_images/9637.png  \n",
            "  inflating: data_80k/output_images/9638.png  \n",
            "  inflating: data_80k/output_images/9639.png  \n",
            "  inflating: data_80k/output_images/964.png  \n",
            "  inflating: data_80k/output_images/9640.png  \n",
            "  inflating: data_80k/output_images/9641.png  \n",
            "  inflating: data_80k/output_images/9642.png  \n",
            "  inflating: data_80k/output_images/9643.png  \n",
            "  inflating: data_80k/output_images/9644.png  \n",
            "  inflating: data_80k/output_images/9645.png  \n",
            "  inflating: data_80k/output_images/9646.png  \n",
            "  inflating: data_80k/output_images/9647.png  \n",
            "  inflating: data_80k/output_images/9648.png  \n",
            "  inflating: data_80k/output_images/9649.png  \n",
            "  inflating: data_80k/output_images/965.png  \n",
            "  inflating: data_80k/output_images/9650.png  \n",
            "  inflating: data_80k/output_images/9651.png  \n",
            "  inflating: data_80k/output_images/9652.png  \n",
            "  inflating: data_80k/output_images/9653.png  \n",
            "  inflating: data_80k/output_images/9654.png  \n",
            "  inflating: data_80k/output_images/9655.png  \n",
            "  inflating: data_80k/output_images/9656.png  \n",
            "  inflating: data_80k/output_images/9657.png  \n",
            "  inflating: data_80k/output_images/9658.png  \n",
            "  inflating: data_80k/output_images/9659.png  \n",
            "  inflating: data_80k/output_images/966.png  \n",
            "  inflating: data_80k/output_images/9660.png  \n",
            "  inflating: data_80k/output_images/9661.png  \n",
            "  inflating: data_80k/output_images/9662.png  \n",
            "  inflating: data_80k/output_images/9663.png  \n",
            "  inflating: data_80k/output_images/9664.png  \n",
            "  inflating: data_80k/output_images/9665.png  \n",
            "  inflating: data_80k/output_images/9666.png  \n",
            "  inflating: data_80k/output_images/9667.png  \n",
            "  inflating: data_80k/output_images/9668.png  \n",
            "  inflating: data_80k/output_images/9669.png  \n",
            "  inflating: data_80k/output_images/967.png  \n",
            "  inflating: data_80k/output_images/9670.png  \n",
            "  inflating: data_80k/output_images/9671.png  \n",
            "  inflating: data_80k/output_images/9672.png  \n",
            "  inflating: data_80k/output_images/9673.png  \n",
            "  inflating: data_80k/output_images/9674.png  \n",
            "  inflating: data_80k/output_images/9675.png  \n",
            "  inflating: data_80k/output_images/9676.png  \n",
            "  inflating: data_80k/output_images/9677.png  \n",
            "  inflating: data_80k/output_images/9678.png  \n",
            "  inflating: data_80k/output_images/9679.png  \n",
            "  inflating: data_80k/output_images/968.png  \n",
            "  inflating: data_80k/output_images/9680.png  \n",
            "  inflating: data_80k/output_images/9681.png  \n",
            "  inflating: data_80k/output_images/9682.png  \n",
            "  inflating: data_80k/output_images/9683.png  \n",
            "  inflating: data_80k/output_images/9684.png  \n",
            "  inflating: data_80k/output_images/9685.png  \n",
            "  inflating: data_80k/output_images/9686.png  \n",
            "  inflating: data_80k/output_images/9687.png  \n",
            "  inflating: data_80k/output_images/9688.png  \n",
            "  inflating: data_80k/output_images/9689.png  \n",
            "  inflating: data_80k/output_images/969.png  \n",
            "  inflating: data_80k/output_images/9690.png  \n",
            "  inflating: data_80k/output_images/9691.png  \n",
            "  inflating: data_80k/output_images/9692.png  \n",
            "  inflating: data_80k/output_images/9693.png  \n",
            "  inflating: data_80k/output_images/9694.png  \n",
            "  inflating: data_80k/output_images/9695.png  \n",
            "  inflating: data_80k/output_images/9696.png  \n",
            "  inflating: data_80k/output_images/9697.png  \n",
            "  inflating: data_80k/output_images/9698.png  \n",
            "  inflating: data_80k/output_images/9699.png  \n",
            "  inflating: data_80k/output_images/97.png  \n",
            "  inflating: data_80k/output_images/970.png  \n",
            "  inflating: data_80k/output_images/9700.png  \n",
            "  inflating: data_80k/output_images/9701.png  \n",
            "  inflating: data_80k/output_images/9702.png  \n",
            "  inflating: data_80k/output_images/9703.png  \n",
            "  inflating: data_80k/output_images/9704.png  \n",
            "  inflating: data_80k/output_images/9705.png  \n",
            "  inflating: data_80k/output_images/9706.png  \n",
            "  inflating: data_80k/output_images/9707.png  \n",
            "  inflating: data_80k/output_images/9708.png  \n",
            "  inflating: data_80k/output_images/9709.png  \n",
            "  inflating: data_80k/output_images/971.png  \n",
            "  inflating: data_80k/output_images/9710.png  \n",
            "  inflating: data_80k/output_images/9711.png  \n",
            "  inflating: data_80k/output_images/9712.png  \n",
            "  inflating: data_80k/output_images/9713.png  \n",
            "  inflating: data_80k/output_images/9714.png  \n",
            "  inflating: data_80k/output_images/9715.png  \n",
            "  inflating: data_80k/output_images/9716.png  \n",
            "  inflating: data_80k/output_images/9717.png  \n",
            "  inflating: data_80k/output_images/9718.png  \n",
            "  inflating: data_80k/output_images/9719.png  \n",
            "  inflating: data_80k/output_images/972.png  \n",
            "  inflating: data_80k/output_images/9720.png  \n",
            "  inflating: data_80k/output_images/9721.png  \n",
            "  inflating: data_80k/output_images/9722.png  \n",
            "  inflating: data_80k/output_images/9723.png  \n",
            "  inflating: data_80k/output_images/9724.png  \n",
            "  inflating: data_80k/output_images/9725.png  \n",
            "  inflating: data_80k/output_images/9726.png  \n",
            "  inflating: data_80k/output_images/9727.png  \n",
            "  inflating: data_80k/output_images/9728.png  \n",
            "  inflating: data_80k/output_images/9729.png  \n",
            "  inflating: data_80k/output_images/973.png  \n",
            "  inflating: data_80k/output_images/9730.png  \n",
            "  inflating: data_80k/output_images/9731.png  \n",
            "  inflating: data_80k/output_images/9732.png  \n",
            "  inflating: data_80k/output_images/9733.png  \n",
            "  inflating: data_80k/output_images/9734.png  \n",
            "  inflating: data_80k/output_images/9735.png  \n",
            "  inflating: data_80k/output_images/9736.png  \n",
            "  inflating: data_80k/output_images/9737.png  \n",
            "  inflating: data_80k/output_images/9738.png  \n",
            "  inflating: data_80k/output_images/9739.png  \n",
            "  inflating: data_80k/output_images/974.png  \n",
            "  inflating: data_80k/output_images/9740.png  \n",
            "  inflating: data_80k/output_images/9741.png  \n",
            "  inflating: data_80k/output_images/9742.png  \n",
            "  inflating: data_80k/output_images/9743.png  \n",
            "  inflating: data_80k/output_images/9744.png  \n",
            "  inflating: data_80k/output_images/9745.png  \n",
            "  inflating: data_80k/output_images/9746.png  \n",
            "  inflating: data_80k/output_images/9747.png  \n",
            "  inflating: data_80k/output_images/9748.png  \n",
            "  inflating: data_80k/output_images/9749.png  \n",
            "  inflating: data_80k/output_images/975.png  \n",
            "  inflating: data_80k/output_images/9750.png  \n",
            "  inflating: data_80k/output_images/9751.png  \n",
            "  inflating: data_80k/output_images/9752.png  \n",
            "  inflating: data_80k/output_images/9753.png  \n",
            "  inflating: data_80k/output_images/9754.png  \n",
            "  inflating: data_80k/output_images/9755.png  \n",
            "  inflating: data_80k/output_images/9756.png  \n",
            "  inflating: data_80k/output_images/9757.png  \n",
            "  inflating: data_80k/output_images/9758.png  \n",
            "  inflating: data_80k/output_images/9759.png  \n",
            "  inflating: data_80k/output_images/976.png  \n",
            "  inflating: data_80k/output_images/9760.png  \n",
            "  inflating: data_80k/output_images/9761.png  \n",
            "  inflating: data_80k/output_images/9762.png  \n",
            "  inflating: data_80k/output_images/9763.png  \n",
            "  inflating: data_80k/output_images/9764.png  \n",
            "  inflating: data_80k/output_images/9765.png  \n",
            "  inflating: data_80k/output_images/9766.png  \n",
            "  inflating: data_80k/output_images/9767.png  \n",
            "  inflating: data_80k/output_images/9768.png  \n",
            "  inflating: data_80k/output_images/9769.png  \n",
            "  inflating: data_80k/output_images/977.png  \n",
            "  inflating: data_80k/output_images/9770.png  \n",
            "  inflating: data_80k/output_images/9771.png  \n",
            "  inflating: data_80k/output_images/9772.png  \n",
            "  inflating: data_80k/output_images/9773.png  \n",
            "  inflating: data_80k/output_images/9774.png  \n",
            "  inflating: data_80k/output_images/9775.png  \n",
            "  inflating: data_80k/output_images/9776.png  \n",
            "  inflating: data_80k/output_images/9777.png  \n",
            "  inflating: data_80k/output_images/9778.png  \n",
            "  inflating: data_80k/output_images/9779.png  \n",
            "  inflating: data_80k/output_images/978.png  \n",
            "  inflating: data_80k/output_images/9780.png  \n",
            "  inflating: data_80k/output_images/9781.png  \n",
            "  inflating: data_80k/output_images/9782.png  \n",
            "  inflating: data_80k/output_images/9783.png  \n",
            "  inflating: data_80k/output_images/9784.png  \n",
            "  inflating: data_80k/output_images/9785.png  \n",
            "  inflating: data_80k/output_images/9786.png  \n",
            "  inflating: data_80k/output_images/9787.png  \n",
            "  inflating: data_80k/output_images/9788.png  \n",
            "  inflating: data_80k/output_images/9789.png  \n",
            "  inflating: data_80k/output_images/979.png  \n",
            "  inflating: data_80k/output_images/9790.png  \n",
            "  inflating: data_80k/output_images/9791.png  \n",
            "  inflating: data_80k/output_images/9792.png  \n",
            "  inflating: data_80k/output_images/9793.png  \n",
            "  inflating: data_80k/output_images/9794.png  \n",
            "  inflating: data_80k/output_images/9795.png  \n",
            "  inflating: data_80k/output_images/9796.png  \n",
            "  inflating: data_80k/output_images/9797.png  \n",
            "  inflating: data_80k/output_images/9798.png  \n",
            "  inflating: data_80k/output_images/9799.png  \n",
            "  inflating: data_80k/output_images/98.png  \n",
            "  inflating: data_80k/output_images/980.png  \n",
            "  inflating: data_80k/output_images/9800.png  \n",
            "  inflating: data_80k/output_images/9801.png  \n",
            "  inflating: data_80k/output_images/9802.png  \n",
            "  inflating: data_80k/output_images/9803.png  \n",
            "  inflating: data_80k/output_images/9804.png  \n",
            "  inflating: data_80k/output_images/9805.png  \n",
            "  inflating: data_80k/output_images/9806.png  \n",
            "  inflating: data_80k/output_images/9807.png  \n",
            "  inflating: data_80k/output_images/9808.png  \n",
            "  inflating: data_80k/output_images/9809.png  \n",
            "  inflating: data_80k/output_images/981.png  \n",
            "  inflating: data_80k/output_images/9810.png  \n",
            "  inflating: data_80k/output_images/9811.png  \n",
            "  inflating: data_80k/output_images/9812.png  \n",
            "  inflating: data_80k/output_images/9813.png  \n",
            "  inflating: data_80k/output_images/9814.png  \n",
            "  inflating: data_80k/output_images/9815.png  \n",
            "  inflating: data_80k/output_images/9816.png  \n",
            "  inflating: data_80k/output_images/9817.png  \n",
            "  inflating: data_80k/output_images/9818.png  \n",
            "  inflating: data_80k/output_images/9819.png  \n",
            "  inflating: data_80k/output_images/982.png  \n",
            "  inflating: data_80k/output_images/9820.png  \n",
            "  inflating: data_80k/output_images/9821.png  \n",
            "  inflating: data_80k/output_images/9822.png  \n",
            "  inflating: data_80k/output_images/9823.png  \n",
            "  inflating: data_80k/output_images/9824.png  \n",
            "  inflating: data_80k/output_images/9825.png  \n",
            "  inflating: data_80k/output_images/9826.png  \n",
            "  inflating: data_80k/output_images/9827.png  \n",
            "  inflating: data_80k/output_images/9828.png  \n",
            "  inflating: data_80k/output_images/9829.png  \n",
            "  inflating: data_80k/output_images/983.png  \n",
            "  inflating: data_80k/output_images/9830.png  \n",
            "  inflating: data_80k/output_images/9831.png  \n",
            "  inflating: data_80k/output_images/9832.png  \n",
            "  inflating: data_80k/output_images/9833.png  \n",
            "  inflating: data_80k/output_images/9834.png  \n",
            "  inflating: data_80k/output_images/9835.png  \n",
            "  inflating: data_80k/output_images/9836.png  \n",
            "  inflating: data_80k/output_images/9837.png  \n",
            "  inflating: data_80k/output_images/9838.png  \n",
            "  inflating: data_80k/output_images/9839.png  \n",
            "  inflating: data_80k/output_images/984.png  \n",
            "  inflating: data_80k/output_images/9840.png  \n",
            "  inflating: data_80k/output_images/9841.png  \n",
            "  inflating: data_80k/output_images/9842.png  \n",
            "  inflating: data_80k/output_images/9843.png  \n",
            "  inflating: data_80k/output_images/9844.png  \n",
            "  inflating: data_80k/output_images/9845.png  \n",
            "  inflating: data_80k/output_images/9846.png  \n",
            "  inflating: data_80k/output_images/9847.png  \n",
            "  inflating: data_80k/output_images/9848.png  \n",
            "  inflating: data_80k/output_images/9849.png  \n",
            "  inflating: data_80k/output_images/985.png  \n",
            "  inflating: data_80k/output_images/9850.png  \n",
            "  inflating: data_80k/output_images/9851.png  \n",
            "  inflating: data_80k/output_images/9852.png  \n",
            "  inflating: data_80k/output_images/9853.png  \n",
            "  inflating: data_80k/output_images/9854.png  \n",
            "  inflating: data_80k/output_images/9855.png  \n",
            "  inflating: data_80k/output_images/9856.png  \n",
            "  inflating: data_80k/output_images/9857.png  \n",
            "  inflating: data_80k/output_images/9858.png  \n",
            "  inflating: data_80k/output_images/9859.png  \n",
            "  inflating: data_80k/output_images/986.png  \n",
            "  inflating: data_80k/output_images/9860.png  \n",
            "  inflating: data_80k/output_images/9861.png  \n",
            "  inflating: data_80k/output_images/9862.png  \n",
            "  inflating: data_80k/output_images/9863.png  \n",
            "  inflating: data_80k/output_images/9864.png  \n",
            "  inflating: data_80k/output_images/9865.png  \n",
            "  inflating: data_80k/output_images/9866.png  \n",
            "  inflating: data_80k/output_images/9867.png  \n",
            "  inflating: data_80k/output_images/9868.png  \n",
            "  inflating: data_80k/output_images/9869.png  \n",
            "  inflating: data_80k/output_images/987.png  \n",
            "  inflating: data_80k/output_images/9870.png  \n",
            "  inflating: data_80k/output_images/9871.png  \n",
            "  inflating: data_80k/output_images/9872.png  \n",
            "  inflating: data_80k/output_images/9873.png  \n",
            "  inflating: data_80k/output_images/9874.png  \n",
            "  inflating: data_80k/output_images/9875.png  \n",
            "  inflating: data_80k/output_images/9876.png  \n",
            "  inflating: data_80k/output_images/9877.png  \n",
            "  inflating: data_80k/output_images/9878.png  \n",
            "  inflating: data_80k/output_images/9879.png  \n",
            "  inflating: data_80k/output_images/988.png  \n",
            "  inflating: data_80k/output_images/9880.png  \n",
            "  inflating: data_80k/output_images/9881.png  \n",
            "  inflating: data_80k/output_images/9882.png  \n",
            "  inflating: data_80k/output_images/9883.png  \n",
            "  inflating: data_80k/output_images/9884.png  \n",
            "  inflating: data_80k/output_images/9885.png  \n",
            "  inflating: data_80k/output_images/9886.png  \n",
            "  inflating: data_80k/output_images/9887.png  \n",
            "  inflating: data_80k/output_images/9888.png  \n",
            "  inflating: data_80k/output_images/9889.png  \n",
            "  inflating: data_80k/output_images/989.png  \n",
            "  inflating: data_80k/output_images/9890.png  \n",
            "  inflating: data_80k/output_images/9891.png  \n",
            "  inflating: data_80k/output_images/9892.png  \n",
            "  inflating: data_80k/output_images/9893.png  \n",
            "  inflating: data_80k/output_images/9894.png  \n",
            "  inflating: data_80k/output_images/9895.png  \n",
            "  inflating: data_80k/output_images/9896.png  \n",
            "  inflating: data_80k/output_images/9897.png  \n",
            "  inflating: data_80k/output_images/9898.png  \n",
            "  inflating: data_80k/output_images/9899.png  \n",
            "  inflating: data_80k/output_images/99.png  \n",
            "  inflating: data_80k/output_images/990.png  \n",
            "  inflating: data_80k/output_images/9900.png  \n",
            "  inflating: data_80k/output_images/9901.png  \n",
            "  inflating: data_80k/output_images/9902.png  \n",
            "  inflating: data_80k/output_images/9903.png  \n",
            "  inflating: data_80k/output_images/9904.png  \n",
            "  inflating: data_80k/output_images/9905.png  \n",
            "  inflating: data_80k/output_images/9906.png  \n",
            "  inflating: data_80k/output_images/9907.png  \n",
            "  inflating: data_80k/output_images/9908.png  \n",
            "  inflating: data_80k/output_images/9909.png  \n",
            "  inflating: data_80k/output_images/991.png  \n",
            "  inflating: data_80k/output_images/9910.png  \n",
            "  inflating: data_80k/output_images/9911.png  \n",
            "  inflating: data_80k/output_images/9912.png  \n",
            "  inflating: data_80k/output_images/9913.png  \n",
            "  inflating: data_80k/output_images/9914.png  \n",
            "  inflating: data_80k/output_images/9915.png  \n",
            "  inflating: data_80k/output_images/9916.png  \n",
            "  inflating: data_80k/output_images/9917.png  \n",
            "  inflating: data_80k/output_images/9918.png  \n",
            "  inflating: data_80k/output_images/9919.png  \n",
            "  inflating: data_80k/output_images/992.png  \n",
            "  inflating: data_80k/output_images/9920.png  \n",
            "  inflating: data_80k/output_images/9921.png  \n",
            "  inflating: data_80k/output_images/9922.png  \n",
            "  inflating: data_80k/output_images/9923.png  \n",
            "  inflating: data_80k/output_images/9924.png  \n",
            "  inflating: data_80k/output_images/9925.png  \n",
            "  inflating: data_80k/output_images/9926.png  \n",
            "  inflating: data_80k/output_images/9927.png  \n",
            "  inflating: data_80k/output_images/9928.png  \n",
            "  inflating: data_80k/output_images/9929.png  \n",
            "  inflating: data_80k/output_images/993.png  \n",
            "  inflating: data_80k/output_images/9930.png  \n",
            "  inflating: data_80k/output_images/9931.png  \n",
            "  inflating: data_80k/output_images/9932.png  \n",
            "  inflating: data_80k/output_images/9933.png  \n",
            "  inflating: data_80k/output_images/9934.png  \n",
            "  inflating: data_80k/output_images/9935.png  \n",
            "  inflating: data_80k/output_images/9936.png  \n",
            "  inflating: data_80k/output_images/9937.png  \n",
            "  inflating: data_80k/output_images/9938.png  \n",
            "  inflating: data_80k/output_images/9939.png  \n",
            "  inflating: data_80k/output_images/994.png  \n",
            "  inflating: data_80k/output_images/9940.png  \n",
            "  inflating: data_80k/output_images/9941.png  \n",
            "  inflating: data_80k/output_images/9942.png  \n",
            "  inflating: data_80k/output_images/9943.png  \n",
            "  inflating: data_80k/output_images/9944.png  \n",
            "  inflating: data_80k/output_images/9945.png  \n",
            "  inflating: data_80k/output_images/9946.png  \n",
            "  inflating: data_80k/output_images/9947.png  \n",
            "  inflating: data_80k/output_images/9948.png  \n",
            "  inflating: data_80k/output_images/9949.png  \n",
            "  inflating: data_80k/output_images/995.png  \n",
            "  inflating: data_80k/output_images/9950.png  \n",
            "  inflating: data_80k/output_images/9951.png  \n",
            "  inflating: data_80k/output_images/9952.png  \n",
            "  inflating: data_80k/output_images/9953.png  \n",
            "  inflating: data_80k/output_images/9954.png  \n",
            "  inflating: data_80k/output_images/9955.png  \n",
            "  inflating: data_80k/output_images/9956.png  \n",
            "  inflating: data_80k/output_images/9957.png  \n",
            "  inflating: data_80k/output_images/9958.png  \n",
            "  inflating: data_80k/output_images/9959.png  \n",
            "  inflating: data_80k/output_images/996.png  \n",
            "  inflating: data_80k/output_images/9960.png  \n",
            "  inflating: data_80k/output_images/9961.png  \n",
            "  inflating: data_80k/output_images/9962.png  \n",
            "  inflating: data_80k/output_images/9963.png  \n",
            "  inflating: data_80k/output_images/9964.png  \n",
            "  inflating: data_80k/output_images/9965.png  \n",
            "  inflating: data_80k/output_images/9966.png  \n",
            "  inflating: data_80k/output_images/9967.png  \n",
            "  inflating: data_80k/output_images/9968.png  \n",
            "  inflating: data_80k/output_images/9969.png  \n",
            "  inflating: data_80k/output_images/997.png  \n",
            "  inflating: data_80k/output_images/9970.png  \n",
            "  inflating: data_80k/output_images/9971.png  \n",
            "  inflating: data_80k/output_images/9972.png  \n",
            "  inflating: data_80k/output_images/9973.png  \n",
            "  inflating: data_80k/output_images/9974.png  \n",
            "  inflating: data_80k/output_images/9975.png  \n",
            "  inflating: data_80k/output_images/9976.png  \n",
            "  inflating: data_80k/output_images/9977.png  \n",
            "  inflating: data_80k/output_images/9978.png  \n",
            "  inflating: data_80k/output_images/9979.png  \n",
            "  inflating: data_80k/output_images/998.png  \n",
            "  inflating: data_80k/output_images/9980.png  \n",
            "  inflating: data_80k/output_images/9981.png  \n",
            "  inflating: data_80k/output_images/9982.png  \n",
            "  inflating: data_80k/output_images/9983.png  \n",
            "  inflating: data_80k/output_images/9984.png  \n",
            "  inflating: data_80k/output_images/9985.png  \n",
            "  inflating: data_80k/output_images/9986.png  \n",
            "  inflating: data_80k/output_images/9987.png  \n",
            "  inflating: data_80k/output_images/9988.png  \n",
            "  inflating: data_80k/output_images/9989.png  \n",
            "  inflating: data_80k/output_images/999.png  \n",
            "  inflating: data_80k/output_images/9990.png  \n",
            "  inflating: data_80k/output_images/9991.png  \n",
            "  inflating: data_80k/output_images/9992.png  \n",
            "  inflating: data_80k/output_images/9993.png  \n",
            "  inflating: data_80k/output_images/9994.png  \n",
            "  inflating: data_80k/output_images/9995.png  \n",
            "  inflating: data_80k/output_images/9996.png  \n",
            "  inflating: data_80k/output_images/9997.png  \n",
            "  inflating: data_80k/output_images/9998.png  \n",
            "  inflating: data_80k/output_images/9999.png  \n"
          ]
        }
      ],
      "source": [
        "! unzip hindi-ocr-synthetic-line-image-text-pair.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qJwzqPsQNP_A",
        "outputId": "29a65ab7-7084-4802-9003-3a8a9b728eb6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  image_file                                               text  font_size  \\\n",
            "0      1.png    गर्भनिरोध के लिए महिलाएं क्यों कराती हैं नसबंदी         51   \n",
            "1      2.png  'मस्‍ज‍िद ख़ुदा का घर है तो यह ईमान वाली स्‍त्...         31   \n",
            "2      3.png  नज़रिया: गोरखपुर, नागपुर और दिल्ली के त्रिकोण ...         39   \n",
            "3      4.png  अनुच्छेद 370 के बाद क्या हो सकता है मोदी सरकार...         35   \n",
            "4      5.png                 इराक़ में तीन अमरीकी सैनिक मारे गए         58   \n",
            "\n",
            "                font_file  word_count  \n",
            "0    Lohit-Devanagari.ttf           8  \n",
            "1    Lohit-Devanagari.ttf          17  \n",
            "2        Sura-Regular.ttf          11  \n",
            "3    arial-unicode-ms.ttf          13  \n",
            "4  NotoSansDevanagari.ttf           7  \n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from PIL import Image\n",
        "\n",
        "# Load the CSV file that contains the image references and the corresponding alternate text\n",
        "csv_file_path = '/content/data_80k/data.csv'\n",
        "df = pd.read_csv(csv_file_path)\n",
        "\n",
        "# Preview the dataset\n",
        "print(df.head())\n",
        "\n",
        "# Assuming the images are stored in a folder\n",
        "image_folder_path = '/content/data_80k/output_images'\n",
        "\n",
        "# Function to load an image by reference\n",
        "def load_image(image_name):\n",
        "    image_path = os.path.join(image_folder_path, image_name)\n",
        "    img = Image.open(image_path)\n",
        "    return img\n",
        "\n",
        "# Example: Load the first image from 'image_file' column\n",
        "sample_image_name = df['image_file'][0]\n",
        "img = load_image(sample_image_name)\n",
        "img.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ChKzQs7qOzW7",
        "outputId": "a53b4633-4f38-455d-8fac-26517619b57d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
            "Token is valid (permission: fineGrained).\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n"
          ]
        }
      ],
      "source": [
        "from huggingface_hub import login\n",
        "\n",
        "# Your Huggingface token\n",
        "huggingface_token = \"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\"\n",
        "\n",
        "# Log in to Huggingface\n",
        "login(huggingface_token)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AiYXeUw4bFpI",
        "outputId": "b0bba5ed-3599-47b1-8b8e-904b41896492"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 80000 entries, 0 to 79999\n",
            "Data columns (total 5 columns):\n",
            " #   Column      Non-Null Count  Dtype \n",
            "---  ------      --------------  ----- \n",
            " 0   image_file  80000 non-null  object\n",
            " 1   text        80000 non-null  object\n",
            " 2   font_size   80000 non-null  int64 \n",
            " 3   font_file   80000 non-null  object\n",
            " 4   word_count  80000 non-null  int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 3.1+ MB\n",
            "None\n",
            "0      गर्भनिरोध के लिए महिलाएं क्यों कराती हैं नसबंदी\n",
            "1    'मस्‍ज‍िद ख़ुदा का घर है तो यह ईमान वाली स्‍त्...\n",
            "2    नज़रिया: गोरखपुर, नागपुर और दिल्ली के त्रिकोण ...\n",
            "3    अनुच्छेद 370 के बाद क्या हो सकता है मोदी सरकार...\n",
            "4                   इराक़ में तीन अमरीकी सैनिक मारे गए\n",
            "Name: text, dtype: object\n"
          ]
        }
      ],
      "source": [
        "print(df.info())  # To check the structure and types of columns\n",
        "print(df['text'].head())  # To check the actual content in the 'text' column\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_9yp6HGbWlH",
        "outputId": "7ae40b56-ffce-4579-dd0d-944aafc2f2fa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['image_file', 'text', 'font_size', 'font_file', 'word_count'],\n",
            "    num_rows: 80000\n",
            "})\n",
            "{'image_file': '1.png', 'text': 'गर्भनिरोध के लिए महिलाएं क्यों कराती हैं नसबंदी', 'font_size': 51, 'font_file': 'Lohit-Devanagari.ttf', 'word_count': 8}\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "\n",
        "dataset = Dataset.from_pandas(df)\n",
        "print(dataset)  # To see the structure of the dataset\n",
        "print(dataset[0])  # To check the first entry in the dataset\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "HKd9tkVibjcC"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoProcessor, AutoTokenizer\n",
        "\n",
        "# Initialize processor and tokenizer\n",
        "processor = AutoProcessor.from_pretrained(\"ucaslcl/GOT-OCR2_0\", trust_remote_code=True, token=huggingface_token)\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"ucaslcl/GOT-OCR2_0\", trust_remote_code=True, token=huggingface_token)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uKRYQsInOJSV",
        "outputId": "29be7357-0010-4dc2-968c-b8b0e2b66933"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1525: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from transformers import Seq2SeqTrainingArguments\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=\"./got_ocr_results\",\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    num_train_epochs=3,\n",
        "    logging_dir=\"./logs\",\n",
        "    logging_steps=10,\n",
        "    evaluation_strategy=\"steps\",\n",
        "    eval_steps=500,\n",
        "    save_steps=500,\n",
        "    save_total_limit=2,\n",
        "    predict_with_generate=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "48b86b33d94d42e4b37a35f61123cb33",
            "5d6657aa5c53421da794d75329c2689a",
            "c1ea9cf07132496099074290d13e6d3f",
            "7a4d598fbfc54470bae58952c2e46bae",
            "c22c8020217246c5aacace9309c08bda",
            "d948dca9c33943f9a0d855d2b39243c7",
            "749aed1d4ab94234baf15c709472558f",
            "32546a6fa4c441b69cc13c8339695fc3",
            "bbc8d7af5aba4c0cba31d1a91da9cf99",
            "c6e66571407348118292d710806980f1",
            "855d32c18b9d4284b1259e70b0b8da78"
          ]
        },
        "id": "u7SdSf5wOe8V",
        "outputId": "ae11c2e9-91bd-4a3c-8dba-b2479b636297"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/80000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "48b86b33d94d42e4b37a35f61123cb33"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6793.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>भारत की जेल में क़ैद नेपाली नागरिक को 40 साल बाद मिली रिहाई\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146101,  31411,    108,  79238,  47809,  43647,\n",
            "          14925,    250,  54784,    110,  91217,  54784,    224,  47809,   5502,\n",
            "            120,  12619,    230, 145256,  14925,    101,  54784,    103,  31411,\n",
            "            110,  43647,  14925,    101,  31411,    245,  44179,  42311,    243,\n",
            "          47809,  54575,    220,     19,     15,  68158,  31411,    110,  14925,\n",
            "            105,  31411,     99,  91217,  42311,    110,  43647,  14925,    108,\n",
            "          42311,    117,  31411,    230, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6794.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कामरान ऑन बाइक: साइकिल से 50 हज़ार किलोमीटर की यात्रा करने वाले कामरान अली\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  31411,    106,  44179,  31411,    101,\n",
            "          14925,    239,  60096,  14925,    105,  31411,    229,  64704,     25,\n",
            "          68158,  31411,    229,  64704,  42311,    110,  68158,  34370,    220,\n",
            "             20,     15,  84310, 146114,   5502,    120,  23868,  44179,  47809,\n",
            "          42311,    110,  54575,  87244,  43647, 145769,  44179,  47809,  43647,\n",
            "          14925,    107,  31411,     97,  85033,  23868,  47809,  44179,  60096,\n",
            "          34370,  14925,    113,  31411,    110,  34370,  47809,  31411,    106,\n",
            "          44179,  31411,    101,  14925,    227,  91811,  43647, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6795.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>इसराइल से भारत का अवाक्स सिस्टम सौदा चीन के लिए कितना ख़तरा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146575,  78368,  44179,  31411,    229,  91811,\n",
            "          68158,  34370,  14925,    255,  31411,    108,  79238,  47809,  23868,\n",
            "          14925,    227, 145535,  31411,    243,  30484,    116,  68158,  42311,\n",
            "            116,  30484,    253,  87244,  68158,  12619,    234, 145256,  23868,\n",
            "          14925,    248,  43647,  60096,  47809,  34370,  14925,    110,  42311,\n",
            "            237,  47809,  42311,     97,  60096,  23868,  14925,    244,   5502,\n",
            "            120,  79238,  44179,  23868, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6796.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>उद्धव ठाकरे ने एक साल में महाराष्ट्र में क्या किया कमाल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147181, 145256,  30484,    100, 145535,  14925,\n",
            "            254,  31411,    243,  44179,  34370,  14925,    101,  34370,  14925,\n",
            "            237,  64704,  68158,  31411,    110,  91217,  54784,    224,  91217,\n",
            "          93948,  31411,    108,  31411,    115,  30484,    253,  85033,  91217,\n",
            "          54784,    224,  47809,  30484,    107,  23868,  47809,  42311,    107,\n",
            "          23868,  47809,  87244,  31411,    110, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6797.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>भारत से विवाद के बाद पाकिस्तान में क्या चलता है?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146101,  31411,    108,  79238,  68158,  34370,\n",
            "          14925,    113,  42311,    113,  31411,     99,  47809,  34370,  14925,\n",
            "            105,  31411,     99,  83636,  31411,    243,  42311,    116,  30484,\n",
            "             97,  31411,    101,  91217,  54784,    224,  47809,  30484,    107,\n",
            "          23868,  14925,    248,  91811,  79238,  23868,  84310,  12619,    230,\n",
            "             30, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6798.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जेएनयू: फ़ीस विवाद पर छात्र पीछे हटने को तैयार क्यों नहीं?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,  54784,    237,  60096, 145420,  12619,\n",
            "            224,     25,  14925,    104,   5502,    120,  43647,  78368,  14925,\n",
            "            113,  42311,    113,  31411,     99,  83636,  44179,  14925,    249,\n",
            "          31411,     97,  85033,  83636,  43647, 147877,  34370,  84310, 145769,\n",
            "          60096,  34370,  47809,  54575,  14925,     97,  12619,    230, 145420,\n",
            "          31411,    108,  47809,  30484,    107,  54575,  72314,  14925,    101,\n",
            "          93948,  43647,  72314,     30, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6799.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ऐश्वर्या राय बच्चन और बेटी आराध्या भी कोरोना पॉज़िटिव\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 149524, 145966,  30484,    113,  44179,  30484,\n",
            "            107,  23868,  14925,    108,  31411,    107,  14925,    105, 146113,\n",
            "          30484,    248,  60096,  14925,    242,  44179,  14925,    105,  54784,\n",
            "            253,  43647,  14925,    228,  44179,  31411,    100,  30484,    107,\n",
            "          23868,  14925,    255,  43647,  47809,  54575,  44179,  54575,  60096,\n",
            "          23868,  83636,  12619,    231, 146114,   5502,    120,  38851, 145769,\n",
            "          42311,    113, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6800.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जूते फेंकने की 'मास्टरक्लास'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,  12619,    224,  79238,  34370,  14925,\n",
            "            104,  54784,    224,  64704,  60096,  34370,  47809,  43647,    364,\n",
            "          87244,  31411,    116,  30484,    253,  44179,  64704,  30484,    110,\n",
            "          31411,    116,      6, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6801.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या हो सकता है तीसरा विश्व युद्ध?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,  84310,  54575,\n",
            "          68158,  64704,  79238,  23868,  84310,  12619,    230,  14925,     97,\n",
            "          43647,  78368,  44179,  23868,  14925,    113,  42311,    114,  30484,\n",
            "            113,  14925,    107,  72653, 145256,  30484,    100,     30, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6802.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'ऐसे ही चलता है भारत में राहत का काम'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 149524,  78368,  34370,  84310,  43647,  14925,\n",
            "            248,  91811,  79238,  23868,  84310,  12619,    230,  14925,    255,\n",
            "          31411,    108,  79238,  91217,  54784,    224,  14925,    108,  31411,\n",
            "            117,  79238,  47809,  23868,  47809,  31411,    106,      6, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6803.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>10 साल की बच्ची ने क्यों बनाया ख़ुद के रेप का वीडियो?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,     16,     15,  68158,  31411,    110,  47809,\n",
            "          43647,  14925,    105, 146113,  30484,    248,  43647,  14925,    101,\n",
            "          34370,  47809,  30484,    107,  54575,  72314,  14925,    105,  60096,\n",
            "          31411,    107,  23868,  14925,    244,   5502,    120,  72653, 145256,\n",
            "          47809,  34370,  14925,    108,  54784,    103,  47809,  23868,  14925,\n",
            "            113,  43647, 146187,  42311,    107,  54575,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6804.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>इस दर्द की दवा है क्या?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146575,  78368,  14925,     99,  44179,  30484,\n",
            "             99,  47809,  43647,  14925,     99, 145535,  23868,  84310,  12619,\n",
            "            230,  47809,  30484,    107,  23868,     30, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6805.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>आईपीएल: वाटसन की 'अविश्वसनीय' बल्लेबाज़ी और राजस्थान की हार\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146399, 147131,  86162,  43647, 146049,  91811,\n",
            "             25,  14925,    113,  31411,    253,  78368,  60096,  47809,  43647,\n",
            "            364, 146378, 145535,  42311,    114,  30484,    113,  78368,  60096,\n",
            "          43647, 145420,      6,  14925,    105,  91811,  30484,    110,  54784,\n",
            "            105,  31411,    250,   5502,    120,  43647,  14925,    242,  44179,\n",
            "          14925,    108,  31411,    250,  78368,  30484,     98,  31411,    101,\n",
            "          47809,  43647,  84310,  31411,    108, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6806.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>प्रकाश झा को 100 करोड़ कमाने की चाहत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  85033,  64704,  31411,    114,  14925,\n",
            "            251,  23868,  47809,  54575,    220,     16,     15,     15,  47809,\n",
            "          44179,  54575, 146187,   5502,    120,  47809,  87244,  31411,    101,\n",
            "          34370,  47809,  43647,  14925,    248,  31411,    117,  79238, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6807.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जवाहरलाल नेहरू यूनिवर्सिटी क्यों और कितना उबल रही है?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114, 145535,  31411,    117,  44179,  91811,\n",
            "          31411,    110,  14925,    101,  54784,    117,  44179,  12619,    224,\n",
            "          14925,    107,  12619,    224,  60096,  42311,    113,  44179,  30484,\n",
            "            116,  42311,    253,  43647,  47809,  30484,    107,  54575,  72314,\n",
            "          14925,    242,  44179,  47809,  42311,     97,  60096,  23868,  14925,\n",
            "            231, 145799,  91811,  14925,    108,  93948,  43647,  84310,  12619,\n",
            "            230,     30, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6808.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सुरक्षा परिषद की बैठक बुलाएँ: इसराइल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  72653,  44179,  64704,  30484,    115,\n",
            "          23868,  83636,  44179,  42311,    115, 145256,  47809,  43647,  14925,\n",
            "            105,  12619,    230, 146826,  64704,  14925,    105,  72653,  91811,\n",
            "          31411,    237,   5502,    223,     25,  14925,    229,  78368,  44179,\n",
            "          31411,    229,  91811, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6809.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>बूचड़खाने में जानवरों के क़त्ल को लेकर यूरोप में विवाद\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145799,  12619,    224, 146113, 146187,   5502,\n",
            "            120, 146800,  31411,    101,  34370,  91217,  54784,    224,  14925,\n",
            "            250,  31411,    101, 145535,  44179,  54575,  72314,  47809,  34370,\n",
            "          47809,   5502,    120,  79238,  30484,    110,  47809,  54575,  14925,\n",
            "            110,  54784,    243,  44179,  14925,    107,  12619,    224,  44179,\n",
            "          54575,  86162,  91217,  54784,    224,  14925,    113,  42311,    113,\n",
            "          31411,     99, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6810.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>एशियाई शेयर बाज़ारों में सुधार जारी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146049, 145966,  42311,    107,  31411,    230,\n",
            "          14925,    114,  54784,    107,  44179,  14925,    105,  31411,    250,\n",
            "           5502,    120,  23868,  44179,  54575,  72314,  91217,  54784,    224,\n",
            "          68158,  72653, 146821,  31411,    108,  14925,    250,  31411,    108,\n",
            "          43647, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6811.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सनाउल्ला की हालत गंभीर, मौत की ख़बर ग़लत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  60096,  31411,    231,  91811,  30484,\n",
            "            110,  23868,  47809,  43647,  84310,  31411,    110,  79238,  14925,\n",
            "            245,  72314, 146101,  43647,  44179,     11,  91217,  12619,    234,\n",
            "          79238,  47809,  43647,  14925,    244,   5502,    120, 145799,  44179,\n",
            "          14925,    245,   5502,    120,  91811,  79238, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6812.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>उर्दू प्रेस रिव्यू: 'क्या पाकिस्तान के अर्दोआन साबित होंगे इमरान ख़ान?'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147181,  44179,  30484,     99,  12619,    224,\n",
            "          83636,  85033,  54784,    116,  14925,    108,  42311,    113,  30484,\n",
            "            107,  12619,    224,     25,    364,  64704,  30484,    107,  23868,\n",
            "          83636,  31411,    243,  42311,    116,  30484,     97,  31411,    101,\n",
            "          47809,  34370,  14925,    227,  44179,  30484,     99,  54575, 146399,\n",
            "          60096,  68158,  31411,    105,  42311,     97,  84310,  54575,  72314,\n",
            "         145959,  34370,  14925,    229,  87244,  44179,  31411,    101,  14925,\n",
            "            244,   5502,    120,  23868,  60096,  20224, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6813.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या मायावती के कहने पर दलित बदलेंगे धर्म?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,  91217,  31411,\n",
            "            107,  31411,    113,  79238,  43647,  47809,  34370,  47809,  93948,\n",
            "          60096,  34370,  83636,  44179,  14925,     99,  91811,  42311,     97,\n",
            "          14925,    105, 145256,  91811,  54784,    224, 145959,  34370,  14925,\n",
            "            100,  44179,  30484,    106,     30, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6814.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'सेना हटाने की बात इस्लाम की जीत'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592,  78368,  54784,    101,  23868,  84310, 145769,\n",
            "          31411,    101,  34370,  47809,  43647,  14925,    105,  31411,     97,\n",
            "          14925,    229,  78368,  30484,    110,  31411,    106,  47809,  43647,\n",
            "          14925,    250,  43647,  79238,      6, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6815.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सेना प्रमुख जनरल नरवणे ने बताई एलएसी की स्थिति\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  54784,    101,  23868,  83636,  85033,\n",
            "          87244,  72653, 146800,  14925,    250,  60096,  44179,  91811,  14925,\n",
            "            101,  44179, 145535, 146548,  34370,  14925,    101,  34370,  14925,\n",
            "            105,  79238,  31411,    230,  14925,    237,  91811, 146049,  78368,\n",
            "          43647,  47809,  43647,  68158,  30484,     98,  42311,     97,  38851,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6816.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>महाराष्ट्र में राष्ट्रपति शासन लागू\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  93948,  31411,    108,  31411,    115,\n",
            "          30484,    253,  85033,  91217,  54784,    224,  14925,    108,  31411,\n",
            "            115,  30484,    253,  85033,  86162,  79238,  38851,  14925,    114,\n",
            "          31411,    116,  60096,  14925,    110,  31411,    245,  12619,    224,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6817.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>भारत-चीन तनाव: अमरीका क्या मुश्किल वक़्त में भारत से मुंह मोड़ लेता है?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146101,  31411,    108,  79238,     12, 146113,\n",
            "          43647,  60096,  14925,     97,  60096,  31411,    113,     25,  14925,\n",
            "            227,  87244,  44179,  43647,  64704,  23868,  47809,  30484,    107,\n",
            "          23868,  91217,  72653, 145966,  30484,    243,  42311,    110,  14925,\n",
            "            113,  64704,   5502,    120,  29607,  79238,  91217,  54784,    224,\n",
            "          14925,    255,  31411,    108,  79238,  68158,  34370,  91217,  72653,\n",
            "          72314,  93948,  91217,  54575, 146187,   5502,    120,  14925,    110,\n",
            "          54784,     97,  23868,  84310,  12619,    230,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6818.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'फ़ेक न्यूज़ से लड़ना है तो रियल न्यूज़ को वायरल बनाना होगा'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 146934,   5502,    120,  34370,  64704,  14925,\n",
            "            101,  30484,    107,  12619,    224, 146114,   5502,    120,  68158,\n",
            "          34370,  14925,    110, 146187,   5502,    120,  60096,  23868,  84310,\n",
            "          12619,    230,  14925,     97,  54575,  14925,    108,  42311,    107,\n",
            "          91811,  14925,    101,  30484,    107,  12619,    224, 146114,   5502,\n",
            "            120,  47809,  54575,  14925,    113,  31411,    107,  44179,  91811,\n",
            "          14925,    105,  60096,  31411,    101,  23868,  84310,  54575, 145959,\n",
            "          23868,      6, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6819.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>उत्तराखंड: बारिश में 12 की मौत, पीएम ने जताया दुख\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147181,  79238,  30484,     97,  44179,  31411,\n",
            "            244,  72314, 146187,     25,  14925,    105,  31411,    108,  42311,\n",
            "            114,  91217,  54784,    224,    220,     16,     17,  47809,  43647,\n",
            "          91217,  12619,    234,  79238,     11,  83636,  43647, 146049,  87244,\n",
            "          14925,    101,  34370,  14925,    250,  79238,  31411,    107,  23868,\n",
            "          14925,     99,  72653, 146800, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6820.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>दिल्ली के सिग्नेचर ब्रिज़ से गिरकर दो मेडिकल छात्रों की मौत: प्रेस रिव्यू\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145256,  42311,    110,  30484,    110,  43647,\n",
            "          47809,  34370,  68158,  42311,    245,  30484,    101,  54784,    248,\n",
            "          44179,  14925,    105,  85033,  42311,    250,   5502,    120,  68158,\n",
            "          34370,  14925,    245,  42311,    108,  64704,  44179,  14925,     99,\n",
            "          54575,  91217,  54784,     94,  42311,    243,  91811,  14925,    249,\n",
            "          31411,     97,  85033,  54575,  72314,  47809,  43647,  91217,  12619,\n",
            "            234,  79238,     25,  83636,  85033,  54784,    116,  14925,    108,\n",
            "          42311,    113,  30484,    107,  12619,    224, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6821.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अनुच्छेद 370: कश्मीर की बेहतरी के सरकारी दावों में कितना दम: नज़रिया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  60096,  72653, 146113,  30484,    249,\n",
            "          54784,     99,    220,     18,     22,     15,     25,  47809, 145966,\n",
            "          30484,    106,  43647,  44179,  47809,  43647,  14925,    105,  54784,\n",
            "            117,  79238,  44179,  43647,  47809,  34370,  68158,  44179,  64704,\n",
            "          31411,    108,  43647,  14925,     99,  31411,    113,  54575,  72314,\n",
            "          91217,  54784,    224,  47809,  42311,     97,  60096,  23868,  14925,\n",
            "             99,  87244,     25,  14925,    101, 146114,   5502,    120,  44179,\n",
            "          42311,    107,  23868, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6822.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>शिवरात्रि पर नेपाल के पशुपतिनाथ मंदिर पहुंचने वाली भारत की साध्वी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145966,  42311,    113,  44179,  31411,     97,\n",
            "          85033,  38851,  83636,  44179,  14925,    101,  54784,    103,  31411,\n",
            "            110,  47809,  34370,  83636, 145966,  72653,  86162,  79238,  42311,\n",
            "            101,  31411,     98,  91217,  72314, 145256,  42311,    108,  83636,\n",
            "          93948,  72653,  72314, 146113,  60096,  34370,  14925,    113,  31411,\n",
            "            110,  43647,  14925,    255,  31411,    108,  79238,  47809,  43647,\n",
            "          68158,  31411,    100,  30484,    113,  43647, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6823.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>लोकसभा चुनाव 2019: उत्तर प्रदेश की कौन सी सीटें करेंगी फ़ैसला\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  91811,  54575,  64704,  78368, 146101,  23868,\n",
            "          14925,    248,  72653,  60096,  31411,    113,    220,     17,     15,\n",
            "             16,     24,     25,  14925,    231,  79238,  30484,     97,  44179,\n",
            "          83636,  85033, 145256,  54784,    114,  47809,  43647,  47809,  12619,\n",
            "            234,  60096,  68158,  43647,  68158,  43647, 145769,  54784,    224,\n",
            "          47809,  44179,  54784,    224, 145959,  43647,  14925,    104,   5502,\n",
            "            120,  12619,    230,  78368,  91811,  23868, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6824.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>बिहार तोड़ेगा 'अबकी बार मोदी सरकार' का सपना?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145799,  42311,    117,  31411,    108,  14925,\n",
            "             97,  54575, 146187,   5502,    120,  34370, 145959,  23868,    364,\n",
            "         146378, 145799,  64704,  43647,  14925,    105,  31411,    108,  91217,\n",
            "          54575, 145256,  43647,  68158,  44179,  64704,  31411,    108,      6,\n",
            "          47809,  23868,  68158,  86162,  60096,  23868,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6825.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जेरेमी कोबिन लेबर पार्टी के नए नेता\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,  34370,  44179,  54784,    106,  43647,\n",
            "          47809,  54575, 145799,  42311,    101,  14925,    110,  54784,    105,\n",
            "          44179,  83636,  31411,    108,  30484,    253,  43647,  47809,  34370,\n",
            "          14925,    101, 146049,  14925,    101,  54784,     97,  23868, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6826.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>मनोज बाजपेयी को मीडिया से क्या है शिकायत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  60096,  54575, 146114,  14925,    105,\n",
            "          31411,    250,  86162,  54784,    107,  43647,  47809,  54575,  91217,\n",
            "          43647, 146187,  42311,    107,  23868,  68158,  34370,  47809,  30484,\n",
            "            107,  23868,  84310,  12619,    230,  14925,    114,  42311,    243,\n",
            "          31411,    107,  79238, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6827.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अलीगढ़ मुस्लिम यूनिवर्सिटी में पाकिस्तान ज़िंदाबाद के नारों का क्या है सच?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  91811,  43647, 145959, 149269,   5502,\n",
            "            120,  91217,  72653,  78368,  30484,    110,  42311,    106,  14925,\n",
            "            107,  12619,    224,  60096,  42311,    113,  44179,  30484,    116,\n",
            "          42311,    253,  43647,  91217,  54784,    224,  83636,  31411,    243,\n",
            "          42311,    116,  30484,     97,  31411,    101,  14925,    250,   5502,\n",
            "            120,  42311,    224, 145256,  31411,    105,  31411,     99,  47809,\n",
            "          34370,  14925,    101,  31411,    108,  54575,  72314,  47809,  23868,\n",
            "          47809,  30484,    107,  23868,  84310,  12619,    230,  68158, 146113,\n",
            "             30, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6828.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'कश्मीर में सेना की भूमिका को कम करने की सख़्त ज़रूरत'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592,  64704, 145966,  30484,    106,  43647,  44179,\n",
            "          91217,  54784,    224,  68158,  54784,    101,  23868,  47809,  43647,\n",
            "          14925,    255,  12619,    224,  87244,  42311,    243,  23868,  47809,\n",
            "          54575,  47809,  87244,  47809,  44179,  60096,  34370,  47809,  43647,\n",
            "          68158, 146800,   5502,    120,  29607,  79238,  14925,    250,   5502,\n",
            "            120,  44179,  12619,    224,  44179,  79238,      6, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6829.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>फ़ेसबुक चीफ का 'डिज़िटल इंडिया' को सलाम\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146934,   5502,    120,  34370,  78368, 145799,\n",
            "          72653,  64704,  14925,    248,  43647, 146934,  47809,  23868,    364,\n",
            "         146187,  42311,    250,   5502,    120,  38851, 145769,  91811,  14925,\n",
            "            229,  72314, 146187,  42311,    107,  23868,      6,  47809,  54575,\n",
            "          68158,  91811,  31411,    106, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6830.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अभिनव बिंद्रा ने जीता गोल्ड मेडल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378, 146101,  42311,    101, 145535,  14925,\n",
            "            105,  42311,    224, 145256,  85033,  23868,  14925,    101,  34370,\n",
            "          14925,    250,  43647,  79238,  23868,  14925,    245,  54575,  91811,\n",
            "          30484,     94,  91217,  54784,     94,  91811, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6831.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सिंहस्थ में आस्था की डुबकी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  42311,    224,  93948,  78368,  30484,\n",
            "             98,  91217,  54784,    224,  14925,    228,  78368,  30484,     98,\n",
            "          23868,  47809,  43647,  14925,     94,  72653, 145799,  64704,  43647,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6832.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>इस कैंप में आने वाले बच्चे करोड़पति बनते हैं\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146575,  78368,  47809,  12619,    230,  72314,\n",
            "          86162,  91217,  54784,    224,  14925,    228,  60096,  34370,  14925,\n",
            "            113,  31411,    110,  34370,  14925,    105, 146113,  30484,    248,\n",
            "          34370,  47809,  44179,  54575, 146187,   5502,    120,  86162,  79238,\n",
            "          38851,  14925,    105,  60096,  79238,  34370,  84310,  12619,    230,\n",
            "          72314, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6833.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नेपाल में क्यों बढ़ रही है भारत विरोधी भावना? चीन में बढ़ी दिलचस्पी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096,  54784,    103,  31411,    110,  91217,\n",
            "          54784,    224,  47809,  30484,    107,  54575,  72314,  14925,    105,\n",
            "         149269,   5502,    120,  14925,    108,  93948,  43647,  84310,  12619,\n",
            "            230,  14925,    255,  31411,    108,  79238,  14925,    113,  42311,\n",
            "            108,  54575, 146821,  43647,  14925,    255,  31411,    113,  60096,\n",
            "          23868,     30,  14925,    248,  43647,  60096,  91217,  54784,    224,\n",
            "          14925,    105, 149269,   5502,    120,  43647,  14925,     99,  42311,\n",
            "            110, 146113,  78368,  30484,    103,  43647, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6834.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>रेप पीड़ित बेटियों को न्याय दिलाने के लिए लड़ रही मांएं\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  44179,  54784,    103,  83636,  43647, 146187,\n",
            "           5502,    120,  38851,  79238,  14925,    105,  54784,    253,  42311,\n",
            "            107,  54575,  72314,  47809,  54575,  14925,    101,  30484,    107,\n",
            "          31411,    107,  14925,     99,  42311,    110,  31411,    101,  34370,\n",
            "          47809,  34370,  14925,    110,  42311,    237,  14925,    110, 146187,\n",
            "           5502,    120,  14925,    108,  93948,  43647,  91217,  31411,    224,\n",
            "         146049,  72314, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6835.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>प्रधानमंत्री नरेंद्र मोदी के भाषण के दौरान हिरासत में लिए गए कई आदिवासी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  85033, 146821,  31411,    101,  87244,\n",
            "          72314,  79238,  85033,  43647,  14925,    101,  44179,  54784,    224,\n",
            "         145256,  85033,  91217,  54575, 145256,  43647,  47809,  34370,  14925,\n",
            "            255,  31411,    115, 146548,  47809,  34370,  14925,     99,  12619,\n",
            "            234,  44179,  31411,    101,  84310,  42311,    108,  31411,    116,\n",
            "          79238,  91217,  54784,    224,  14925,    110,  42311,    237,  14925,\n",
            "            245, 146049,  47809, 147131,  14925,    228, 145256,  42311,    113,\n",
            "          31411,    116,  43647, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6836.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अमित शाहः दिल्ली पुलिस ने दंगों में अच्छा काम किया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  87244,  42311,     97,  14925,    114,\n",
            "          31411,    117,   5502,    225,  14925,     99,  42311,    110,  30484,\n",
            "            110,  43647,  83636,  72653,  91811,  42311,    116,  14925,    101,\n",
            "          34370,  14925,     99,  72314, 145959,  54575,  72314,  91217,  54784,\n",
            "            224,  14925,    227, 146113,  30484,    249,  23868,  47809,  31411,\n",
            "            106,  47809,  42311,    107,  23868, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6837.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>म्यांमार: रोहिंग्या मुसलमानों के गांव उजाड़कर बनाई गई सरकारी इमारतें\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  30484,    107,  31411,    224,  87244,\n",
            "          31411,    108,     25,  14925,    108,  54575,  93948,  42311,    224,\n",
            "         145959,  30484,    107,  23868,  91217,  72653,  78368,  91811,  87244,\n",
            "          31411,    101,  54575,  72314,  47809,  34370,  14925,    245,  31411,\n",
            "            224, 145535,  14925,    231, 146114,  31411,     94,   5502,    120,\n",
            "          64704,  44179,  14925,    105,  60096,  31411,    230,  14925,    245,\n",
            "         147131,  68158,  44179,  64704,  31411,    108,  43647,  14925,    229,\n",
            "          87244,  31411,    108,  79238,  54784,    224, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6838.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कवाफ़ी की कुछ कविताएँ\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704, 145535,  31411,    104,   5502,    120,\n",
            "          43647,  47809,  43647,  47809,  72653, 147877,  47809, 145535,  42311,\n",
            "             97,  31411,    237,   5502,    223, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6839.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अपने गूगल अकाउंट पर कैसे रखें नज़र\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  86162,  60096,  34370,  14925,    245,\n",
            "          12619,    224, 145959,  91811,  14925,    227,  64704,  31411,    231,\n",
            "          72314, 145769,  83636,  44179,  47809,  12619,    230,  78368,  34370,\n",
            "          14925,    108, 146800,  54784,    224,  14925,    101, 146114,   5502,\n",
            "            120,  44179, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6840.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>इमरान ख़ान चीन के वीगर मुसलमानों पर क्या बोले- पाँच बड़ी ख़बरें\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146575,  87244,  44179,  31411,    101,  14925,\n",
            "            244,   5502,    120,  23868,  60096,  14925,    248,  43647,  60096,\n",
            "          47809,  34370,  14925,    113,  43647, 145959,  44179,  91217,  72653,\n",
            "          78368,  91811,  87244,  31411,    101,  54575,  72314,  83636,  44179,\n",
            "          47809,  30484,    107,  23868,  14925,    105,  54575,  91811,  34370,\n",
            "             12,  83636,  31411,    223, 146113,  14925,    105, 146187,   5502,\n",
            "            120,  43647,  14925,    244,   5502,    120, 145799,  44179,  54784,\n",
            "            224, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6841.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>किस हैसियत में ब्रूनी को लाएँगे सार्कोज़ी....\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  42311,    116,  84310,  12619,    230,\n",
            "          78368,  42311,    107,  79238,  91217,  54784,    224,  14925,    105,\n",
            "          85033,  12619,    224,  60096,  43647,  47809,  54575,  14925,    110,\n",
            "          31411,    237,   5502,    223, 145959,  34370,  68158,  31411,    108,\n",
            "          30484,    243,  54575, 146114,   5502,    120,  43647,   1934, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6842.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नेपाल में जब नदी के ऊपर लटककर पहुंची एंबुलेंस\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096,  54784,    103,  31411,    110,  91217,\n",
            "          54784,    224,  14925,    250, 145799,  14925,    101, 145256,  43647,\n",
            "          47809,  34370,  14925,    232,  86162,  44179,  14925,    110, 145769,\n",
            "          64704,  64704,  44179,  83636,  93948,  72653,  72314, 146113,  43647,\n",
            "          14925,    237,  72314, 145799,  72653,  91811,  54784,    224,  78368,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6843.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नज़रिया: यूं ही नहीं कोई अमित शाह हो जाता है\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096, 146114,   5502,    120,  44179,  42311,\n",
            "            107,  23868,     25,  14925,    107,  12619,    224,  72314,  84310,\n",
            "          43647,  14925,    101,  93948,  43647,  72314,  47809,  54575, 147131,\n",
            "          14925,    227,  87244,  42311,     97,  14925,    114,  31411,    117,\n",
            "          84310,  54575,  14925,    250,  31411,     97,  23868,  84310,  12619,\n",
            "            230, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6844.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अंखी दास कौन हैं जिन्होंने फ़ेसबुक से दिया इस्तीफ़ा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  72314, 146800,  43647,  14925,     99,\n",
            "          31411,    116,  47809,  12619,    234,  60096,  84310,  12619,    230,\n",
            "          72314,  14925,    250,  42311,    101,  30484,    117,  54575,  72314,\n",
            "          60096,  34370,  14925,    104,   5502,    120,  34370,  78368, 145799,\n",
            "          72653,  64704,  68158,  34370,  14925,     99,  42311,    107,  23868,\n",
            "          14925,    229,  78368,  30484,     97,  43647, 146934,   5502,    120,\n",
            "          23868, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6845.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>डोनल्ड ट्रंप- नरेंद्र मोदी की केमिस्ट्री और पांच बड़े सवाल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146187,  54575,  60096,  91811,  30484,     94,\n",
            "          14925,    253,  85033,  72314,  86162,     12,  14925,    101,  44179,\n",
            "          54784,    224, 145256,  85033,  91217,  54575, 145256,  43647,  47809,\n",
            "          43647,  47809,  54784,    106,  42311,    116,  30484,    253,  85033,\n",
            "          43647,  14925,    242,  44179,  83636,  31411,    224, 146113,  14925,\n",
            "            105, 146187,   5502,    120,  34370,  68158, 145535,  31411,    110,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6846.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना की बंदी से ज़रूरी सेवाएं भी ठप\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          47809,  43647,  14925,    105,  72314, 145256,  43647,  68158,  34370,\n",
            "          14925,    250,   5502,    120,  44179,  12619,    224,  44179,  43647,\n",
            "          68158,  54784,    113,  31411,    237,  72314,  14925,    255,  43647,\n",
            "          14925,    254,  86162, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6847.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'कलाकार अकेला रहे तो अच्छा है'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592,  64704,  91811,  31411,    243,  31411,    108,\n",
            "          14925,    227,  64704,  54784,    110,  23868,  14925,    108,  93948,\n",
            "          34370,  14925,     97,  54575,  14925,    227, 146113,  30484,    249,\n",
            "          23868,  84310,  12619,    230,      6, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6848.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>उत्तर कोरिया: ट्रंप ने ट्रेन से जाने वाले किम जोंग-उन को जब एयर फ़ोर्स वन से घर छोड़ने को कहा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147181,  79238,  30484,     97,  44179,  47809,\n",
            "          54575,  44179,  42311,    107,  23868,     25,  14925,    253,  85033,\n",
            "          72314,  86162,  14925,    101,  34370,  14925,    253,  85033,  54784,\n",
            "            101,  68158,  34370,  14925,    250,  31411,    101,  34370,  14925,\n",
            "            113,  31411,    110,  34370,  47809,  42311,    106,  14925,    250,\n",
            "          54575,  72314, 145959,     12, 147181,  60096,  47809,  54575,  14925,\n",
            "            250, 145799,  14925,    237, 145420,  44179,  14925,    104,   5502,\n",
            "            120,  54575,  44179,  30484,    116,  14925,    113,  60096,  68158,\n",
            "          34370,  14925,    246,  44179,  14925,    249,  54575, 146187,   5502,\n",
            "            120,  60096,  34370,  47809,  54575,  47809,  93948,  23868, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6849.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सोरेन मामले पर विपक्ष का हंगामा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  54575,  44179,  54784,    101,  91217,\n",
            "          31411,    106,  91811,  34370,  83636,  44179,  14925,    113,  42311,\n",
            "            103,  64704,  30484,    115,  47809,  23868,  84310,  72314, 145959,\n",
            "          31411,    106,  23868, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6850.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>बाइडन प्रशासन में छाये रह सकते हैं भारतीय मूल के ये अमेरिकी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145799,  31411,    229, 146187,  60096,  83636,\n",
            "          85033, 145966,  31411,    116,  60096,  91217,  54784,    224,  14925,\n",
            "            249,  31411,    107,  34370,  14925,    108,  93948,  68158,  64704,\n",
            "          79238,  34370,  84310,  12619,    230,  72314,  14925,    255,  31411,\n",
            "            108,  79238,  43647, 145420,  91217,  12619,    224,  91811,  47809,\n",
            "          34370,  14925,    107,  34370,  14925,    227,  87244,  34370,  44179,\n",
            "          42311,    243,  43647, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6851.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>बीबीसी का फ़ोन क्यों काट रहे हैं ‘अस्सी के काशी’?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145799,  43647, 145799,  43647,  78368,  43647,\n",
            "          47809,  23868,  14925,    104,   5502,    120,  54575,  60096,  47809,\n",
            "          30484,    107,  54575,  72314,  47809,  31411,    253,  14925,    108,\n",
            "          93948,  34370,  84310,  12619,    230,  72314,   3369, 146378,  78368,\n",
            "          30484,    116,  43647,  47809,  34370,  47809,  31411,    114,  43647,\n",
            "            527,     30, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6852.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कबाड़ से बाइक बनाने वाला स्टूडेंट\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704, 145799,  31411,     94,   5502,    120,\n",
            "          68158,  34370,  14925,    105,  31411,    229,  64704,  14925,    105,\n",
            "          60096,  31411,    101,  34370,  14925,    113,  31411,    110,  23868,\n",
            "          68158,  30484,    253,  12619,    224, 146187,  54784,    224, 145769,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6853.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क़दीर ने उपकरण और 'डिज़ाइन' दिए थे\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,   5502,    120, 145256,  43647,  44179,\n",
            "          14925,    101,  34370,  14925,    231,  86162,  64704,  44179, 146548,\n",
            "          14925,    242,  44179,    364, 146187,  42311,    250,   5502,    120,\n",
            "          23868, 146575,  60096,      6,  14925,     99,  42311,    237,  14925,\n",
            "             98,  34370, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6854.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>म्यांमार में प्रदर्शनकारियों की मौत के बाद आर्मी जनरलों ने की पार्टी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  30484,    107,  31411,    224,  87244,\n",
            "          31411,    108,  91217,  54784,    224,  83636,  85033, 145256,  44179,\n",
            "          30484,    114,  60096,  64704,  31411,    108,  42311,    107,  54575,\n",
            "          72314,  47809,  43647,  91217,  12619,    234,  79238,  47809,  34370,\n",
            "          14925,    105,  31411,     99,  14925,    228,  44179,  30484,    106,\n",
            "          43647,  14925,    250,  60096,  44179,  91811,  54575,  72314,  14925,\n",
            "            101,  34370,  47809,  43647,  83636,  31411,    108,  30484,    253,\n",
            "          43647, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6855.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नरेंद्र मोदी तमिलनाडु की जनता को क्यों नहीं रिझा पाते?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096,  44179,  54784,    224, 145256,  85033,\n",
            "          91217,  54575, 145256,  43647,  14925,     97,  87244,  42311,    110,\n",
            "          60096,  31411,     94,  72653,  47809,  43647,  14925,    250,  60096,\n",
            "          79238,  23868,  47809,  54575,  47809,  30484,    107,  54575,  72314,\n",
            "          14925,    101,  93948,  43647,  72314,  14925,    108,  42311,    251,\n",
            "          23868,  83636,  31411,     97,  34370,     30, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6856.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>गुजरात में धमाका, पाँच की मौत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145959,  72653, 146114,  44179,  31411,     97,\n",
            "          91217,  54784,    224,  14925,    100,  87244,  31411,    243,  23868,\n",
            "             11,  83636,  31411,    223, 146113,  47809,  43647,  91217,  12619,\n",
            "            234,  79238, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6857.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>फे़क न्यूज़ पर बीबीसी रिसर्च को समझने के लिए इसे ज़रूर पढ़ें\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146934,  54784,    120,  64704,  14925,    101,\n",
            "          30484,    107,  12619,    224, 146114,   5502,    120,  83636,  44179,\n",
            "          14925,    105,  43647, 145799,  43647,  78368,  43647,  14925,    108,\n",
            "          42311,    116,  44179,  30484,    248,  47809,  54575,  68158,  87244,\n",
            "         148971,  60096,  34370,  47809,  34370,  14925,    110,  42311,    237,\n",
            "          14925,    229,  78368,  34370,  14925,    250,   5502,    120,  44179,\n",
            "          12619,    224,  44179,  83636, 149269,   5502,    120,  54784,    224,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6858.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'सेंसर बोर्ड का निर्णय अंतिम होना चाहिए'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592,  78368,  54784,    224,  78368,  44179,  14925,\n",
            "            105,  54575,  44179,  30484,     94,  47809,  23868,  14925,    101,\n",
            "          42311,    108,  30484,     96, 145420,  14925,    227,  72314,  79238,\n",
            "          42311,    106,  84310,  54575,  60096,  23868,  14925,    248,  31411,\n",
            "            117,  42311,    237,      6, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6859.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नई शिक्षा नीति क्या लड़कियों की स्कूल वापसी करा पाएगी?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096, 147131,  14925,    114,  42311,    243,\n",
            "          30484,    115,  23868,  14925,    101,  43647,  79238,  38851,  47809,\n",
            "          30484,    107,  23868,  14925,    110, 146187,   5502,    120,  64704,\n",
            "          42311,    107,  54575,  72314,  47809,  43647,  68158,  30484,    243,\n",
            "          12619,    224,  91811,  14925,    113,  31411,    103,  78368,  43647,\n",
            "          47809,  44179,  23868,  83636,  31411,    237, 145959,  43647,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6860.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>महज़ मुस्कान से लोग यहां करते हैं स्वागत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  93948, 146114,   5502,    120,  91217,\n",
            "          72653,  78368,  30484,    243,  31411,    101,  68158,  34370,  14925,\n",
            "            110,  54575, 145959,  14925,    107,  93948,  31411,    224,  47809,\n",
            "          44179,  79238,  34370,  84310,  12619,    230,  72314,  68158,  30484,\n",
            "            113,  31411,    245,  79238, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6861.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कौन हैं वो मुसलमान जिनके दरबार में पहुँचे प्रधानमंत्री नरेंद्र मोदी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  12619,    234,  60096,  84310,  12619,\n",
            "            230,  72314,  14925,    113,  54575,  91217,  72653,  78368,  91811,\n",
            "          87244,  31411,    101,  14925,    250,  42311,    101,  64704,  34370,\n",
            "          14925,     99,  44179, 145799,  31411,    108,  91217,  54784,    224,\n",
            "          83636,  93948,  72653,   5502,    223, 146113,  34370,  83636,  85033,\n",
            "         146821,  31411,    101,  87244,  72314,  79238,  85033,  43647,  14925,\n",
            "            101,  44179,  54784,    224, 145256,  85033,  91217,  54575, 145256,\n",
            "          43647, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6862.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>बलूचिस्तान: फ़ुटबॉल कैसे बदल रही हज़ारा औरतों की ज़िंदगी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145799,  91811,  12619,    224, 146113,  42311,\n",
            "            116,  30484,     97,  31411,    101,     25,  14925,    104,   5502,\n",
            "            120,  72653, 145769, 145799,  12619,    231,  91811,  47809,  12619,\n",
            "            230,  78368,  34370,  14925,    105, 145256,  91811,  14925,    108,\n",
            "          93948,  43647,  84310, 146114,   5502,    120,  23868,  44179,  23868,\n",
            "          14925,    242,  44179,  79238,  54575,  72314,  47809,  43647,  14925,\n",
            "            250,   5502,    120,  42311,    224, 145256, 145959,  43647, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6863.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पृथ्वी की वो जगहें जहां हैं मंगल ग्रह जैसे हालात\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  12619,    225, 146489,  30484,    113,\n",
            "          43647,  47809,  43647,  14925,    113,  54575,  14925,    250, 145959,\n",
            "          93948,  54784,    224,  14925,    250,  93948,  31411,    224,  84310,\n",
            "          12619,    230,  72314,  91217,  72314, 145959,  91811,  14925,    245,\n",
            "          85033,  93948,  14925,    250,  12619,    230,  78368,  34370,  84310,\n",
            "          31411,    110,  31411,     97, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6864.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>लंदन की टेम्स नदी में खोया हुआ 'ख़ज़ाना' ढूंढने वाले लोग\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  91811,  72314, 145256,  60096,  47809,  43647,\n",
            "          14925,    253,  54784,    106,  30484,    116,  14925,    101, 145256,\n",
            "          43647,  91217,  54784,    224,  14925,    244,  54575, 145420,  23868,\n",
            "          84310,  72653, 146399,    364, 146800,   5502,    120, 146114,   5502,\n",
            "            120,  23868,  60096,  23868,      6,  14925,     95,  12619,    224,\n",
            "          72314, 149269,  60096,  34370,  14925,    113,  31411,    110,  34370,\n",
            "          14925,    110,  54575, 145959, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6865.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>देवेंद्र फडणवीस का महाराष्ट्र की सत्ता का सफ़र और चुनौतियाँ\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145256,  54784,    113,  54784,    224, 145256,\n",
            "          85033,  14925,    104, 146187, 146548, 145535,  43647,  78368,  47809,\n",
            "          23868,  91217,  93948,  31411,    108,  31411,    115,  30484,    253,\n",
            "          85033,  47809,  43647,  68158,  79238,  30484,     97,  23868,  47809,\n",
            "          23868,  68158, 146934,   5502,    120,  44179,  14925,    242,  44179,\n",
            "          14925,    248,  72653,  60096,  12619,    234,  79238,  42311,    107,\n",
            "          31411,    223, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6866.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ये विधेयक बन गया है सेक्स वर्कर के लिए गले की फांस\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145420,  34370,  14925,    113,  42311,    100,\n",
            "          54784,    107,  64704,  14925,    105,  60096,  14925,    245, 145420,\n",
            "          23868,  84310,  12619,    230,  68158,  54784,    243,  30484,    116,\n",
            "          14925,    113,  44179,  30484,    243,  44179,  47809,  34370,  14925,\n",
            "            110,  42311,    237,  14925,    245,  91811,  34370,  47809,  43647,\n",
            "          14925,    104,  31411,    224,  78368, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6867.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'भारत-पाकिस्तान को झुलसा न दे अफ़ग़ानिस्तान की आग'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 146101,  31411,    108,  79238,     12,  86162,\n",
            "          31411,    243,  42311,    116,  30484,     97,  31411,    101,  47809,\n",
            "          54575,  14925,    251,  72653,  91811,  78368,  23868,  14925,    101,\n",
            "          14925,     99,  34370,  14925,    227, 146934,   5502,    120, 145959,\n",
            "           5502,    120,  23868,  60096,  42311,    116,  30484,     97,  31411,\n",
            "            101,  47809,  43647,  14925,    228, 145959,      6, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6868.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पिनराई विजयन: जिन्हें केरल में लोग 'धोती पहनने वाला मोदी' कहते हैं\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  42311,    101,  44179,  31411,    230,\n",
            "          14925,    113,  42311,    250, 145420,  60096,     25,  14925,    250,\n",
            "          42311,    101,  30484,    117,  54784,    224,  47809,  34370,  44179,\n",
            "          91811,  91217,  54784,    224,  14925,    110,  54575, 145959,    364,\n",
            "         146821,  54575,  79238,  43647,  83636,  93948,  60096,  60096,  34370,\n",
            "          14925,    113,  31411,    110,  23868,  91217,  54575, 145256,  43647,\n",
            "              6,  47809,  93948,  79238,  34370,  84310,  12619,    230,  72314,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6869.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>श्रीकृष्ण सिंह जयंती में लालू को मुख्य अतिथि बनाए जाने पर मचा घमासान\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145966,  85033,  43647,  64704,  12619,    225,\n",
            "         146352,  30484,     96,  68158,  42311,    224,  93948,  14925,    250,\n",
            "         145420,  72314,  79238,  43647,  91217,  54784,    224,  14925,    110,\n",
            "          31411,    110,  12619,    224,  47809,  54575,  91217,  72653, 146800,\n",
            "          30484,    107,  14925,    227,  79238,  42311,     98,  38851,  14925,\n",
            "            105,  60096,  31411,    237,  14925,    250,  31411,    101,  34370,\n",
            "          83636,  44179,  91217, 146113,  23868,  14925,    246,  87244,  31411,\n",
            "            116,  31411,    101, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6870.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जब अपू ने अमरीका में मचाई थी खलबली\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114, 145799,  14925,    227,  86162,  12619,\n",
            "            224,  14925,    101,  34370,  14925,    227,  87244,  44179,  43647,\n",
            "          64704,  23868,  91217,  54784,    224,  91217, 146113,  31411,    230,\n",
            "          14925,     98,  43647,  14925,    244,  91811, 145799,  91811,  43647,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6871.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>वनडे टीम में लक्ष्मण और कुंबले नहीं\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145535,  60096, 146187,  34370,  14925,    253,\n",
            "          43647,  87244,  91217,  54784,    224,  14925,    110,  64704,  30484,\n",
            "            115,  30484,    106, 146548,  14925,    242,  44179,  47809,  72653,\n",
            "          72314, 145799,  91811,  34370,  14925,    101,  93948,  43647,  72314,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6872.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस: क्या कोविड-19 टेस्ट का नतीजा ग़लत भी आ सकता है\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,     25,  47809,  30484,\n",
            "            107,  23868,  47809,  54575, 145535,  42311,     94,     12,     16,\n",
            "             24,  14925,    253,  54784,    116,  30484,    253,  47809,  23868,\n",
            "          14925,    101,  79238,  43647, 146114,  23868,  14925,    245,   5502,\n",
            "            120,  91811,  79238,  14925,    255,  43647,  14925,    228,  68158,\n",
            "          64704,  79238,  23868,  84310,  12619,    230, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6873.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'हैरी का विनोदपूर्ण व्यवहार दिखाना अच्छा रहेगा'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592,  93948,  12619,    230,  44179,  43647,  47809,\n",
            "          23868,  14925,    113,  42311,    101,  54575, 145256,  86162,  12619,\n",
            "            224,  44179,  30484,     96,  14925,    113,  30484,    107, 145535,\n",
            "          93948,  31411,    108,  14925,     99,  42311,    244,  31411,    101,\n",
            "          23868,  14925,    227, 146113,  30484,    249,  23868,  14925,    108,\n",
            "          93948,  54784,    245,  23868,      6, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6874.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या होता है जब महिलाएं करती हैं ड्राइविंग...\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,  84310,  54575,\n",
            "          79238,  23868,  84310,  12619,    230,  14925,    250, 145799,  91217,\n",
            "          93948,  42311,    110,  31411,    237,  72314,  47809,  44179,  79238,\n",
            "          43647,  84310,  12619,    230,  72314,  14925,     94,  85033,  31411,\n",
            "            229, 145535,  42311,    224, 145959,   1112, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6875.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'आपके पैर बहुत ख़ूबसूरत हैं'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 146399,  86162,  64704,  34370,  83636,  12619,\n",
            "            230,  44179,  14925,    105,  93948,  72653,  79238,  14925,    244,\n",
            "           5502,    120,  12619,    224, 145799,  78368,  12619,    224,  44179,\n",
            "          79238,  84310,  12619,    230,  72314,      6, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6876.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सीएए हिंसा: वसूली को लेकर इतनी आक्रामक क्यों है यूपी सरकार?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  43647, 146049, 146049,  84310,  42311,\n",
            "            224,  78368,  23868,     25,  14925,    113,  78368,  12619,    224,\n",
            "          91811,  43647,  47809,  54575,  14925,    110,  54784,    243,  44179,\n",
            "          14925,    229,  79238,  60096,  43647,  14925,    228,  64704,  85033,\n",
            "          31411,    106,  64704,  47809,  30484,    107,  54575,  72314,  84310,\n",
            "          12619,    230,  14925,    107,  12619,    224,  86162,  43647,  68158,\n",
            "          44179,  64704,  31411,    108,     30, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6877.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>रक्षा बजट पर कौन बेहतर- मोदी या मनमोहन?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  44179,  64704,  30484,    115,  23868,  14925,\n",
            "            105, 146114, 145769,  83636,  44179,  47809,  12619,    234,  60096,\n",
            "          14925,    105,  54784,    117,  79238,  44179,     12,  91217,  54575,\n",
            "         145256,  43647,  14925,    107,  23868,  91217,  60096,  87244,  54575,\n",
            "          93948,  60096,     30, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6878.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सीरिया में संघर्ष के बीच राष्ट्रपति चुनाव\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  43647,  44179,  42311,    107,  23868,\n",
            "          91217,  54784,    224,  68158,  72314, 147676,  44179,  30484,    115,\n",
            "          47809,  34370,  14925,    105,  43647, 146113,  14925,    108,  31411,\n",
            "            115,  30484,    253,  85033,  86162,  79238,  38851,  14925,    248,\n",
            "          72653,  60096,  31411,    113, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6879.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस: एक लड़ाई जो पर्दे के पीछे लड़ी जा रही है\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,     25,  14925,    237,\n",
            "          64704,  14925,    110, 146187,   5502,    120,  23868, 147131,  14925,\n",
            "            250,  54575,  83636,  44179,  30484,     99,  34370,  47809,  34370,\n",
            "          83636,  43647, 147877,  34370,  14925,    110, 146187,   5502,    120,\n",
            "          43647,  14925,    250,  23868,  14925,    108,  93948,  43647,  84310,\n",
            "          12619,    230, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6880.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>भारत ने दुनिया को शून्य देकर यूं की थी क्रांति\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146101,  31411,    108,  79238,  14925,    101,\n",
            "          34370,  14925,     99,  72653,  60096,  42311,    107,  23868,  47809,\n",
            "          54575,  14925,    114,  12619,    224,  60096,  30484,    107,  14925,\n",
            "             99,  54784,    243,  44179,  14925,    107,  12619,    224,  72314,\n",
            "          47809,  43647,  14925,     98,  43647,  47809,  85033,  31411,    224,\n",
            "          79238,  38851, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6881.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या कोरोना संक्रमण से निपटना पीएम मोदी की सबसे बड़ी अग्निपरीक्षा है?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,  47809,  54575,\n",
            "          44179,  54575,  60096,  23868,  68158,  72314,  64704,  85033,  87244,\n",
            "         146548,  68158,  34370,  14925,    101,  42311,    103, 145769,  60096,\n",
            "          23868,  83636,  43647, 146049,  87244,  91217,  54575, 145256,  43647,\n",
            "          47809,  43647,  68158, 145799,  78368,  34370,  14925,    105, 146187,\n",
            "           5502,    120,  43647,  14925,    227, 145959,  30484,    101,  42311,\n",
            "            103,  44179,  43647,  64704,  30484,    115,  23868,  84310,  12619,\n",
            "            230,     30, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6882.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ऊँची एड़ी के जूतों से क्या फ़ायदा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 149523,   5502,    223, 146113,  43647,  14925,\n",
            "            237, 146187,   5502,    120,  43647,  47809,  34370,  14925,    250,\n",
            "          12619,    224,  79238,  54575,  72314,  68158,  34370,  47809,  30484,\n",
            "            107,  23868,  14925,    104,   5502,    120,  23868, 145420, 145256,\n",
            "          23868,     30, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6883.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'पहले शौचालय है फिर देवालय'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592,  86162,  93948,  91811,  34370,  14925,    114,\n",
            "          12619,    234, 146113,  31411,    110, 145420,  84310,  12619,    230,\n",
            "          14925,    104,  42311,    108,  14925,     99,  54784,    113,  31411,\n",
            "            110, 145420,      6, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6884.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्रिकेट वर्ल्ड कपः वेस्टइंडीज़ ने पाकिस्तान को 7 विकेट से हराया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  85033,  42311,    243,  54784,    253,\n",
            "          14925,    113,  44179,  30484,    110,  30484,     94,  47809,  86162,\n",
            "           5502,    225,  14925,    113,  54784,    116,  30484,    253, 146575,\n",
            "          72314, 146187,  43647, 146114,   5502,    120,  14925,    101,  34370,\n",
            "          83636,  31411,    243,  42311,    116,  30484,     97,  31411,    101,\n",
            "          47809,  54575,    220,     22,  14925,    113,  42311,    243,  54784,\n",
            "            253,  68158,  34370,  84310,  44179,  31411,    107,  23868, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6885.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ये 5 देश तकनीक को सबसे आसान बना रहे हैं\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145420,  34370,    220,     20,  14925,     99,\n",
            "          54784,    114,  14925,     97,  64704,  60096,  43647,  64704,  47809,\n",
            "          54575,  68158, 145799,  78368,  34370,  14925,    228,  78368,  31411,\n",
            "            101,  14925,    105,  60096,  23868,  14925,    108,  93948,  34370,\n",
            "          84310,  12619,    230,  72314, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6886.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या दो बच्चों के क़ानून से नियंत्रित होगी जनसंख्या?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,  14925,     99,\n",
            "          54575,  14925,    105, 146113,  30484,    248,  54575,  72314,  47809,\n",
            "          34370,  47809,   5502,    120,  23868,  60096,  12619,    224,  60096,\n",
            "          68158,  34370,  14925,    101,  42311,    107,  72314,  79238,  85033,\n",
            "          42311,     97,  84310,  54575, 145959,  43647,  14925,    250,  60096,\n",
            "          78368,  72314, 146800,  30484,    107,  23868,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6887.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अज़रबैजान आर्मीनिया की लड़ाई के पीछे फ्रांस, तुर्की ने लगाया आरोप\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378, 146114,   5502,    120,  44179, 145799,\n",
            "          12619,    230, 146114,  31411,    101,  14925,    228,  44179,  30484,\n",
            "            106,  43647,  60096,  42311,    107,  23868,  47809,  43647,  14925,\n",
            "            110, 146187,   5502,    120,  23868, 147131,  47809,  34370,  83636,\n",
            "          43647, 147877,  34370,  14925,    104,  85033,  31411,    224,  78368,\n",
            "             11,  14925,     97,  72653,  44179,  30484,    243,  43647,  14925,\n",
            "            101,  34370,  14925,    110, 145959,  31411,    107,  23868,  14925,\n",
            "            228,  44179,  54575,  86162, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6888.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पीएम मोदी ने करदाताओं के लिए क्या घोषणा की?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  43647, 146049,  87244,  91217,  54575,\n",
            "         145256,  43647,  14925,    101,  34370,  47809,  44179, 145256,  31411,\n",
            "             97,  31411,    241,  72314,  47809,  34370,  14925,    110,  42311,\n",
            "            237,  47809,  30484,    107,  23868,  14925,    246,  54575, 146352,\n",
            "         146548,  23868,  47809,  43647,     30, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6889.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना: गंगा किनारे रेत में दबाए गए शवों को परंपरा का हिस्सा बताना कितना सही?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "             25,  14925,    245,  72314, 145959,  23868,  47809,  42311,    101,\n",
            "          31411,    108,  34370,  14925,    108,  54784,     97,  91217,  54784,\n",
            "            224,  14925,     99, 145799,  31411,    237,  14925,    245, 146049,\n",
            "          14925,    114, 145535,  54575,  72314,  47809,  54575,  83636,  44179,\n",
            "          72314,  86162,  44179,  23868,  47809,  23868,  84310,  42311,    116,\n",
            "          30484,    116,  23868,  14925,    105,  79238,  31411,    101,  23868,\n",
            "          47809,  42311,     97,  60096,  23868,  68158,  93948,  43647,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6890.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>वीडियोः सद्दाम के मुक़दमे की सुनवाई\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145535,  43647, 146187,  42311,    107,  54575,\n",
            "           5502,    225,  68158, 145256,  30484,     99,  31411,    106,  47809,\n",
            "          34370,  91217,  72653,  64704,   5502,    120, 145256,  87244,  34370,\n",
            "          47809,  43647,  68158,  72653,  60096, 145535,  31411,    230, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6891.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नेपाल बोला- एक तिहाई क्षेत्र भारत से हम पहले ही हार गए थे- प्रेस रिव्यू\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096,  54784,    103,  31411,    110,  14925,\n",
            "            105,  54575,  91811,  23868,     12,  14925,    237,  64704,  14925,\n",
            "             97,  42311,    117,  31411,    230,  47809,  30484,    115,  54784,\n",
            "             97,  85033,  14925,    255,  31411,    108,  79238,  68158,  34370,\n",
            "          84310,  87244,  83636,  93948,  91811,  34370,  84310,  43647,  84310,\n",
            "          31411,    108,  14925,    245, 146049,  14925,     98,  34370,     12,\n",
            "          83636,  85033,  54784,    116,  14925,    108,  42311,    113,  30484,\n",
            "            107,  12619,    224, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6892.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>काबुल वाया कोलकाता..तस्वीरों में\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  31411,    105,  72653,  91811,  14925,\n",
            "            113,  31411,    107,  23868,  47809,  54575,  91811,  64704,  31411,\n",
            "             97,  23868,    496,  79238,  78368,  30484,    113,  43647,  44179,\n",
            "          54575,  72314,  91217,  54784,    224, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6893.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जिसने खींची थी भारत-पाकिस्तान के बीच बंटवारे की रेखा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,  42311,    116,  60096,  34370,  14925,\n",
            "            244,  43647,  72314, 146113,  43647,  14925,     98,  43647,  14925,\n",
            "            255,  31411,    108,  79238,     12,  86162,  31411,    243,  42311,\n",
            "            116,  30484,     97,  31411,    101,  47809,  34370,  14925,    105,\n",
            "          43647, 146113,  14925,    105,  72314, 145769, 145535,  31411,    108,\n",
            "          34370,  47809,  43647,  14925,    108,  54784,    244,  23868, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6894.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>दुनिया के सारे पेड़ ख़त्म हो जाएंगे तो क्या होगा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145256,  72653,  60096,  42311,    107,  23868,\n",
            "          47809,  34370,  68158,  31411,    108,  34370,  83636,  54784,     94,\n",
            "           5502,    120,  14925,    244,   5502,    120,  79238,  30484,    106,\n",
            "          84310,  54575,  14925,    250,  31411,    237,  72314, 145959,  34370,\n",
            "          14925,     97,  54575,  47809,  30484,    107,  23868,  84310,  54575,\n",
            "         145959,  23868,     30, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6895.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अयोध्या मामला: बीजेपी को राम मंदिर आंदोलन से क्या हुआ हासिल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378, 145420,  54575, 146821,  30484,    107,\n",
            "          23868,  91217,  31411,    106,  91811,  23868,     25,  14925,    105,\n",
            "          43647, 146114,  54784,    103,  43647,  47809,  54575,  14925,    108,\n",
            "          31411,    106,  91217,  72314, 145256,  42311,    108,  14925,    228,\n",
            "          72314, 145256,  54575,  91811,  60096,  68158,  34370,  47809,  30484,\n",
            "            107,  23868,  84310,  72653, 146399,  84310,  31411,    116,  42311,\n",
            "            110, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6896.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>दिशा रवि कौन हैं, जिनकी गिरफ़्तारी से डरे हुए हैं पर्यावरण कार्यकर्ता\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145256,  42311,    114,  23868,  14925,    108,\n",
            "         145535,  38851,  47809,  12619,    234,  60096,  84310,  12619,    230,\n",
            "          72314,     11,  14925,    250,  42311,    101,  64704,  43647,  14925,\n",
            "            245,  42311,    108, 146934,   5502,    120,  29607,  79238,  31411,\n",
            "            108,  43647,  68158,  34370,  14925,     94,  44179,  34370,  84310,\n",
            "          72653, 146049,  84310,  12619,    230,  72314,  83636,  44179,  30484,\n",
            "            107,  31411,    113,  44179, 146548,  47809,  31411,    108,  30484,\n",
            "            107,  64704,  44179,  30484,     97,  23868, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6897.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अलवर हत्याकांड में 20 को उम्रक़ैद\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  91811, 145535,  44179,  84310,  79238,\n",
            "          30484,    107,  31411,    243,  31411,    224, 146187,  91217,  54784,\n",
            "            224,    220,     17,     15,  47809,  54575,  14925,    231,  87244,\n",
            "          85033,  64704,   5502,    120,  12619,    230, 145256, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6898.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>25000 मौतें, सज़ा 35 मिनट प्रति मौत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,     17,     20,     15,     15,     15,  91217,\n",
            "          12619,    234,  79238,  54784,    224,     11,  68158, 146114,   5502,\n",
            "            120,  23868,    220,     18,     20,  91217,  42311,    101, 145769,\n",
            "          83636,  85033,  79238,  38851,  91217,  12619,    234,  79238, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6899.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना: 'जब हम संक्रमित परिवारों को खाना पहुँचाते हैं, तो वो हाथ जोड़ शुक्रिया कहते हैं\"\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "             25,    364, 146114, 145799,  84310,  87244,  68158,  72314,  64704,\n",
            "          85033,  87244,  42311,     97,  83636,  44179,  42311,    113,  31411,\n",
            "            108,  54575,  72314,  47809,  54575,  14925,    244,  31411,    101,\n",
            "          23868,  83636,  93948,  72653,   5502,    223, 146113,  31411,     97,\n",
            "          34370,  84310,  12619,    230,  72314,     11,  14925,     97,  54575,\n",
            "          14925,    113,  54575,  84310,  31411,     98,  14925,    250,  54575,\n",
            "         146187,   5502,    120,  14925,    114,  72653,  64704,  85033,  42311,\n",
            "            107,  23868,  47809,  93948,  79238,  34370,  84310,  12619,    230,\n",
            "          72314,      1, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6900.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>श्रीलंका: गोटाभाया के भारत दौरे के मायने क्या हैं\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145966,  85033,  43647,  91811,  72314,  64704,\n",
            "          23868,     25,  14925,    245,  54575, 145769,  31411,    255,  31411,\n",
            "            107,  23868,  47809,  34370,  14925,    255,  31411,    108,  79238,\n",
            "          14925,     99,  12619,    234,  44179,  34370,  47809,  34370,  91217,\n",
            "          31411,    107,  60096,  34370,  47809,  30484,    107,  23868,  84310,\n",
            "          12619,    230,  72314, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6901.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना: वायरस की उत्पत्ति की जाँच पर अड़ा अमेरिका, नाराज़ हुआ चीन\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "             25,  14925,    113,  31411,    107,  44179,  78368,  47809,  43647,\n",
            "          14925,    231,  79238,  30484,    103,  79238,  30484,     97,  38851,\n",
            "          47809,  43647,  14925,    250,  31411,    223, 146113,  83636,  44179,\n",
            "          14925,    227, 146187,   5502,    120,  23868,  14925,    227,  87244,\n",
            "          34370,  44179,  42311,    243,  23868,     11,  14925,    101,  31411,\n",
            "            108,  31411,    250,   5502,    120,  84310,  72653, 146399,  14925,\n",
            "            248,  43647,  60096, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6902.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कितनी प्रैक्टिस से आप बन सकते हैं बेस्ट\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  42311,     97,  60096,  43647,  83636,\n",
            "          85033,  12619,    230,  64704,  30484,    253,  42311,    116,  68158,\n",
            "          34370,  14925,    228,  86162,  14925,    105,  60096,  68158,  64704,\n",
            "          79238,  34370,  84310,  12619,    230,  72314,  14925,    105,  54784,\n",
            "            116,  30484,    253, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6903.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नज़रिया: डोकलाम विवाद पर चीन क्यों है बैकफ़ुट पर?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096, 146114,   5502,    120,  44179,  42311,\n",
            "            107,  23868,     25,  14925,     94,  54575,  64704,  91811,  31411,\n",
            "            106,  14925,    113,  42311,    113,  31411,     99,  83636,  44179,\n",
            "          14925,    248,  43647,  60096,  47809,  30484,    107,  54575,  72314,\n",
            "          84310,  12619,    230,  14925,    105,  12619,    230,  64704, 146934,\n",
            "           5502,    120,  72653, 145769,  83636,  44179,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6904.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पुलवामा हमला: मसूद अज़हर को 'आतंकवादी' क्यों नहीं मानता है चीन?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  72653,  91811, 145535,  31411,    106,\n",
            "          23868,  84310,  87244,  91811,  23868,     25,  91217,  78368,  12619,\n",
            "            224, 145256,  14925,    227, 146114,   5502,    120,  93948,  44179,\n",
            "          47809,  54575,    364, 146399,  79238,  72314,  64704, 145535,  31411,\n",
            "             99,  43647,      6,  47809,  30484,    107,  54575,  72314,  14925,\n",
            "            101,  93948,  43647,  72314,  91217,  31411,    101,  79238,  23868,\n",
            "          84310,  12619,    230,  14925,    248,  43647,  60096,     30, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6905.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ध्यानचंद का भारत रत्न कैसे 'गोल' कर गईं सरकारें\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146821,  30484,    107,  31411,    101, 146113,\n",
            "          72314, 145256,  47809,  23868,  14925,    255,  31411,    108,  79238,\n",
            "          14925,    108,  79238,  30484,    101,  47809,  12619,    230,  78368,\n",
            "          34370,    364, 145959,  54575,  91811,      6,  47809,  44179,  14925,\n",
            "            245, 147131,  72314,  68158,  44179,  64704,  31411,    108,  54784,\n",
            "            224, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6906.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पार्टी कहेगी तो बन जाऊंगा प्रधानमंत्री : रमन सिंह\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  31411,    108,  30484,    253,  43647,\n",
            "          47809,  93948,  54784,    245,  43647,  14925,     97,  54575,  14925,\n",
            "            105,  60096,  14925,    250,  31411,    232,  72314, 145959,  23868,\n",
            "          83636,  85033, 146821,  31411,    101,  87244,  72314,  79238,  85033,\n",
            "          43647,    549,  14925,    108,  87244,  60096,  68158,  42311,    224,\n",
            "          93948, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6907.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>लखनऊ मुठभेड़ खत्म, एक संदिग्ध की मौत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  91811, 146800,  60096, 149523,  91217,  72653,\n",
            "         146826, 146101,  54784,     94,   5502,    120,  14925,    244,  79238,\n",
            "          30484,    106,     11,  14925,    237,  64704,  68158,  72314, 145256,\n",
            "          42311,    245,  30484,    100,  47809,  43647,  91217,  12619,    234,\n",
            "          79238, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6908.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>लॉकडाउन की चोट खाने वाले प्रवासी मज़दूर के ज़ख़्म आपने देखे?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  91811,  12619,    231,  64704, 146187,  31411,\n",
            "            231,  60096,  47809,  43647,  14925,    248,  54575, 145769,  14925,\n",
            "            244,  31411,    101,  34370,  14925,    113,  31411,    110,  34370,\n",
            "          83636,  85033, 145535,  31411,    116,  43647,  91217, 146114,   5502,\n",
            "            120, 145256,  12619,    224,  44179,  47809,  34370,  14925,    250,\n",
            "           5502,    120, 146800,   5502,    120,  29607,  87244,  14925,    228,\n",
            "          86162,  60096,  34370,  14925,     99,  54784,    244,  34370,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6909.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>रियो ओलंपिक: हॉकी में भारत जीता\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  44179,  42311,    107,  54575,  14925,    241,\n",
            "          91811,  72314,  86162,  42311,    243,     25,  84310,  12619,    231,\n",
            "          64704,  43647,  91217,  54784,    224,  14925,    255,  31411,    108,\n",
            "          79238,  14925,    250,  43647,  79238,  23868, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6910.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कैसा होता है दुनिया के 'सबसे ख़ुशनुमा देश' में डिप्रेशन में होना?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  12619,    230,  78368,  23868,  84310,\n",
            "          54575,  79238,  23868,  84310,  12619,    230,  14925,     99,  72653,\n",
            "          60096,  42311,    107,  23868,  47809,  34370,    364,  78368, 145799,\n",
            "          78368,  34370,  14925,    244,   5502,    120,  72653, 145966,  60096,\n",
            "          72653,  87244,  23868,  14925,     99,  54784,    114,      6,  91217,\n",
            "          54784,    224,  14925,     94,  42311,    103,  85033,  54784,    114,\n",
            "          60096,  91217,  54784,    224,  84310,  54575,  60096,  23868,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6911.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>किरण बेदी के बारे में ख़ास बातें\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  42311,    108, 146548,  14925,    105,\n",
            "          54784,     99,  43647,  47809,  34370,  14925,    105,  31411,    108,\n",
            "          34370,  91217,  54784,    224,  14925,    244,   5502,    120,  23868,\n",
            "          78368,  14925,    105,  31411,     97,  54784,    224, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6912.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>हवा से रोज़ 2000 लीटर पानी निकालेगी ये मशीन\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  93948, 145535,  23868,  68158,  34370,  14925,\n",
            "            108,  54575, 146114,   5502,    120,    220,     17,     15,     15,\n",
            "             15,  14925,    110,  43647, 145769,  44179,  83636,  31411,    101,\n",
            "          43647,  14925,    101,  42311,    243,  31411,    110,  54784,    245,\n",
            "          43647,  14925,    107,  34370,  91217, 145966,  43647,  60096, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6913.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस के कारण लॉकडाउन में कैसे जी रहे हैं जम्मू-कश्मीर के गुज्जर और बकरवाल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,  47809,  34370,  47809,\n",
            "          31411,    108, 146548,  14925,    110,  12619,    231,  64704, 146187,\n",
            "          31411,    231,  60096,  91217,  54784,    224,  47809,  12619,    230,\n",
            "          78368,  34370,  14925,    250,  43647,  14925,    108,  93948,  34370,\n",
            "          84310,  12619,    230,  72314,  14925,    250,  87244,  30484,    106,\n",
            "          12619,    224,     12,  64704, 145966,  30484,    106,  43647,  44179,\n",
            "          47809,  34370,  14925,    245,  72653, 146114,  30484,    250,  44179,\n",
            "          14925,    242,  44179,  14925,    105,  64704,  44179, 145535,  31411,\n",
            "            110, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6914.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>मैडोना ने किया ऐतिहासिक अनुबंध\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  12619,    230, 146187,  54575,  60096,\n",
            "          23868,  14925,    101,  34370,  47809,  42311,    107,  23868,  14925,\n",
            "            238,  79238,  42311,    117,  31411,    116,  42311,    243,  14925,\n",
            "            227,  60096,  72653, 145799,  72314, 146821, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6915.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>प्रथम विश्व युद्ध के दौर की मौत की सुरंग को कैसे ढूँढा गया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  85033, 146489,  87244,  14925,    113,\n",
            "          42311,    114,  30484,    113,  14925,    107,  72653, 145256,  30484,\n",
            "            100,  47809,  34370,  14925,     99,  12619,    234,  44179,  47809,\n",
            "          43647,  91217,  12619,    234,  79238,  47809,  43647,  68158,  72653,\n",
            "          44179,  72314, 145959,  47809,  54575,  47809,  12619,    230,  78368,\n",
            "          34370,  14925,     95,  12619,    224,   5502,    223, 149269,  23868,\n",
            "          14925,    245, 145420,  23868, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6916.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>भारतीय व्यंजन असल में कितने भारतीय हैं?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146101,  31411,    108,  79238,  43647, 145420,\n",
            "          14925,    113,  30484,    107,  72314, 146114,  60096,  14925,    227,\n",
            "          78368,  91811,  91217,  54784,    224,  47809,  42311,     97,  60096,\n",
            "          34370,  14925,    255,  31411,    108,  79238,  43647, 145420,  84310,\n",
            "          12619,    230,  72314,     30, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6917.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>तो क्या 'बेमतलब' है श्री श्री की अयोध्या यात्रा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  79238,  54575,  47809,  30484,    107,  23868,\n",
            "            364, 145799,  54784,    106,  79238,  91811, 145799,      6,  84310,\n",
            "          12619,    230,  14925,    114,  85033,  43647,  14925,    114,  85033,\n",
            "          43647,  47809,  43647,  14925,    227, 145420,  54575, 146821,  30484,\n",
            "            107,  23868,  14925,    107,  31411,     97,  85033,  23868,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6918.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अब किसी भी देश का फ़ोन नंबर अपना कर लें\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378, 145799,  47809,  42311,    116,  43647,\n",
            "          14925,    255,  43647,  14925,     99,  54784,    114,  47809,  23868,\n",
            "          14925,    104,   5502,    120,  54575,  60096,  14925,    101,  72314,\n",
            "         145799,  44179,  14925,    227,  86162,  60096,  23868,  47809,  44179,\n",
            "          14925,    110,  54784,    224, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6919.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ज़ुल्फ़िकार अली भुट्टो शासन के आख़िरी दिनों की कहानी - विवेचना\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,   5502,    120,  72653,  91811,  30484,\n",
            "            104,   5502,    120,  38851,  64704,  31411,    108,  14925,    227,\n",
            "          91811,  43647,  14925,    255,  72653, 145769,  30484,    253,  54575,\n",
            "          14925,    114,  31411,    116,  60096,  47809,  34370,  14925,    228,\n",
            "         146800,   5502,    120,  38851,  44179,  43647,  14925,     99,  42311,\n",
            "            101,  54575,  72314,  47809,  43647,  47809,  93948,  31411,    101,\n",
            "          43647,    481,  14925,    113,  42311,    113,  54784,    248,  60096,\n",
            "          23868, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6920.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>और वो घड़ी आ गई है...\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147536,  44179,  14925,    113,  54575,  14925,\n",
            "            246, 146187,   5502,    120,  43647,  14925,    228,  14925,    245,\n",
            "         147131,  84310,  12619,    230,   1112, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6921.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कायले मेयर्स: टेस्ट क्रिकेट की नयी सनसनी, नॉटआउट 210 रन बनाकर दिलाई हैरतअंगेज़ जीत\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  31411,    107,  91811,  34370,  91217,\n",
            "          54784,    107,  44179,  30484,    116,     25,  14925,    253,  54784,\n",
            "            116,  30484,    253,  47809,  85033,  42311,    243,  54784,    253,\n",
            "          47809,  43647,  14925,    101, 145420,  43647,  68158,  60096,  78368,\n",
            "          60096,  43647,     11,  14925,    101,  12619,    231, 145769, 146399,\n",
            "         147181, 145769,    220,     17,     16,     15,  14925,    108,  60096,\n",
            "          14925,    105,  60096,  31411,    243,  44179,  14925,     99,  42311,\n",
            "            110,  31411,    230,  84310,  12619,    230,  44179,  79238, 146378,\n",
            "          72314, 145959,  54784,    250,   5502,    120,  14925,    250,  43647,\n",
            "          79238, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6922.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>लंदन नाकाम विस्फोट का 'हमलावर' गिरफ़्तार\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  91811,  72314, 145256,  60096,  14925,    101,\n",
            "          31411,    243,  31411,    106,  14925,    113,  42311,    116,  30484,\n",
            "            104,  54575, 145769,  47809,  23868,    364,  93948,  87244,  91811,\n",
            "          31411,    113,  44179,      6,  14925,    245,  42311,    108, 146934,\n",
            "           5502,    120,  29607,  79238,  31411,    108, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6923.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कश्मीर में युवा क्यों उठा रहे हैं बंदूक?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704, 145966,  30484,    106,  43647,  44179,\n",
            "          91217,  54784,    224,  14925,    107,  72653, 145535,  23868,  47809,\n",
            "          30484,    107,  54575,  72314,  14925,    231, 146826,  23868,  14925,\n",
            "            108,  93948,  34370,  84310,  12619,    230,  72314,  14925,    105,\n",
            "          72314, 145256,  12619,    224,  64704,     30, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6924.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या कोविड-19 हाथ मिलाने का चलन ख़त्म कर देगा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,  47809,  54575,\n",
            "         145535,  42311,     94,     12,     16,     24,  84310,  31411,     98,\n",
            "          91217,  42311,    110,  31411,    101,  34370,  47809,  23868,  14925,\n",
            "            248,  91811,  60096,  14925,    244,   5502,    120,  79238,  30484,\n",
            "            106,  47809,  44179,  14925,     99,  54784,    245,  23868,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6925.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>मुस्लिम विद्वान की गिरफ़्तारी का वारंट\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  72653,  78368,  30484,    110,  42311,\n",
            "            106,  14925,    113,  42311,     99,  30484,    113,  31411,    101,\n",
            "          47809,  43647,  14925,    245,  42311,    108, 146934,   5502,    120,\n",
            "          29607,  79238,  31411,    108,  43647,  47809,  23868,  14925,    113,\n",
            "          31411,    108,  72314, 145769, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6926.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>27 साल जेल के बाद चावल और चिकन करी !!\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,     17,     22,  68158,  31411,    110,  14925,\n",
            "            250,  54784,    110,  47809,  34370,  14925,    105,  31411,     99,\n",
            "          14925,    248,  31411,    113,  91811,  14925,    242,  44179,  14925,\n",
            "            248,  42311,    243,  60096,  47809,  44179,  43647,  11015, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6927.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सफल रहा आठवाँ भारत रंग महोत्सव\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368, 146934,  91811,  14925,    108,  93948,\n",
            "          23868,  14925,    228, 146826, 145535,  31411,    223,  14925,    255,\n",
            "          31411,    108,  79238,  14925,    108,  72314, 145959,  91217,  93948,\n",
            "          54575,  79238,  30484,    116, 145535, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6928.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पीएम मोदी ने जेएनयू के कार्यक्रम में क्या कहा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  43647, 146049,  87244,  91217,  54575,\n",
            "         145256,  43647,  14925,    101,  34370,  14925,    250,  54784,    237,\n",
            "          60096, 145420,  12619,    224,  47809,  34370,  47809,  31411,    108,\n",
            "          30484,    107,  64704,  85033,  87244,  91217,  54784,    224,  47809,\n",
            "          30484,    107,  23868,  47809,  93948,  23868,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6929.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सरोद वादक शरन रानी का निधन\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  44179,  54575, 145256,  14925,    113,\n",
            "          31411,     99,  64704,  14925,    114,  44179,  60096,  14925,    108,\n",
            "          31411,    101,  43647,  47809,  23868,  14925,    101,  42311,    100,\n",
            "          60096, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6930.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ट्रंप के विस्कॉन्सिन दौरे के ख़िलाफ़ प्रदर्शन\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145769,  85033,  72314,  86162,  47809,  34370,\n",
            "          14925,    113,  42311,    116,  30484,    243,  12619,    231,  60096,\n",
            "          30484,    116,  42311,    101,  14925,     99,  12619,    234,  44179,\n",
            "          34370,  47809,  34370,  14925,    244,   5502,    120,  38851,  91811,\n",
            "          31411,    104,   5502,    120,  83636,  85033, 145256,  44179,  30484,\n",
            "            114,  60096, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6931.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>किसान आंदोलन का समर्थन क्यों कर रहे हैं ब्रिटेन के कई सांसद\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  42311,    116,  31411,    101,  14925,\n",
            "            228,  72314, 145256,  54575,  91811,  60096,  47809,  23868,  68158,\n",
            "          87244,  44179,  30484,     98,  60096,  47809,  30484,    107,  54575,\n",
            "          72314,  47809,  44179,  14925,    108,  93948,  34370,  84310,  12619,\n",
            "            230,  72314,  14925,    105,  85033,  42311,    253,  54784,    101,\n",
            "          47809,  34370,  47809, 147131,  68158,  31411,    224,  78368, 145256,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6932.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस की वैक्सीन में केकड़ा के ख़ून का क्या इस्तेमाल?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,  47809,  43647,  14925,\n",
            "            113,  12619,    230,  64704,  30484,    116,  43647,  60096,  91217,\n",
            "          54784,    224,  47809,  54784,    243, 146187,   5502,    120,  23868,\n",
            "          47809,  34370,  14925,    244,   5502,    120,  12619,    224,  60096,\n",
            "          47809,  23868,  47809,  30484,    107,  23868,  14925,    229,  78368,\n",
            "          30484,     97,  54784,    106,  31411,    110,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6933.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वैक्सीन बनाने का काम किस देश में कहां तक पहुंचा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  12619,    230,  64704,  30484,    116,  43647,  60096,\n",
            "          14925,    105,  60096,  31411,    101,  34370,  47809,  23868,  47809,\n",
            "          31411,    106,  47809,  42311,    116,  14925,     99,  54784,    114,\n",
            "          91217,  54784,    224,  47809,  93948,  31411,    224,  14925,     97,\n",
            "          64704,  83636,  93948,  72653,  72314, 146113,  23868,     30, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6934.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>दिल्ली: 18 साल के एक लड़के की कथित तौर पर पीट-पीटकर हत्या\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145256,  42311,    110,  30484,    110,  43647,\n",
            "             25,    220,     16,     23,  68158,  31411,    110,  47809,  34370,\n",
            "          14925,    237,  64704,  14925,    110, 146187,   5502,    120,  64704,\n",
            "          34370,  47809,  43647,  47809, 146489,  42311,     97,  14925,     97,\n",
            "          12619,    234,  44179,  83636,  44179,  83636,  43647, 145769,     12,\n",
            "          86162,  43647, 145769,  64704,  44179,  84310,  79238,  30484,    107,\n",
            "          23868, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6935.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस: पुरुष मास्क पहनने से परहेज क्यों करते हैं?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,     25,  83636,  72653,\n",
            "          44179,  72653, 146352,  91217,  31411,    116,  30484,    243,  83636,\n",
            "          93948,  60096,  60096,  34370,  68158,  34370,  83636,  44179,  93948,\n",
            "          54784,    250,  47809,  30484,    107,  54575,  72314,  47809,  44179,\n",
            "          79238,  34370,  84310,  12619,    230,  72314,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6936.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>हवा कैसे चलती है...\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  93948, 145535,  23868,  47809,  12619,    230,\n",
            "          78368,  34370,  14925,    248,  91811,  79238,  43647,  84310,  12619,\n",
            "            230,   1112, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6937.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>रॉबर्ट राजनीतिक कारणों से निशाने पर हैं: प्रियंका गांधी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  44179,  12619,    231, 145799,  44179,  30484,\n",
            "            253,  14925,    108,  31411,    250,  60096,  43647,  79238,  42311,\n",
            "            243,  47809,  31411,    108, 146548,  54575,  72314,  68158,  34370,\n",
            "          14925,    101,  42311,    114,  31411,    101,  34370,  83636,  44179,\n",
            "          84310,  12619,    230,  72314,     25,  83636,  85033,  42311,    107,\n",
            "          72314,  64704,  23868,  14925,    245,  31411,    224, 146821,  43647,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6938.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>बरगद का यह पेड़ अकेला पूरे जंगल के बराबर है\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145799,  44179, 145959, 145256,  47809,  23868,\n",
            "          14925,    107,  93948,  83636,  54784,     94,   5502,    120,  14925,\n",
            "            227,  64704,  54784,    110,  23868,  83636,  12619,    224,  44179,\n",
            "          34370,  14925,    250,  72314, 145959,  91811,  47809,  34370,  14925,\n",
            "            105,  44179,  31411,    105,  44179,  84310,  12619,    230, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6939.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस कोविड-19 की पूरी एनाटॉमी: अब तक जो पता चला है\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,  47809,  54575, 145535,\n",
            "          42311,     94,     12,     16,     24,  47809,  43647,  83636,  12619,\n",
            "            224,  44179,  43647,  14925,    237,  60096,  31411,    253,  12619,\n",
            "            231,  87244,  43647,     25,  14925,    227, 145799,  14925,     97,\n",
            "          64704,  14925,    250,  54575,  83636,  79238,  23868,  14925,    248,\n",
            "          91811,  23868,  84310,  12619,    230, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6940.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>व्यापमं: सरकार ने बताया ऐसे मरे 34\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145535,  30484,    107,  31411,    103,  87244,\n",
            "          72314,     25,  68158,  44179,  64704,  31411,    108,  14925,    101,\n",
            "          34370,  14925,    105,  79238,  31411,    107,  23868,  14925,    238,\n",
            "          78368,  34370,  91217,  44179,  34370,    220,     18,     19, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6941.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस से ईरान का इतना बुरा हाल कैसे हो गया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,  68158,  34370,  14925,\n",
            "            230,  44179,  31411,    101,  47809,  23868,  14925,    229,  79238,\n",
            "          60096,  23868,  14925,    105,  72653,  44179,  23868,  84310,  31411,\n",
            "            110,  47809,  12619,    230,  78368,  34370,  84310,  54575,  14925,\n",
            "            245, 145420,  23868, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6942.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या 'आतंकवादी' भी व्यवसायी की तरह सोचते हैं?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,    364, 146399,\n",
            "          79238,  72314,  64704, 145535,  31411,     99,  43647,      6,  14925,\n",
            "            255,  43647,  14925,    113,  30484,    107, 145535,  78368,  31411,\n",
            "            107,  43647,  47809,  43647,  14925,     97,  44179,  93948,  68158,\n",
            "          54575, 146113,  79238,  34370,  84310,  12619,    230,  72314,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6943.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>निर्मला सीतारमण की घोषणा से सिर्फ़ सरकारी कर्मचारियों को ही फ़ायदा या प्राइवेट सेक्टर के लिए भी है कुछ?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096,  42311,    108,  30484,    106,  91811,\n",
            "          23868,  68158,  43647,  79238,  31411,    108,  87244, 146548,  47809,\n",
            "          43647,  14925,    246,  54575, 146352, 146548,  23868,  68158,  34370,\n",
            "          68158,  42311,    108,  30484,    104,   5502,    120,  68158,  44179,\n",
            "          64704,  31411,    108,  43647,  47809,  44179,  30484,    106, 146113,\n",
            "          31411,    108,  42311,    107,  54575,  72314,  47809,  54575,  84310,\n",
            "          43647,  14925,    104,   5502,    120,  23868, 145420, 145256,  23868,\n",
            "          14925,    107,  23868,  83636,  85033,  31411,    229, 145535,  54784,\n",
            "            253,  68158,  54784,    243,  30484,    253,  44179,  47809,  34370,\n",
            "          14925,    110,  42311,    237,  14925,    255,  43647,  84310,  12619,\n",
            "            230,  47809,  72653, 147877,     30, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6944.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अज़रबैजान-आर्मीनिया से ग्राउंड रिपोर्ट: बमबारी, जंग और दहशत की दुनिया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378, 146114,   5502,    120,  44179, 145799,\n",
            "          12619,    230, 146114,  31411,    101,     12, 146399,  44179,  30484,\n",
            "            106,  43647,  60096,  42311,    107,  23868,  68158,  34370,  14925,\n",
            "            245,  85033,  31411,    231,  72314, 146187,  14925,    108,  42311,\n",
            "            103,  54575,  44179,  30484,    253,     25,  14925,    105,  87244,\n",
            "         145799,  31411,    108,  43647,     11,  14925,    250,  72314, 145959,\n",
            "          14925,    242,  44179,  14925,     99,  93948, 145966,  79238,  47809,\n",
            "          43647,  14925,     99,  72653,  60096,  42311,    107,  23868, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6945.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>गोरखपुर त्रासदी: डॉक्टरों की बदसलूकी और सरकार की बेरुखी से लोगों में ग़ुस्सा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145959,  54575,  44179, 146800,  86162,  72653,\n",
            "          44179,  14925,     97,  85033,  31411,    116, 145256,  43647,     25,\n",
            "          14925,     94,  12619,    231,  64704,  30484,    253,  44179,  54575,\n",
            "          72314,  47809,  43647,  14925,    105, 145256,  78368,  91811,  12619,\n",
            "            224,  64704,  43647,  14925,    242,  44179,  68158,  44179,  64704,\n",
            "          31411,    108,  47809,  43647,  14925,    105,  34370,  44179,  72653,\n",
            "         146800,  43647,  68158,  34370,  14925,    110,  54575, 145959,  54575,\n",
            "          72314,  91217,  54784,    224,  14925,    245,   5502,    120,  72653,\n",
            "          78368,  30484,    116,  23868, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6946.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ब्लॉग: जहां शादी के बाद पति अपनी पत्नी का सरनेम लेते हैं\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145799,  30484,    110,  12619,    231, 145959,\n",
            "             25,  14925,    250,  93948,  31411,    224,  14925,    114,  31411,\n",
            "             99,  43647,  47809,  34370,  14925,    105,  31411,     99,  83636,\n",
            "          79238,  38851,  14925,    227,  86162,  60096,  43647,  83636,  79238,\n",
            "          30484,    101,  43647,  47809,  23868,  68158,  44179,  60096,  54784,\n",
            "            106,  14925,    110,  54784,     97,  34370,  84310,  12619,    230,\n",
            "          72314, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6947.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जिन्होंने शौचालय न होने पर छोड़ दिया ससुराल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,  42311,    101,  30484,    117,  54575,\n",
            "          72314,  60096,  34370,  14925,    114,  12619,    234, 146113,  31411,\n",
            "            110, 145420,  14925,    101,  84310,  54575,  60096,  34370,  83636,\n",
            "          44179,  14925,    249,  54575, 146187,   5502,    120,  14925,     99,\n",
            "          42311,    107,  23868,  68158,  78368,  72653,  44179,  31411,    110,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6948.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>प्रताड़ना मामले में क्रिकेटर, पत्नी पर चार्जशीट\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  85033,  79238,  31411,     94,   5502,\n",
            "            120,  60096,  23868,  91217,  31411,    106,  91811,  34370,  91217,\n",
            "          54784,    224,  47809,  85033,  42311,    243,  54784,    253,  44179,\n",
            "             11,  83636,  79238,  30484,    101,  43647,  83636,  44179,  14925,\n",
            "            248,  31411,    108,  30484,    250, 145966,  43647, 145769, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6949.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस: 'हम चल नहीं सकते, देख नहीं सकते, लॉकडाउन में कैसे रहें? सोशल डिस्टेंसिंग कैसे करें?'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,     25,    364,  93948,\n",
            "          87244,  14925,    248,  91811,  14925,    101,  93948,  43647,  72314,\n",
            "          68158,  64704,  79238,  34370,     11,  14925,     99,  54784,    244,\n",
            "          14925,    101,  93948,  43647,  72314,  68158,  64704,  79238,  34370,\n",
            "             11,  14925,    110,  12619,    231,  64704, 146187,  31411,    231,\n",
            "          60096,  91217,  54784,    224,  47809,  12619,    230,  78368,  34370,\n",
            "          14925,    108,  93948,  54784,    224,     30,  68158,  54575, 145966,\n",
            "          91811,  14925,     94,  42311,    116,  30484,    253,  54784,    224,\n",
            "          78368,  42311,    224, 145959,  47809,  12619,    230,  78368,  34370,\n",
            "          47809,  44179,  54784,    224,  20224, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6950.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>शोएब और आसिफ़ का रास्ता साफ़\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145966,  54575, 146049, 145799,  14925,    242,\n",
            "          44179,  14925,    228,  78368,  42311,    104,   5502,    120,  47809,\n",
            "          23868,  14925,    108,  31411,    116,  30484,     97,  23868,  68158,\n",
            "          31411,    104,   5502,    120, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6951.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>चौथे लॉकडाउन में क्या खुलेगा, क्या बंद रहेगा, जानिए पूरा ब्योरा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146113,  12619,    234, 146489,  34370,  14925,\n",
            "            110,  12619,    231,  64704, 146187,  31411,    231,  60096,  91217,\n",
            "          54784,    224,  47809,  30484,    107,  23868,  14925,    244,  72653,\n",
            "          91811,  54784,    245,  23868,     11,  47809,  30484,    107,  23868,\n",
            "          14925,    105,  72314, 145256,  14925,    108,  93948,  54784,    245,\n",
            "          23868,     11,  14925,    250,  31411,    101,  42311,    237,  83636,\n",
            "          12619,    224,  44179,  23868,  14925,    105,  30484,    107,  54575,\n",
            "          44179,  23868, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6952.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'गांव उबारेंगे देश को आर्थिक बदहाली से'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 145959,  31411,    224, 145535,  14925,    231,\n",
            "         145799,  31411,    108,  54784,    224, 145959,  34370,  14925,     99,\n",
            "          54784,    114,  47809,  54575,  14925,    228,  44179,  30484,     98,\n",
            "          42311,    243,  14925,    105, 145256,  93948,  31411,    110,  43647,\n",
            "          68158,  34370,      6, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6953.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>उप-चुनाव: दिग्गजों की धमक ख़त्म, अब जनता की बारी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147181,  86162,     12, 146113,  72653,  60096,\n",
            "          31411,    113,     25,  14925,     99,  42311,    245,  30484,    245,\n",
            "         146114,  54575,  72314,  47809,  43647,  14925,    100,  87244,  64704,\n",
            "          14925,    244,   5502,    120,  79238,  30484,    106,     11,  14925,\n",
            "            227, 145799,  14925,    250,  60096,  79238,  23868,  47809,  43647,\n",
            "          14925,    105,  31411,    108,  43647, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6954.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>चीन की तरक्की की क़ीमत चुका रहे हैं वहां के लोग?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146113,  43647,  60096,  47809,  43647,  14925,\n",
            "             97,  44179,  64704,  30484,    243,  43647,  47809,  43647,  47809,\n",
            "           5502,    120,  43647,  87244,  79238,  14925,    248,  72653,  64704,\n",
            "          23868,  14925,    108,  93948,  34370,  84310,  12619,    230,  72314,\n",
            "          14925,    113,  93948,  31411,    224,  47809,  34370,  14925,    110,\n",
            "          54575, 145959,     30, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6955.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>सेक्स के लिए हज़ारों मील का सफ़र\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  78368,  54784,    243,  30484,    116,  47809,\n",
            "          34370,  14925,    110,  42311,    237,  84310, 146114,   5502,    120,\n",
            "          23868,  44179,  54575,  72314,  91217,  43647,  91811,  47809,  23868,\n",
            "          68158, 146934,   5502,    120,  44179, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6956.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ईदः भारत, पाकिस्तान, बांग्लादेश में कोरोना से फ़ीका पड़ा त्योहार\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147131, 145256,   5502,    225,  14925,    255,\n",
            "          31411,    108,  79238,     11,  83636,  31411,    243,  42311,    116,\n",
            "          30484,     97,  31411,    101,     11,  14925,    105,  31411,    224,\n",
            "         145959,  30484,    110,  31411,     99,  54784,    114,  91217,  54784,\n",
            "            224,  47809,  54575,  44179,  54575,  60096,  23868,  68158,  34370,\n",
            "          14925,    104,   5502,    120,  43647,  64704,  23868,  83636, 146187,\n",
            "           5502,    120,  23868,  14925,     97,  30484,    107,  54575,  93948,\n",
            "          31411,    108, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6957.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क़तर में शूरा काउंसिल की सिफ़ारिशें भारतीय कामगारों की मुश्किलें बढ़ाएगी?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,   5502,    120,  79238,  44179,  91217,\n",
            "          54784,    224,  14925,    114,  12619,    224,  44179,  23868,  47809,\n",
            "          31411,    231,  72314,  78368,  42311,    110,  47809,  43647,  68158,\n",
            "          42311,    104,   5502,    120,  23868,  44179,  42311,    114,  54784,\n",
            "            224,  14925,    255,  31411,    108,  79238,  43647, 145420,  47809,\n",
            "          31411,    106, 145959,  31411,    108,  54575,  72314,  47809,  43647,\n",
            "          91217,  72653, 145966,  30484,    243,  42311,    110,  54784,    224,\n",
            "          14925,    105, 149269,   5502,    120,  23868, 146049, 145959,  43647,\n",
            "             30, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6958.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जेईई मेन परीक्षा का रिज़ल्ट हुआ जारी, 24 छात्रों को 100 परसेंटाइल-आज की बड़ी ख़बरें\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,  54784,    230, 147131,  91217,  54784,\n",
            "            101,  83636,  44179,  43647,  64704,  30484,    115,  23868,  47809,\n",
            "          23868,  14925,    108,  42311,    250,   5502,    120,  91811,  30484,\n",
            "            253,  84310,  72653, 146399,  14925,    250,  31411,    108,  43647,\n",
            "             11,    220,     17,     19,  14925,    249,  31411,     97,  85033,\n",
            "          54575,  72314,  47809,  54575,    220,     16,     15,     15,  83636,\n",
            "          44179,  78368,  54784,    224, 145769,  31411,    229,  91811,     12,\n",
            "         146399, 146114,  47809,  43647,  14925,    105, 146187,   5502,    120,\n",
            "          43647,  14925,    244,   5502,    120, 145799,  44179,  54784,    224,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6959.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>एक डॉक्टर पर लगा मरीज़ों की हत्या का आरोप\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146049,  64704,  14925,     94,  12619,    231,\n",
            "          64704,  30484,    253,  44179,  83636,  44179,  14925,    110, 145959,\n",
            "          23868,  91217,  44179,  43647, 146114,   5502,    120,  54575,  72314,\n",
            "          47809,  43647,  84310,  79238,  30484,    107,  23868,  47809,  23868,\n",
            "          14925,    228,  44179,  54575,  86162, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6960.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अज़रबैजान के लिए कुछ भी करेंगे तुर्की के राष्ट्रपति अर्दोआन?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378, 146114,   5502,    120,  44179, 145799,\n",
            "          12619,    230, 146114,  31411,    101,  47809,  34370,  14925,    110,\n",
            "          42311,    237,  47809,  72653, 147877,  14925,    255,  43647,  47809,\n",
            "          44179,  54784,    224, 145959,  34370,  14925,     97,  72653,  44179,\n",
            "          30484,    243,  43647,  47809,  34370,  14925,    108,  31411,    115,\n",
            "          30484,    253,  85033,  86162,  79238,  38851,  14925,    227,  44179,\n",
            "          30484,     99,  54575, 146399,  60096,     30, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6961.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना लॉकडाउनः चाइल्ड पॉर्नोग्राफ़ी से जुड़े कंटेंट के डाउनलोड बढ़े\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    110,  12619,    231,  64704, 146187,  31411,    231,  60096,\n",
            "           5502,    225,  14925,    248,  31411,    229,  91811,  30484,     94,\n",
            "          83636,  12619,    231,  44179,  30484,    101,  54575, 145959,  85033,\n",
            "          31411,    104,   5502,    120,  43647,  68158,  34370,  14925,    250,\n",
            "          72653, 146187,   5502,    120,  34370,  47809,  72314, 145769,  54784,\n",
            "            224, 145769,  47809,  34370,  14925,     94,  31411,    231,  60096,\n",
            "          91811,  54575, 146187,  14925,    105, 149269,   5502,    120,  34370,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6962.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नज़रिया: अकेली मायावती को ही 'अपना खून' प्यारा नहीं है\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096, 146114,   5502,    120,  44179,  42311,\n",
            "            107,  23868,     25,  14925,    227,  64704,  54784,    110,  43647,\n",
            "          91217,  31411,    107,  31411,    113,  79238,  43647,  47809,  54575,\n",
            "          84310,  43647,    364, 146378,  86162,  60096,  23868,  14925,    244,\n",
            "          12619,    224,  60096,      6,  83636,  30484,    107,  31411,    108,\n",
            "          23868,  14925,    101,  93948,  43647,  72314,  84310,  12619,    230,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6963.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क्या मंगल ग्रह पर कभी ज़िंदगी थी?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  30484,    107,  23868,  91217,  72314,\n",
            "         145959,  91811,  14925,    245,  85033,  93948,  83636,  44179,  47809,\n",
            "         146101,  43647,  14925,    250,   5502,    120,  42311,    224, 145256,\n",
            "         145959,  43647,  14925,     98,  43647,     30, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6964.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>दुनिया के सबसे पुराने खाने का नुस्खा मिल गया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145256,  72653,  60096,  42311,    107,  23868,\n",
            "          47809,  34370,  68158, 145799,  78368,  34370,  83636,  72653,  44179,\n",
            "          31411,    101,  34370,  14925,    244,  31411,    101,  34370,  47809,\n",
            "          23868,  14925,    101,  72653,  78368,  30484,    244,  23868,  91217,\n",
            "          42311,    110,  14925,    245, 145420,  23868, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6965.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नागार्जुन के बाद आधुनिक हिंदी के सबसे लोकप्रिय कवि थे केदारनाथ\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096,  31411,    245,  31411,    108,  30484,\n",
            "            250,  72653,  60096,  47809,  34370,  14925,    105,  31411,     99,\n",
            "          14925,    228, 146821,  72653,  60096,  42311,    243,  84310,  42311,\n",
            "            224, 145256,  43647,  47809,  34370,  68158, 145799,  78368,  34370,\n",
            "          14925,    110,  54575,  64704,  86162,  85033,  42311,    107,  47809,\n",
            "         145535,  38851,  14925,     98,  34370,  47809,  54784,     99,  31411,\n",
            "            108,  60096,  31411,     98, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6966.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>वो देश जहां लोग नौकरी छोड़ने के लिए ख़र्च करते हैं पैसे\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145535,  54575,  14925,     99,  54784,    114,\n",
            "          14925,    250,  93948,  31411,    224,  14925,    110,  54575, 145959,\n",
            "          14925,    101,  12619,    234,  64704,  44179,  43647,  14925,    249,\n",
            "          54575, 146187,   5502,    120,  60096,  34370,  47809,  34370,  14925,\n",
            "            110,  42311,    237,  14925,    244,   5502,    120,  44179,  30484,\n",
            "            248,  47809,  44179,  79238,  34370,  84310,  12619,    230,  72314,\n",
            "          83636,  12619,    230,  78368,  34370, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6967.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>मोदी के बाद लपेटे में आई केन्द्र सरकार तो चुप्पी क्यों?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  54575, 145256,  43647,  47809,  34370,\n",
            "          14925,    105,  31411,     99,  14925,    110,  86162,  54784,    253,\n",
            "          34370,  91217,  54784,    224,  14925,    228, 147131,  47809,  54784,\n",
            "            101,  30484,     99,  85033,  68158,  44179,  64704,  31411,    108,\n",
            "          14925,     97,  54575,  14925,    248,  72653,  86162,  30484,    103,\n",
            "          43647,  47809,  30484,    107,  54575,  72314,     30, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6968.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>भारतीयों की 'टैक्स चोरी': चार सवाल\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146101,  31411,    108,  79238,  43647, 145420,\n",
            "          54575,  72314,  47809,  43647,    364, 145769,  12619,    230,  64704,\n",
            "          30484,    116,  14925,    248,  54575,  44179,  43647,   1210,  14925,\n",
            "            248,  31411,    108,  68158, 145535,  31411,    110, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6969.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>आधी सज़ा काट चुके अभियुक्त रिहा हों: सुप्रीम कोर्ट\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146399, 146821,  43647,  68158, 146114,   5502,\n",
            "            120,  23868,  47809,  31411,    253,  14925,    248,  72653,  64704,\n",
            "          34370,  14925,    227, 146101,  42311,    107,  72653,  64704,  30484,\n",
            "             97,  14925,    108,  42311,    117,  23868,  84310,  54575,  72314,\n",
            "             25,  68158,  72653,  86162,  85033,  43647,  87244,  47809,  54575,\n",
            "          44179,  30484,    253, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6970.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>लोकसभा चुनाव 2019: पश्चिम बंगाल में उधार के उम्मीदवारों के भरोसे बीजेपी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  91811,  54575,  64704,  78368, 146101,  23868,\n",
            "          14925,    248,  72653,  60096,  31411,    113,    220,     17,     15,\n",
            "             16,     24,     25,  83636, 145966,  30484,    248,  42311,    106,\n",
            "          14925,    105,  72314, 145959,  31411,    110,  91217,  54784,    224,\n",
            "          14925,    231, 146821,  31411,    108,  47809,  34370,  14925,    231,\n",
            "          87244,  30484,    106,  43647, 145256, 145535,  31411,    108,  54575,\n",
            "          72314,  47809,  34370,  14925,    255,  44179,  54575,  78368,  34370,\n",
            "          14925,    105,  43647, 146114,  54784,    103,  43647, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6971.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>मैच फिक्सिंग की दास्ताँ; कब, क्या हुआ...\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  12619,    230, 146113,  14925,    104,\n",
            "          42311,    243,  30484,    116,  42311,    224, 145959,  47809,  43647,\n",
            "          14925,     99,  31411,    116,  30484,     97,  31411,    223,     26,\n",
            "          47809, 145799,     11,  47809,  30484,    107,  23868,  84310,  72653,\n",
            "         146399,   1112, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6972.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>क़र्ज़ में डूबा पाकिस्तान अंतरिक्ष में क्या दे पाएगा भारत को टक्कर?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,   5502,    120,  44179,  30484,    250,\n",
            "           5502,    120,  91217,  54784,    224,  14925,     94,  12619,    224,\n",
            "         145799,  23868,  83636,  31411,    243,  42311,    116,  30484,     97,\n",
            "          31411,    101,  14925,    227,  72314,  79238,  44179,  42311,    243,\n",
            "          30484,    115,  91217,  54784,    224,  47809,  30484,    107,  23868,\n",
            "          14925,     99,  34370,  83636,  31411,    237, 145959,  23868,  14925,\n",
            "            255,  31411,    108,  79238,  47809,  54575,  14925,    253,  64704,\n",
            "          30484,    243,  44179,     30, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6973.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अमरीका में प्रदर्शनकारियों को अपने घर में पनाह देने वाले राहुल दुबे कौन हैं?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  87244,  44179,  43647,  64704,  23868,\n",
            "          91217,  54784,    224,  83636,  85033, 145256,  44179,  30484,    114,\n",
            "          60096,  64704,  31411,    108,  42311,    107,  54575,  72314,  47809,\n",
            "          54575,  14925,    227,  86162,  60096,  34370,  14925,    246,  44179,\n",
            "          91217,  54784,    224,  83636,  60096,  31411,    117,  14925,     99,\n",
            "          54784,    101,  34370,  14925,    113,  31411,    110,  34370,  14925,\n",
            "            108,  31411,    117,  72653,  91811,  14925,     99,  72653, 145799,\n",
            "          34370,  47809,  12619,    234,  60096,  84310,  12619,    230,  72314,\n",
            "             30, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6974.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>केजीबी अधिकारी के आरोपों का खंडन\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54784,    250,  43647, 145799,  43647,\n",
            "          14925,    227, 146821,  42311,    243,  31411,    108,  43647,  47809,\n",
            "          34370,  14925,    228,  44179,  54575,  86162,  54575,  72314,  47809,\n",
            "          23868,  14925,    244,  72314, 146187,  60096, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6975.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>1965 युद्ध: न भारत जीता, न पाकिस्तान हारा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,     16,     24,     21,     20,  14925,    107,\n",
            "          72653, 145256,  30484,    100,     25,  14925,    101,  14925,    255,\n",
            "          31411,    108,  79238,  14925,    250,  43647,  79238,  23868,     11,\n",
            "          14925,    101,  83636,  31411,    243,  42311,    116,  30484,     97,\n",
            "          31411,    101,  84310,  31411,    108,  23868, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6976.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ऐपल, अमेज़न, फेसबुक और गूगल कठघरे में, जानें क्या है पूरा मामला\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 149524,  86162,  91811,     11,  14925,    227,\n",
            "          87244,  54784,    250,   5502,    120,  60096,     11,  14925,    104,\n",
            "          54784,    116, 145799,  72653,  64704,  14925,    242,  44179,  14925,\n",
            "            245,  12619,    224, 145959,  91811,  47809, 146826, 147676,  44179,\n",
            "          34370,  91217,  54784,    224,     11,  14925,    250,  31411,    101,\n",
            "          54784,    224,  47809,  30484,    107,  23868,  84310,  12619,    230,\n",
            "          83636,  12619,    224,  44179,  23868,  91217,  31411,    106,  91811,\n",
            "          23868, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6977.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अमरीकी राष्ट्रपति चुनाव: मिट रोमनी पर भारी पड़े ओबामा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  87244,  44179,  43647,  64704,  43647,\n",
            "          14925,    108,  31411,    115,  30484,    253,  85033,  86162,  79238,\n",
            "          38851,  14925,    248,  72653,  60096,  31411,    113,     25,  91217,\n",
            "          42311,    253,  14925,    108,  54575,  87244,  60096,  43647,  83636,\n",
            "          44179,  14925,    255,  31411,    108,  43647,  83636, 146187,   5502,\n",
            "            120,  34370,  14925,    241, 145799,  31411,    106,  23868,     30,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6978.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>गर्मी की मार से चिकन महंगा\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145959,  44179,  30484,    106,  43647,  47809,\n",
            "          43647,  91217,  31411,    108,  68158,  34370,  14925,    248,  42311,\n",
            "            243,  60096,  91217,  93948,  72314, 145959,  23868, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6979.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पश्तो की पहली फ़िल्म कराची या काबुल में नहीं बॉम्बे में बनी थी\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162, 145966,  30484,     97,  54575,  47809,\n",
            "          43647,  83636,  93948,  91811,  43647,  14925,    104,   5502,    120,\n",
            "          38851,  91811,  30484,    106,  47809,  44179,  31411,    248,  43647,\n",
            "          14925,    107,  23868,  47809,  31411,    105,  72653,  91811,  91217,\n",
            "          54784,    224,  14925,    101,  93948,  43647,  72314,  14925,    105,\n",
            "          12619,    231,  87244,  30484,    105,  34370,  91217,  54784,    224,\n",
            "          14925,    105,  60096,  43647,  14925,     98,  43647, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6980.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>एक सपने का चकनाचूर होना....\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146049,  64704,  68158,  86162,  60096,  34370,\n",
            "          47809,  23868,  14925,    248,  64704,  60096,  31411,    248,  12619,\n",
            "            224,  44179,  84310,  54575,  60096,  23868,   1934, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6981.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>यूरोपीय देशों में क्यों है केन्या के शहद की मांग?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145420,  12619,    224,  44179,  54575,  86162,\n",
            "          43647, 145420,  14925,     99,  54784,    114,  54575,  72314,  91217,\n",
            "          54784,    224,  47809,  30484,    107,  54575,  72314,  84310,  12619,\n",
            "            230,  47809,  54784,    101,  30484,    107,  23868,  47809,  34370,\n",
            "          14925,    114,  93948, 145256,  47809,  43647,  91217,  31411,    224,\n",
            "         145959,     30, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6982.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>चुनाव के विभिन्न चरण, अहम राज्यों की स्थिति\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146113,  72653,  60096,  31411,    113,  47809,\n",
            "          34370,  14925,    113,  42311,    255,  42311,    101,  30484,    101,\n",
            "          14925,    248,  44179, 146548,     11,  14925,    227,  93948,  87244,\n",
            "          14925,    108,  31411,    250,  30484,    107,  54575,  72314,  47809,\n",
            "          43647,  68158,  30484,     98,  42311,     97,  38851, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6983.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'जिन्हें एक हस्ताक्षर पर मिलते हैं हज़ारों करोड़'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 146114,  42311,    101,  30484,    117,  54784,\n",
            "            224,  14925,    237,  64704,  84310,  78368,  30484,     97,  31411,\n",
            "            243,  30484,    115,  44179,  83636,  44179,  91217,  42311,    110,\n",
            "          79238,  34370,  84310,  12619,    230,  72314,  84310, 146114,   5502,\n",
            "            120,  23868,  44179,  54575,  72314,  47809,  44179,  54575, 146187,\n",
            "           5502,    120,      6, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6984.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'भारत-पाकिस्तान सीरिज़ हर वर्ष होगी'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 146101,  31411,    108,  79238,     12,  86162,\n",
            "          31411,    243,  42311,    116,  30484,     97,  31411,    101,  68158,\n",
            "          43647,  44179,  42311,    250,   5502,    120,  84310,  44179,  14925,\n",
            "            113,  44179,  30484,    115,  84310,  54575, 145959,  43647,      6,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6985.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>जम्मू में रह रहे रोहिंग्या मुसलमान एकाएक क्यों आए पुलिस के निशाने पर- ग्राउंड रिपोर्ट\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146114,  87244,  30484,    106,  12619,    224,\n",
            "          91217,  54784,    224,  14925,    108,  93948,  14925,    108,  93948,\n",
            "          34370,  14925,    108,  54575,  93948,  42311,    224, 145959,  30484,\n",
            "            107,  23868,  91217,  72653,  78368,  91811,  87244,  31411,    101,\n",
            "          14925,    237,  64704,  31411,    237,  64704,  47809,  30484,    107,\n",
            "          54575,  72314,  14925,    228, 146049,  83636,  72653,  91811,  42311,\n",
            "            116,  47809,  34370,  14925,    101,  42311,    114,  31411,    101,\n",
            "          34370,  83636,  44179,     12,  14925,    245,  85033,  31411,    231,\n",
            "          72314, 146187,  14925,    108,  42311,    103,  54575,  44179,  30484,\n",
            "            253, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6986.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>आत्मघाती हमलावरों का पुलिस पर हमला\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146399,  79238,  30484,    106, 147676,  31411,\n",
            "             97,  43647,  84310,  87244,  91811,  31411,    113,  44179,  54575,\n",
            "          72314,  47809,  23868,  83636,  72653,  91811,  42311,    116,  83636,\n",
            "          44179,  84310,  87244,  91811,  23868, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6987.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अमरीकी चुनाव: ट्रंप की जीत के लिए महिलाएं क्यों हैं ज़रूरी?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378,  87244,  44179,  43647,  64704,  43647,\n",
            "          14925,    248,  72653,  60096,  31411,    113,     25,  14925,    253,\n",
            "          85033,  72314,  86162,  47809,  43647,  14925,    250,  43647,  79238,\n",
            "          47809,  34370,  14925,    110,  42311,    237,  91217,  93948,  42311,\n",
            "            110,  31411,    237,  72314,  47809,  30484,    107,  54575,  72314,\n",
            "          84310,  12619,    230,  72314,  14925,    250,   5502,    120,  44179,\n",
            "          12619,    224,  44179,  43647,     30, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6988.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>प्रियंका गांधी के मामले में आक्रामक क्यों हो जाती है योगी सरकार\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  85033,  42311,    107,  72314,  64704,\n",
            "          23868,  14925,    245,  31411,    224, 146821,  43647,  47809,  34370,\n",
            "          91217,  31411,    106,  91811,  34370,  91217,  54784,    224,  14925,\n",
            "            228,  64704,  85033,  31411,    106,  64704,  47809,  30484,    107,\n",
            "          54575,  72314,  84310,  54575,  14925,    250,  31411,     97,  43647,\n",
            "          84310,  12619,    230,  14925,    107,  54575, 145959,  43647,  68158,\n",
            "          44179,  64704,  31411,    108, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6989.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>अब तक अनसुलझा है अमेज़न नदी का रहस्य\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 146378, 145799,  14925,     97,  64704,  14925,\n",
            "            227,  60096,  78368,  72653,  91811, 148971,  23868,  84310,  12619,\n",
            "            230,  14925,    227,  87244,  54784,    250,   5502,    120,  60096,\n",
            "          14925,    101, 145256,  43647,  47809,  23868,  14925,    108,  93948,\n",
            "          78368,  30484,    107, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6990.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>मिथुन चक्रवर्ती ने ख़ुद को 'मोदी भक्त' बोले जाने पर क्या कहा?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  87244,  42311,     98,  72653,  60096,  14925,\n",
            "            248,  64704,  85033, 145535,  44179,  30484,     97,  43647,  14925,\n",
            "            101,  34370,  14925,    244,   5502,    120,  72653, 145256,  47809,\n",
            "          54575,    364,  87244,  54575, 145256,  43647,  14925,    255,  64704,\n",
            "          30484,     97,      6,  14925,    105,  54575,  91811,  34370,  14925,\n",
            "            250,  31411,    101,  34370,  83636,  44179,  47809,  30484,    107,\n",
            "          23868,  47809,  93948,  23868,     30, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6991.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'भूत' बनना रोज़ी-रोटी का ज़रिया\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 146101,  12619,    224,  79238,      6,  14925,\n",
            "            105,  60096,  60096,  23868,  14925,    108,  54575, 146114,   5502,\n",
            "            120,  43647,     12,  44179,  54575, 145769,  43647,  47809,  23868,\n",
            "          14925,    250,   5502,    120,  44179,  42311,    107,  23868, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6992.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>घर लौटा परदेसी, देस बुलाए रे\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 147676,  44179,  14925,    110,  12619,    234,\n",
            "         145769,  23868,  83636,  44179, 145256,  54784,    116,  43647,     11,\n",
            "          14925,     99,  54784,    116,  14925,    105,  72653,  91811,  31411,\n",
            "            237,  14925,    108,  34370, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6993.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नेल्सन मंडेला ने जब रंगभेद के दौर में कँटीली बाड़ से देखा था टेस्ट मैच\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096,  54784,    110,  30484,    116,  60096,\n",
            "          91217,  72314, 146187,  54784,    110,  23868,  14925,    101,  34370,\n",
            "          14925,    250, 145799,  14925,    108,  72314, 145959, 146101,  54784,\n",
            "             99,  47809,  34370,  14925,     99,  12619,    234,  44179,  91217,\n",
            "          54784,    224,  47809,   5502,    223, 145769,  43647,  91811,  43647,\n",
            "          14925,    105,  31411,     94,   5502,    120,  68158,  34370,  14925,\n",
            "             99,  54784,    244,  23868,  14925,     98,  23868,  14925,    253,\n",
            "          54784,    116,  30484,    253,  91217,  12619,    230, 146113, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6994.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कश्मीर: उत्पीड़न के आरोप, सेना का इनकार\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704, 145966,  30484,    106,  43647,  44179,\n",
            "             25,  14925,    231,  79238,  30484,    103,  43647, 146187,   5502,\n",
            "            120,  60096,  47809,  34370,  14925,    228,  44179,  54575,  86162,\n",
            "             11,  68158,  54784,    101,  23868,  47809,  23868,  14925,    229,\n",
            "          60096,  64704,  31411,    108, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6995.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>देश के हर ज़िले में होगा महिला बैंक: उषा अनंतसुब्रमण्यन\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145256,  54784,    114,  47809,  34370,  84310,\n",
            "          44179,  14925,    250,   5502,    120,  38851,  91811,  34370,  91217,\n",
            "          54784,    224,  84310,  54575, 145959,  23868,  91217,  93948,  42311,\n",
            "            110,  23868,  14925,    105,  12619,    230,  72314,  64704,     25,\n",
            "          14925,    231, 146352,  23868,  14925,    227,  60096,  72314,  79238,\n",
            "          78368,  72653, 145799,  85033,  87244, 146548,  30484,    107,  60096,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6996.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>नज़रियाः कर्नाटक में चलेगा मोदी का जादू या सिद्धारमैया का 'भाग्य'?\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  60096, 146114,   5502,    120,  44179,  42311,\n",
            "            107,  31411,    225,  47809,  44179,  30484,    101,  31411,    253,\n",
            "          64704,  91217,  54784,    224,  14925,    248,  91811,  54784,    245,\n",
            "          23868,  91217,  54575, 145256,  43647,  47809,  23868,  14925,    250,\n",
            "          31411,     99,  12619,    224,  14925,    107,  23868,  68158,  42311,\n",
            "             99,  30484,    100,  31411,    108,  87244,  12619,    230, 145420,\n",
            "          23868,  47809,  23868,    364, 146101,  31411,    245,  30484,    107,\n",
            "          69990, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6997.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>'आज तस्वीरों की जगह कचरा छप रहा है'\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,   5592, 146399, 146114,  14925,     97,  78368,  30484,\n",
            "            113,  43647,  44179,  54575,  72314,  47809,  43647,  14925,    250,\n",
            "         145959,  93948,  47809, 146113,  44179,  23868,  14925,    249,  86162,\n",
            "          14925,    108,  93948,  23868,  84310,  12619,    230,      6, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6998.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>पाकिस्तान: नाबालिग़ ईसाई लड़की के कथित जबरन धर्म परिवर्तन और शादी का मामला\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  86162,  31411,    243,  42311,    116,  30484,\n",
            "             97,  31411,    101,     25,  14925,    101,  31411,    105,  31411,\n",
            "            110,  42311,    245,   5502,    120,  14925,    230,  78368,  31411,\n",
            "            230,  14925,    110, 146187,   5502,    120,  64704,  43647,  47809,\n",
            "          34370,  47809, 146489,  42311,     97,  14925,    250, 145799,  44179,\n",
            "          60096,  14925,    100,  44179,  30484,    106,  83636,  44179,  42311,\n",
            "            113,  44179,  30484,     97,  60096,  14925,    242,  44179,  14925,\n",
            "            114,  31411,     99,  43647,  47809,  23868,  91217,  31411,    106,\n",
            "          91811,  23868, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 6999.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>ट्विटर पर छाया 'केजरीवाल पर गर्व' है\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29, 145769,  30484,    113,  42311,    253,  44179,\n",
            "          83636,  44179,  14925,    249,  31411,    107,  23868,    364,  64704,\n",
            "          54784,    250,  44179,  43647, 145535,  31411,    110,  83636,  44179,\n",
            "          14925,    245,  44179,  30484,    113,      6,  84310,  12619,    230,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Processing image: 7000.png\n",
            "Resized image size: (900, 64)\n",
            "Image tensor shape: torch.Size([1, 3, 64, 900])\n",
            "Formatted text for processing: <image>कोरोना वायरस: संकट में फंसा एयर इंडिया कैसे बना संकटमोचक\n",
            "Processed text data: {'input_ids': tensor([[    27,   1805,     29,  64704,  54575,  44179,  54575,  60096,  23868,\n",
            "          14925,    113,  31411,    107,  44179,  78368,     25,  68158,  72314,\n",
            "          64704, 145769,  91217,  54784,    224,  14925,    104,  72314,  78368,\n",
            "          23868,  14925,    237, 145420,  44179,  14925,    229,  72314, 146187,\n",
            "          42311,    107,  23868,  47809,  12619,    230,  78368,  34370,  14925,\n",
            "            105,  60096,  23868,  68158,  72314,  64704, 145769,  87244,  54575,\n",
            "         146113,  64704, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643, 151643,\n",
            "         151643]]), 'token_type_ids': tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "         1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "         0, 0, 0, 0]])}\n",
            "Batch processing complete: 1000 images processed.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision.transforms import ToTensor\n",
        "import numpy as np\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    pixel_values_batch = []\n",
        "    labels_batch = []\n",
        "\n",
        "    # Define a maximum length for the text (adjust based on your dataset)\n",
        "    max_length = 100  # Adjust this value based on the average length of your texts\n",
        "\n",
        "    # Ensure the batch contains data\n",
        "    if not examples['image_file']:\n",
        "        print(\"No image files provided.\")\n",
        "        return\n",
        "\n",
        "    print(f\"Processing {len(examples['image_file'])} examples...\")  # Track batch size\n",
        "\n",
        "    # Loop through the batch of images and texts\n",
        "    for image_file, text in zip(examples['image_file'], examples['text']):\n",
        "        try:\n",
        "            # Check image path\n",
        "            print(f\"Processing image: {image_file}\")\n",
        "\n",
        "            image_path = os.path.join(image_folder_path, image_file)\n",
        "            if not os.path.exists(image_path):\n",
        "                print(f\"Error: Image file {image_file} not found.\")\n",
        "                continue\n",
        "\n",
        "            image = Image.open(image_path)\n",
        "\n",
        "            # Ensure the image is a PIL Image, convert from ndarray if needed\n",
        "            if isinstance(image, np.ndarray):\n",
        "                print(f\"Converting numpy array to PIL image.\")\n",
        "                image = Image.fromarray(image)\n",
        "\n",
        "            # Resize image and convert to tensor\n",
        "            image = image.convert(\"RGB\").resize((900, 64))  # Resize to fixed dimensions\n",
        "            print(f\"Resized image size: {image.size}\")\n",
        "            image_tensor = ToTensor()(image).unsqueeze(0)\n",
        "            print(f\"Image tensor shape: {image_tensor.shape}\")\n",
        "\n",
        "            # Format the text to include <image> placeholder\n",
        "            formatted_text = f\"<image>{text}\"\n",
        "            print(f\"Formatted text for processing: {formatted_text}\")\n",
        "\n",
        "            # Process the text using the processor with padding and truncation to max_length\n",
        "            processed_data = processor(\n",
        "                text=formatted_text,\n",
        "                return_tensors=\"pt\",\n",
        "                padding=\"max_length\",  # Pad the text to max_length\n",
        "                truncation=True,       # Truncate texts longer than max_length\n",
        "                max_length=max_length  # Set the max length for padding/truncation\n",
        "            )\n",
        "            print(f\"Processed text data: {processed_data}\")\n",
        "\n",
        "            # Ensure `input_ids` are present\n",
        "            if 'input_ids' not in processed_data:\n",
        "                print(f\"Error: input_ids not found for {image_file}\")\n",
        "                continue\n",
        "\n",
        "            # Append processed data to batches\n",
        "            pixel_values_batch.append(image_tensor)\n",
        "            labels_batch.append(processed_data['input_ids'])\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing image {image_file}: {e}\")\n",
        "            continue\n",
        "\n",
        "    # Check if any data was processed\n",
        "    if len(pixel_values_batch) == 0 or len(labels_batch) == 0:\n",
        "        print(\"No valid images or texts processed in this batch.\")\n",
        "        return\n",
        "\n",
        "    # Return the batch of image tensors and text labels\n",
        "    print(f\"Batch processing complete: {len(pixel_values_batch)} images processed.\")\n",
        "    return {\n",
        "        'pixel_values': torch.cat(pixel_values_batch, dim=0),  # Concatenate all image tensors\n",
        "        'labels': torch.cat(labels_batch, dim=0)  # Concatenate all label tensors\n",
        "    }\n",
        "\n",
        "# Example usage (make sure to call this function properly in your dataset processing pipeline)\n",
        "processed_dataset = dataset.map(preprocess_function, batched=True)\n",
        "\n",
        "# Set the format for PyTorch\n",
        "processed_dataset.set_format(type=\"torch\")\n",
        "\n",
        "# View the first processed batch\n",
        "print(processed_dataset[0])\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply the preprocessing function to the dataset (batch processing)\n",
        "processed_dataset = dataset.map(\n",
        "    lambda examples: preprocess_function(examples['image_file'], examples['text']),\n",
        "    batched=True\n",
        ")\n",
        "\n",
        "# Set the format for PyTorch\n",
        "processed_dataset.set_format(type=\"torch\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "156e520b47a94cd88caaec362a37a29e",
            "3eef6f28cecb4f219c7832209c16fbb1",
            "dc6aee75a01e4e02b2ed6bf4b3232dd5",
            "24b2445e05c245c29b584bfc0ff4dbeb",
            "be11b69bffb44e26ba89f4726f688b9e",
            "8dc6b5cf1d8140dcb91c64dc8551222e",
            "e5c608d16b0d409ab9f362162b38b5b2",
            "70d804e124c941018ab2a5ee0b113b91",
            "d969e33bfaf64000b2f8d708460e4ce9",
            "01b2039530764232918c3cffd1bf914f",
            "77fcb1352f0f41f590580baee90a07b5"
          ]
        },
        "id": "1c2pBSpmXWul",
        "outputId": "3eab3e2c-723a-448a-f959-85f1eddefc66"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Map:   0%|          | 0/80000 [00:00<?, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "156e520b47a94cd88caaec362a37a29e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n",
            "Error processing image: 'list' object has no attribute 'size'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wn6ahnvFKBDP",
        "outputId": "f79b345b-f5d6-44b3-8516-af43b4c82e26"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gradio in /usr/local/lib/python3.10/dist-packages (4.44.0)\n",
            "Requirement already satisfied: aiofiles<24.0,>=22.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (23.2.1)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: fastapi<1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.115.0)\n",
            "Requirement already satisfied: ffmpy in /usr/local/lib/python3.10/dist-packages (from gradio) (0.4.0)\n",
            "Requirement already satisfied: gradio-client==1.3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.3.0)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.27.2)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.24.7)\n",
            "Requirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.4.5)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.1.4)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.5)\n",
            "Requirement already satisfied: matplotlib~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.7.1)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (3.10.7)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from gradio) (24.1)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.1.4)\n",
            "Requirement already satisfied: pillow<11.0,>=8.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (10.4.0)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.9.2)\n",
            "Requirement already satisfied: pydub in /usr/local/lib/python3.10/dist-packages (from gradio) (0.25.1)\n",
            "Requirement already satisfied: python-multipart>=0.0.9 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.0.12)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (6.0.2)\n",
            "Requirement already satisfied: ruff>=0.2.2 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.6.8)\n",
            "Requirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.10.0)\n",
            "Requirement already satisfied: tomlkit==0.12.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.0)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.12.5)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (4.12.2)\n",
            "Requirement already satisfied: urllib3~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (2.2.3)\n",
            "Requirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.10/dist-packages (from gradio) (0.31.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (2024.6.1)\n",
            "Requirement already satisfied: websockets<13.0,>=10.0 in /usr/local/lib/python3.10/dist-packages (from gradio-client==1.3.0->gradio) (12.0)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5.0,>=3.0->gradio) (1.2.2)\n",
            "Requirement already satisfied: starlette<0.39.0,>=0.37.2 in /usr/local/lib/python3.10/dist-packages (from fastapi<1.0->gradio) (0.38.6)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (2024.8.30)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.10/dist-packages (from httpx>=0.24.1->gradio) (1.0.5)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.10/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (3.16.1)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.3->gradio) (4.66.5)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (1.4.7)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib~=3.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic>=2.0->gradio) (2.23.4)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0,>=0.12->gradio) (13.8.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib~=3.0->gradio) (1.16.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.19.3->gradio) (3.3.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import gradio as gr\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "# Huggingface token (ensure it's declared earlier in your script)\n",
        "huggingface_token = \"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\"\n",
        "\n",
        "# Load model and tokenizer function\n",
        "def load_model_and_tokenizer():\n",
        "    # Load the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\n",
        "        'ucaslcl/GOT-OCR2_0',\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=huggingface_token\n",
        "    )\n",
        "\n",
        "    # Check if CUDA (GPU) is available, otherwise use CPU\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Load the model\n",
        "    model = AutoModel.from_pretrained(\n",
        "        'ucaslcl/GOT-OCR2_0',\n",
        "        trust_remote_code=True,\n",
        "        use_auth_token=huggingface_token\n",
        "    ).to(device)  # Move model to the appropriate device (GPU/CPU)\n",
        "\n",
        "    return model, tokenizer, device\n",
        "\n",
        "# Load model, tokenizer, and device\n",
        "model, tokenizer, device = load_model_and_tokenizer()\n",
        "\n",
        "# OCR Inference Function\n",
        "def ocr_inference(image):\n",
        "    try:\n",
        "        # Convert the uploaded image (PIL) to a numpy array\n",
        "        image_array = np.array(image)  # Convert PIL Image to numpy array\n",
        "\n",
        "        # Preprocess the image\n",
        "        text = ''  # May not have text initially\n",
        "        preprocessed_data = preprocess_function(image_array, text)  # Pass image_array and text\n",
        "\n",
        "        if preprocessed_data is None:\n",
        "            return \"Error processing image.\"\n",
        "\n",
        "        # Retrieve input_ids and attention_mask from preprocessed data\n",
        "        input_ids = preprocessed_data['input_ids']\n",
        "        attention_mask = preprocessed_data['attention_mask']\n",
        "\n",
        "        # Check if input_ids is None\n",
        "        if input_ids is None:\n",
        "            return \"Error: input_ids are None.\"\n",
        "\n",
        "        # Pass input_ids and attention_mask to the OCR model (the correct arguments)\n",
        "        outputs = model(input_ids=input_ids, attention_mask=attention_mask)  # Ensure the model expects these parameters\n",
        "\n",
        "        # Extract and return the text results\n",
        "        extracted_text = outputs.get('text', \"No text extracted.\")  # Use .get to avoid KeyError\n",
        "        return extracted_text\n",
        "\n",
        "    except Exception as e:\n",
        "        return f\"An error occurred: {str(e)}\"\n",
        "\n",
        "# Create Gradio interface\n",
        "iface = gr.Interface(fn=ocr_inference, inputs=gr.Image(type=\"pil\"), outputs=\"text\", title=\"OCR Application\", description=\"Upload an image containing text in Hindi or English.\")\n",
        "iface.launch()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 715
        },
        "id": "FBhujAw9Iqx5",
        "outputId": "9d518117-9fbb-41dd-87aa-784fe2a7f9ed"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/tokenization_auto.py:786: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/models/auto/auto_factory.py:469: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "Running on public URL: https://693dda9875fb73b28a.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://693dda9875fb73b28a.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!gradio deploy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NOtQvMrcHqQa",
        "outputId": "90c82df1-f249-46e9-a2b0-0f3a2e6d884e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Need \u001b[32m'write'\u001b[0m access token to create a Spaces repo.\n",
            "\n",
            "    _|    _|  _|    _|    _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|_|_|_|    _|_|      _|_|_|  _|_|_|_|\n",
            "    _|    _|  _|    _|  _|        _|          _|    _|_|    _|  _|            _|        _|    _|  _|        _|\n",
            "    _|_|_|_|  _|    _|  _|  _|_|  _|  _|_|    _|    _|  _|  _|  _|  _|_|      _|_|_|    _|_|_|_|  _|        _|_|_|\n",
            "    _|    _|  _|    _|  _|    _|  _|    _|    _|    _|    _|_|  _|    _|      _|        _|    _|  _|        _|\n",
            "    _|    _|    _|_|      _|_|_|    _|_|_|  _|_|_|  _|      _|    _|_|_|      _|        _|    _|    _|_|_|  _|_|_|_|\n",
            "\n",
            "    A token is already saved on your machine. Run `huggingface-cli whoami` to get more information or `huggingface-cli logout` if you want to log out.\n",
            "    Setting a new token will erase the existing one.\n",
            "    To login, `huggingface_hub` requires a token generated from https://huggingface.co/settings/tokens .\n",
            "Enter your token (input will not be visible): \n",
            "Add token as git credential? (Y/n) y\n",
            "Token is valid (permission: fineGrained).\n",
            "\u001b[1m\u001b[31mCannot authenticate through git-credential as no helper is defined on your machine.\n",
            "You might have to re-authenticate when pushing to the Hugging Face Hub.\n",
            "Run the following command in your terminal in case you want to set the 'store' credential helper as default.\n",
            "\n",
            "git config --global credential.helper store\n",
            "\n",
            "Read https://git-scm.com/book/en/v2/Git-Tools-Credential-Storage for more details.\u001b[0m\n",
            "Token has not been saved to git credential helper.\n",
            "Your token has been saved to /root/.cache/huggingface/token\n",
            "Login successful\n",
            "Creating new Spaces Repo in \u001b[32m'/content'\u001b[0m. Collecting metadata, press Enter to accept default value.\n",
            "Enter Spaces app title [content]: OCR HINDI ENGLISH\n",
            "Formatted to OCR_HINDI_ENGLISH. \n",
            "Enter Gradio app file : \n",
            "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/gradio/cli/commands/\u001b[0m\u001b[1;33mdeploy_space.py\u001b[0m:\u001b[94m157\u001b[0m in \u001b[92mdeploy\u001b[0m        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m154 \u001b[0m\u001b[2m│   │   \u001b[0m\u001b[96mprint\u001b[0m(                                                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m155 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33mCreating new Spaces Repo in \u001b[0m\u001b[33m'\u001b[0m\u001b[33m{\u001b[0mrepo_directory\u001b[33m}\u001b[0m\u001b[33m'\u001b[0m\u001b[33m. Collecting metadata, press\u001b[0m   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m156 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m157 \u001b[2m│   │   \u001b[0mconfiguration = add_configuration_to_readme(                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m158 \u001b[0m\u001b[2m│   │   │   \u001b[0mtitle,                                                                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m159 \u001b[0m\u001b[2m│   │   │   \u001b[0mapp_file,                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m160 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m─────────────────────────────────────\u001b[0m\u001b[33m─╮\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m      app_file = \u001b[94mNone\u001b[0m                                                              \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m configuration = \u001b[94mNone\u001b[0m                                                              \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        hf_api = \u001b[1m<\u001b[0m\u001b[1;95mhuggingface_hub.hf_api.HfApi\u001b[0m\u001b[39m object at \u001b[0m\u001b[94m0x780887e99720\u001b[0m\u001b[1m>\u001b[0m           \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         login = \u001b[94mTrue\u001b[0m                                                              \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m         title = \u001b[94mNone\u001b[0m                                                              \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        whoami = \u001b[1m{\u001b[0m                                                                 \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'type'\u001b[0m: \u001b[33m'user'\u001b[0m,                                               \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'id'\u001b[0m: \u001b[33m'66f41faa78d3e2d74a214995'\u001b[0m,                             \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'name'\u001b[0m: \u001b[33m'akankshachandra'\u001b[0m,                                    \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'fullname'\u001b[0m: \u001b[33m'Akanksha Chandra'\u001b[0m,                               \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'canPay'\u001b[0m: \u001b[94mFalse\u001b[0m,                                              \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'periodEnd'\u001b[0m: \u001b[94mNone\u001b[0m,                                            \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'isPro'\u001b[0m: \u001b[94mFalse\u001b[0m,                                               \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'avatarUrl'\u001b[0m: \u001b[33m'/avatars/d5a6cb4d52bc2700fc24b9756d6e5fd1.svg'\u001b[0m, \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'orgs'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,                                                   \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[33m'auth'\u001b[0m: \u001b[1m{\u001b[0m                                                     \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   \u001b[0m\u001b[33m'type'\u001b[0m: \u001b[33m'access_token'\u001b[0m,                                   \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   \u001b[0m\u001b[33m'accessToken'\u001b[0m: \u001b[1m{\u001b[0m                                          \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   \u001b[0m\u001b[33m'displayName'\u001b[0m: \u001b[33m'OCR'\u001b[0m,                                 \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   \u001b[0m\u001b[33m'role'\u001b[0m: \u001b[33m'fineGrained'\u001b[0m,                                \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   \u001b[0m\u001b[33m'createdAt'\u001b[0m: \u001b[33m'2024-09-25T15:28:33.268Z'\u001b[0m,              \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   \u001b[0m\u001b[33m'fineGrained'\u001b[0m: \u001b[1m{\u001b[0m                                      \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   \u001b[0m\u001b[33m'canReadGatedRepos'\u001b[0m: \u001b[94mTrue\u001b[0m,                        \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   \u001b[0m\u001b[33m'global'\u001b[0m: \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m'inference.serverless.write'\u001b[0m,                 \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m'discussion.write'\u001b[0m,                           \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[33m'post.write'\u001b[0m                                  \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   \u001b[0m\u001b[1m]\u001b[0m,                                                \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   \u001b[0m\u001b[33m'scoped'\u001b[0m: \u001b[1m[\u001b[0m                                       \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[1m{\u001b[0m                                             \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m'entity'\u001b[0m: \u001b[1m{\u001b[0m                               \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'_id'\u001b[0m: \u001b[33m'66e310947b8ce2359ed82126'\u001b[0m,    \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'type'\u001b[0m: \u001b[33m'model'\u001b[0m                       \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[1m}\u001b[0m,                                        \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m'permissions'\u001b[0m: \u001b[1m[\u001b[0m\u001b[33m'repo.content.read'\u001b[0m\u001b[1m]\u001b[0m      \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[1m}\u001b[0m,                                            \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[1m{\u001b[0m                                             \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m'entity'\u001b[0m: \u001b[1m{\u001b[0m                               \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'_id'\u001b[0m: \u001b[33m'66f41faa78d3e2d74a214995'\u001b[0m,    \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'type'\u001b[0m: \u001b[33m'user'\u001b[0m,                       \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'name'\u001b[0m: \u001b[33m'akankshachandra'\u001b[0m             \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[1m}\u001b[0m,                                        \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[33m'permissions'\u001b[0m: \u001b[1m[\u001b[0m                          \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'repo.content.read'\u001b[0m,                  \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'inference.endpoints.infer.write'\u001b[0m,    \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'inference.endpoints.write'\u001b[0m,          \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'user.webhooks.read'\u001b[0m,                 \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'collection.read'\u001b[0m,                    \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'discussion.write'\u001b[0m,                   \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   │   \u001b[0m\u001b[33m'user.billing.read'\u001b[0m                   \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   │   \u001b[0m\u001b[1m]\u001b[0m                                         \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   │   \u001b[0m\u001b[1m}\u001b[0m                                             \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   │   \u001b[0m\u001b[1m]\u001b[0m                                                 \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   │   \u001b[0m\u001b[1m}\u001b[0m                                                     \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   │   \u001b[0m\u001b[1m}\u001b[0m                                                         \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[2m│   \u001b[0m\u001b[1m}\u001b[0m                                                             \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m                 \u001b[1m}\u001b[0m                                                                 \u001b[33m│\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰───────────────────────────────────────────────────────────────────────────────────╯\u001b[0m            \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[2;33m/usr/local/lib/python3.10/dist-packages/gradio/cli/commands/\u001b[0m\u001b[1;33mdeploy_space.py\u001b[0m:\u001b[94m52\u001b[0m in                \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[92madd_configuration_to_readme\u001b[0m                                                                      \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 49 \u001b[0m\u001b[2m│   │   │   \u001b[0m\u001b[95mor\u001b[0m app_file                                                                    \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 50 \u001b[0m\u001b[2m│   │   \u001b[0m)                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 51 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mif\u001b[0m \u001b[95mnot\u001b[0m app_file \u001b[95mor\u001b[0m \u001b[95mnot\u001b[0m os.path.exists(app_file):                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 52 \u001b[2m│   │   \u001b[0m\u001b[94mraise\u001b[0m \u001b[96mFileNotFoundError\u001b[0m(\u001b[33m\"\u001b[0m\u001b[33mFailed to find Gradio app file.\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 53 \u001b[0m\u001b[2m│   \u001b[0mconfiguration[\u001b[33m\"\u001b[0m\u001b[33mapp_file\u001b[0m\u001b[33m\"\u001b[0m] = app_file                                                   \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 54 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m   \u001b[2m 55 \u001b[0m\u001b[2m│   \u001b[0mconfiguration[\u001b[33m\"\u001b[0m\u001b[33msdk\u001b[0m\u001b[33m\"\u001b[0m] = \u001b[33m\"\u001b[0m\u001b[33mgradio\u001b[0m\u001b[33m\"\u001b[0m                                                        \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╭─\u001b[0m\u001b[33m────────────────────\u001b[0m\u001b[33m locals \u001b[0m\u001b[33m────────────────────\u001b[0m\u001b[33m─╮\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        app_file = \u001b[94mNone\u001b[0m                           \u001b[33m│\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m   configuration = \u001b[1m{\u001b[0m\u001b[33m'title'\u001b[0m: \u001b[33m'OCR_HINDI_ENGLISH'\u001b[0m\u001b[1m}\u001b[0m \u001b[33m│\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m        dir_name = \u001b[33m'content'\u001b[0m                      \u001b[33m│\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m            file = \u001b[33m'sample_data'\u001b[0m                  \u001b[33m│\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m       file_path = \u001b[33m'/content/sample_data'\u001b[0m         \u001b[33m│\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m formatted_title = \u001b[33m'OCR_HINDI_ENGLISH'\u001b[0m            \u001b[33m│\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m│\u001b[0m           title = \u001b[33m'OCR HINDI ENGLISH'\u001b[0m            \u001b[33m│\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m│\u001b[0m \u001b[33m╰──────────────────────────────────────────────────╯\u001b[0m                                             \u001b[31m│\u001b[0m\n",
            "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
            "\u001b[1;91mFileNotFoundError: \u001b[0mFailed to find Gradio app file.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOPwF8oKS9xEeJinZJaT+Lk",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "48b86b33d94d42e4b37a35f61123cb33": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5d6657aa5c53421da794d75329c2689a",
              "IPY_MODEL_c1ea9cf07132496099074290d13e6d3f",
              "IPY_MODEL_7a4d598fbfc54470bae58952c2e46bae"
            ],
            "layout": "IPY_MODEL_c22c8020217246c5aacace9309c08bda"
          }
        },
        "5d6657aa5c53421da794d75329c2689a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d948dca9c33943f9a0d855d2b39243c7",
            "placeholder": "​",
            "style": "IPY_MODEL_749aed1d4ab94234baf15c709472558f",
            "value": "Map:   8%"
          }
        },
        "c1ea9cf07132496099074290d13e6d3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_32546a6fa4c441b69cc13c8339695fc3",
            "max": 80000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_bbc8d7af5aba4c0cba31d1a91da9cf99",
            "value": 6000
          }
        },
        "7a4d598fbfc54470bae58952c2e46bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c6e66571407348118292d710806980f1",
            "placeholder": "​",
            "style": "IPY_MODEL_855d32c18b9d4284b1259e70b0b8da78",
            "value": " 6000/80000 [01:53&lt;20:45, 59.44 examples/s]"
          }
        },
        "c22c8020217246c5aacace9309c08bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d948dca9c33943f9a0d855d2b39243c7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "749aed1d4ab94234baf15c709472558f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32546a6fa4c441b69cc13c8339695fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bbc8d7af5aba4c0cba31d1a91da9cf99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c6e66571407348118292d710806980f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "855d32c18b9d4284b1259e70b0b8da78": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "156e520b47a94cd88caaec362a37a29e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3eef6f28cecb4f219c7832209c16fbb1",
              "IPY_MODEL_dc6aee75a01e4e02b2ed6bf4b3232dd5",
              "IPY_MODEL_24b2445e05c245c29b584bfc0ff4dbeb"
            ],
            "layout": "IPY_MODEL_be11b69bffb44e26ba89f4726f688b9e"
          }
        },
        "3eef6f28cecb4f219c7832209c16fbb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8dc6b5cf1d8140dcb91c64dc8551222e",
            "placeholder": "​",
            "style": "IPY_MODEL_e5c608d16b0d409ab9f362162b38b5b2",
            "value": "Map: 100%"
          }
        },
        "dc6aee75a01e4e02b2ed6bf4b3232dd5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_70d804e124c941018ab2a5ee0b113b91",
            "max": 80000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_d969e33bfaf64000b2f8d708460e4ce9",
            "value": 80000
          }
        },
        "24b2445e05c245c29b584bfc0ff4dbeb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01b2039530764232918c3cffd1bf914f",
            "placeholder": "​",
            "style": "IPY_MODEL_77fcb1352f0f41f590580baee90a07b5",
            "value": " 80000/80000 [00:00&lt;00:00, 116949.68 examples/s]"
          }
        },
        "be11b69bffb44e26ba89f4726f688b9e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dc6b5cf1d8140dcb91c64dc8551222e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e5c608d16b0d409ab9f362162b38b5b2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "70d804e124c941018ab2a5ee0b113b91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d969e33bfaf64000b2f8d708460e4ce9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "01b2039530764232918c3cffd1bf914f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77fcb1352f0f41f590580baee90a07b5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}